# REVISED Google Form Update Plan
## Aligned with User Testing Brief Requirements

**Based on:** The brief mandates all users complete Phase 1 (widget exploration), Phase 2 Part A (widget testing), and Phase 2 Part B (full page testing). Questions should assume completion and focus on experience quality.

---

## SECTION 4: Widget vs Full Page Experience (6 questions)

**Question 1: Phase 1 - Widget Discoverability**
- **Question text:** `Before clicking the chat button, did you notice it on the main page?`
- **Question type:** Multiple choice
- **Options:**
  - Yes, immediately - it was obvious
  - Yes, within a few seconds
  - Only after actively looking for it
  - I missed it initially (didn't see until instructed to click)

**Rationale:** Tests Phase 1 objective: "Do you notice a chatbot? Where is it?"

---

**Question 2: Phase 1 - Initial Widget Impression**
- **Question text:** `When you first opened the widget (before asking questions), how clear was its purpose?`
- **Question type:** Multiple choice (scale)
- **Options:**
  - 1 (Very unclear - had no idea what it was for)
  - 2
  - 3
  - 4
  - 5 (Very clear - immediately understood its purpose)

**Rationale:** Tests Phase 1 objective: "What you understand the chatbot's purpose to be"

---

**Question 3: Phase 2 Part A - Widget Usability**
- **Question text:** `During your widget testing (Part A), how intuitive was the widget interface?`
- **Question type:** Multiple choice (scale)
- **Options:**
  - 1 (Very confusing - struggled to use it)
  - 2
  - 3
  - 4
  - 5 (Very intuitive - easy to navigate)

**Rationale:** Tests widget usability during Part A mandatory testing

---

**Question 4: Phase 2 Part A - Session Feature**
- **Question text:** `You were asked to test the Session feature (click Session button, try copying ID). How did this work?`
- **Question type:** Multiple choice
- **Options:**
  - Session button worked, copy button worked perfectly
  - Session button worked, but copy button failed/needed manual copy
  - Session button didn't work
  - The feature seemed confusing or unnecessary
  - The feature was useful and clear

**Rationale:** Tests mandatory Phase 2 Part A step: "Click the 'Session' button to view your session ID. Try copying the session ID using the copy button."

---

**Question 5: Phase 2 Part B - Conversation Transfer**
- **Question text:** `When you clicked "Open in Full Page," did your conversation history transfer correctly?`
- **Question type:** Multiple choice
- **Options:**
  - Yes, perfectly - all previous messages appeared
  - Mostly - minor issues (missing 1-2 messages)
  - No - conversation was lost or incomplete
  - Partially - some messages transferred but with errors

**Rationale:** Tests mandatory Phase 2 Part B step: "Verify your conversation history transferred correctly. Do you see all your previous messages from the widget?"

---

**Question 6: Phase 2 Part B - Interface Preference**
- **Question text:** `You were asked to compare widget vs full page. Which did you prefer overall?`
- **Question type:** Multiple choice
- **Options:**
  - Widget - preferred the compact popup
  - Full page - preferred the expanded view
  - No strong preference - both worked well
  - Neither - both had significant issues

- **Follow-up (required if Widget or Full page selected):** `Why did you prefer this version? (open text)`

**Rationale:** Tests mandatory Phase 2 Part B comparison: "Which interface (widget vs full page) do you prefer and why?"

---

## SECTION 5: Technical Issues (3 questions)

**Question 1: Issue Types**
- **Question text:** `Did you experience any technical problems during testing? (Select all that apply)`
- **Question type:** Checkboxes
- **Options:**
  - [ ] Page wouldn't load or required password issues
  - [ ] Chatbot didn't respond to my queries
  - [ ] Very slow response times (>15 seconds consistently)
  - [ ] Browser crashed/froze
  - [ ] Citations didn't work or links were broken
  - [ ] Widget button was missing or didn't open
  - [ ] Session transfer failed (conversation didn't transfer to full page)
  - [ ] Copy button for session ID didn't work
  - [ ] Other: ___________
  - [ ] No issues - everything worked smoothly

---

**Question 2: Issue Frequency**
- **Question text:** `If you experienced issues, how often did they occur?`
- **Question type:** Multiple choice
- **Show if:** Q1 has anything selected except "No issues"
- **Options:**
  - Every time I tried to use the feature
  - Frequently (more than half the time)
  - Occasionally (less than half the time)
  - Rarely (once or twice total)
  - Never had issues

---

**Question 3: Overall Reliability**
- **Question text:** `Overall, how reliable was the chatbot during your testing session?`
- **Question type:** Multiple choice (scale)
- **Options:**
  - 1 (Very unreliable - major issues throughout)
  - 2
  - 3
  - 4
  - 5 (Very reliable - worked smoothly)

---

## SECTION 6: Specific Use Cases (4 questions)

**Question 1: Queries Asked**
- **Question text:** `What specific questions did you ask the chatbot? (List as many as you remember)`
- **Question type:** Long answer (paragraph)
- **Placeholder text:** `Example: "What are the employment risks from AI automation?" "How does facial recognition bias affect marginalized communities?"`

---

**Question 2: Information Quality**
- **Question text:** `For the repository-related questions you asked (about AI risks), did you get the information you needed?`
- **Question type:** Multiple choice
- **Options:**
  - Yes, completely - answers were thorough and relevant
  - Mostly - got useful info but some gaps
  - Partially - answers were somewhat helpful
  - Not really - answers didn't address my questions
  - Not at all - responses were irrelevant or unhelpful
  - N/A - I only asked edge case questions (non-AI topics)

---

**Question 3: Follow-up Interactions**
- **Question text:** `How many follow-up questions did you ask during your session?`
- **Question type:** Multiple choice
- **Options:**
  - None - initial questions were sufficient
  - 1-2 follow-ups
  - 3-5 follow-ups
  - 6-10 follow-ups
  - More than 10 follow-ups

---

**Question 4: Screen Recording Upload**
- **Question text:** `Upload your screen recording from Phase 1 & 2 (with audio commentary)`
- **Question type:** File upload
- **Settings:** Allow video files (mp4, mov, avi, webm), max 2GB
- **Help text:** "Please upload the recording you made during widget testing (Part A) and full page testing (Part B). This should include your verbal commentary describing your experience."

---

## SECTION 7: Background (3 questions - AT THE END)

**Question 1: Role**
- **Question text:** `What is your role?`
- **Question type:** Multiple choice
- **Options:**
  - Graduate Student
  - Postdoc
  - Faculty
  - Research Staff
  - Undergraduate
  - Industry Professional
  - Other: ___________

---

**Question 2: AI Risk Familiarity**
- **Question text:** `How familiar are you with AI risk research?`
- **Question type:** Multiple choice
- **Options:**
  - Very familiar (I actively research AI risks)
  - Moderately familiar (I've read papers or attended talks)
  - Somewhat familiar (basic awareness of the topic)
  - Not familiar (this is new to me)

---

**Question 3: Repository Usage**
- **Question text:** `Have you used the AI Risk Repository database before this testing session?`
- **Question type:** Multiple choice
- **Options:**
  - Yes, frequently (10+ times)
  - Yes, several times (3-9 times)
  - Once or twice
  - Never - this was my first time

---

## FINAL FORM STRUCTURE (29 Questions Total)

**Sections 1-3: Existing Questions (16 total)** - NO CHANGES
- Section 1: Response Quality (6 questions)
- Section 2: UI/UX (5 questions)
- Section 3: Overall Preference (5 questions)

**Section 4: Widget vs Full Page Experience (6 questions)** - NEW
- All questions assume users completed mandatory testing phases
- Questions focus on experience quality, not whether tasks were done

**Section 5: Technical Issues (3 questions)** - NEW
- Comprehensive issue tracking
- Conditional Q2 only shows if issues were reported

**Section 6: Specific Use Cases (4 questions)** - NEW
- Added screen recording upload question
- Questions align with brief's query recommendations

**Section 7: Background (3 questions)** - NEW, AT THE END
- Positioned at end to avoid selection bias

**Section 8: Improvements (existing ~3 questions)** - KEEP AT END
- What changes would improve experience
- Other chatbots to learn from
- Additional comments

**TOTAL: 29 questions** (16 existing + 13 new)

---

## STEP-BY-STEP IMPLEMENTATION INSTRUCTIONS

### Step 1: Open Google Form Editor
Navigate to: https://docs.google.com/forms/d/e/1FAIpQLSdcoCHp7TiHz6CeFQpgSx6PY2gcZ6i_XyPCvnCmKAwmRlTfGw/edit

### Step 2: Add Section 4 - Widget vs Full Page Experience

**After your existing Section 3 (Overall Preference), click "Add section"**

**Section title:** `Widget vs Full Page Experience`
**Section description:** `You were instructed to test both the widget (Part A) and full page (Part B) versions. These questions ask about your experience with each.`

**Add Question 1:**
- Click "+ Add question"
- Question text: `Before clicking the chat button, did you notice it on the main page?`
- Question type: Multiple choice
- Add options:
  - `Yes, immediately - it was obvious`
  - `Yes, within a few seconds`
  - `Only after actively looking for it`
  - `I missed it initially (didn't see until instructed to click)`
- Make required: Toggle "Required"

**Add Question 2:**
- Click "+ Add question"
- Question text: `When you first opened the widget (before asking questions), how clear was its purpose?`
- Question type: Multiple choice
- Add options:
  - `1 (Very unclear - had no idea what it was for)`
  - `2`
  - `3`
  - `4`
  - `5 (Very clear - immediately understood its purpose)`
- Make required: Toggle "Required"

**Add Question 3:**
- Click "+ Add question"
- Question text: `During your widget testing (Part A), how intuitive was the widget interface?`
- Question type: Multiple choice
- Add options:
  - `1 (Very confusing - struggled to use it)`
  - `2`
  - `3`
  - `4`
  - `5 (Very intuitive - easy to navigate)`
- Make required: Toggle "Required"

**Add Question 4:**
- Click "+ Add question"
- Question text: `You were asked to test the Session feature (click Session button, try copying ID). How did this work?`
- Question type: Multiple choice
- Add options:
  - `Session button worked, copy button worked perfectly`
  - `Session button worked, but copy button failed/needed manual copy`
  - `Session button didn't work`
  - `The feature seemed confusing or unnecessary`
  - `The feature was useful and clear`
- Make required: Toggle "Required"

**Add Question 5:**
- Click "+ Add question"
- Question text: `When you clicked "Open in Full Page," did your conversation history transfer correctly?`
- Question type: Multiple choice
- Add options:
  - `Yes, perfectly - all previous messages appeared`
  - `Mostly - minor issues (missing 1-2 messages)`
  - `No - conversation was lost or incomplete`
  - `Partially - some messages transferred but with errors`
- Make required: Toggle "Required"

**Add Question 6:**
- Click "+ Add question"
- Question text: `You were asked to compare widget vs full page. Which did you prefer overall?`
- Question type: Multiple choice
- Add options:
  - `Widget - preferred the compact popup`
  - `Full page - preferred the expanded view`
  - `No strong preference - both worked well`
  - `Neither - both had significant issues`
- Make required: Toggle "Required"
- Click "⋮" (three dots) → "Go to section based on answer" (optional advanced feature)
  - If "Widget" selected → Add follow-up question
  - If "Full page" selected → Add follow-up question

**Add Question 6b (Follow-up):**
- Click "+ Add question"
- Question text: `Why did you prefer this version?`
- Question type: Paragraph (long answer)
- Make required: Toggle "Required"
- Click "⋮" (three dots) → "Show based on answer" → Select Q6, only show if "Widget" or "Full page" selected

---

### Step 3: Add Section 5 - Technical Issues

**Click "Add section"**

**Section title:** `Technical Issues`
**Section description:** `Help us identify and fix technical problems you encountered during testing.`

**Add Question 1:**
- Click "+ Add question"
- Question text: `Did you experience any technical problems during testing? (Select all that apply)`
- Question type: Checkboxes
- Add options:
  - `Page wouldn't load or required password issues`
  - `Chatbot didn't respond to my queries`
  - `Very slow response times (>15 seconds consistently)`
  - `Browser crashed/froze`
  - `Citations didn't work or links were broken`
  - `Widget button was missing or didn't open`
  - `Session transfer failed (conversation didn't transfer to full page)`
  - `Copy button for session ID didn't work`
  - `Other`
  - `No issues - everything worked smoothly`
- Make required: Toggle "Required"

**Add Question 2:**
- Click "+ Add question"
- Question text: `If you experienced issues, how often did they occur?`
- Question type: Multiple choice
- Add options:
  - `Every time I tried to use the feature`
  - `Frequently (more than half the time)`
  - `Occasionally (less than half the time)`
  - `Rarely (once or twice total)`
  - `Never had issues`
- Click "⋮" (three dots) → "Show based on answer"
- Select Q1, set to show only if ANY option EXCEPT "No issues - everything worked smoothly" is selected
- Make required: Toggle "Required"

**Add Question 3:**
- Click "+ Add question"
- Question text: `Overall, how reliable was the chatbot during your testing session?`
- Question type: Multiple choice
- Add options:
  - `1 (Very unreliable - major issues throughout)`
  - `2`
  - `3`
  - `4`
  - `5 (Very reliable - worked smoothly)`
- Make required: Toggle "Required"

---

### Step 4: Add Section 6 - Specific Use Cases

**Click "Add section"**

**Section title:** `Specific Use Cases`
**Section description:** `Tell us about the questions you asked and whether you got helpful answers.`

**Add Question 1:**
- Click "+ Add question"
- Question text: `What specific questions did you ask the chatbot? (List as many as you remember)`
- Question type: Paragraph
- Add description: `Example: "What are the employment risks from AI automation?" "How does facial recognition bias affect marginalized communities?"`
- Make required: Toggle "Required"

**Add Question 2:**
- Click "+ Add question"
- Question text: `For the repository-related questions you asked (about AI risks), did you get the information you needed?`
- Question type: Multiple choice
- Add options:
  - `Yes, completely - answers were thorough and relevant`
  - `Mostly - got useful info but some gaps`
  - `Partially - answers were somewhat helpful`
  - `Not really - answers didn't address my questions`
  - `Not at all - responses were irrelevant or unhelpful`
  - `N/A - I only asked edge case questions (non-AI topics)`
- Make required: Toggle "Required"

**Add Question 3:**
- Click "+ Add question"
- Question text: `How many follow-up questions did you ask during your session?`
- Question type: Multiple choice
- Add options:
  - `None - initial questions were sufficient`
  - `1-2 follow-ups`
  - `3-5 follow-ups`
  - `6-10 follow-ups`
  - `More than 10 follow-ups`
- Make required: Toggle "Required"

**Add Question 4:**
- Click "+ Add question"
- Question text: `Upload your screen recording from Phase 1 & 2 (with audio commentary)`
- Question type: File upload
- Click "⋮" (three dots) → "Response validation"
- Set file types: Video files (.mp4, .mov, .avi, .webm)
- Set max file size: 2 GB (or 1 GB if form has size limits)
- Add description: `Please upload the recording you made during widget testing (Part A) and full page testing (Part B). This should include your verbal commentary describing your experience.`
- Make required: Toggle "Required"

---

### Step 5: Add Section 7 - Background

**Click "Add section"**

**Section title:** `Background`
**Section description:** `A few questions about your background to help us understand our test audience.`

**Add Question 1:**
- Click "+ Add question"
- Question text: `What is your role?`
- Question type: Multiple choice
- Add options:
  - `Graduate Student`
  - `Postdoc`
  - `Faculty`
  - `Research Staff`
  - `Undergraduate`
  - `Industry Professional`
  - `Other`
- Make required: Toggle "Required"

**Add Question 2:**
- Click "+ Add question"
- Question text: `How familiar are you with AI risk research?`
- Question type: Multiple choice
- Add options:
  - `Very familiar (I actively research AI risks)`
  - `Moderately familiar (I've read papers or attended talks)`
  - `Somewhat familiar (basic awareness of the topic)`
  - `Not familiar (this is new to me)`
- Make required: Toggle "Required"

**Add Question 3:**
- Click "+ Add question"
- Question text: `Have you used the AI Risk Repository database before this testing session?`
- Question type: Multiple choice
- Add options:
  - `Yes, frequently (10+ times)`
  - `Yes, several times (3-9 times)`
  - `Once or twice`
  - `Never - this was my first time`
- Make required: Toggle "Required"

---

### Step 6: Verify Section 8 (Improvements) is at the END

Your existing "Improvements" or "Final Comments" section should remain at the very end, after Section 7 (Background).

If it's not there:
- Click and drag the section to move it to the end
- Or click "⋮" → "Move section" → "Move to end"

---

### Step 7: Enable Progress Bar

- Click the "⚙️ Settings" icon (top right)
- Go to "Presentation" tab
- Enable "Show progress bar"
- Save

---

### Step 8: Optional - Add Email Collection

If you want to follow up with participants:
- At the very end of the form (after Section 8), add a question
- Question text: `(Optional) If you're willing to participate in a follow-up interview, please provide your email:`
- Question type: Short answer
- Click "⋮" → "Response validation" → "Email"
- Do NOT make required (keep optional)

---

## KEY CHANGES FROM ORIGINAL PLAN:

1. **Removed redundant "Did you try both?" question** - Brief mandates this
2. **Rephrased all questions to assume completion** - Questions ask "how was X?" not "did you do X?"
3. **Added question-specific context** - Each question references the brief's phase/part
4. **Session feature question is more detailed** - Tests both Session button AND copy button
5. **Added screen recording upload** - Critical for video analysis
6. **Question 6 now has conditional follow-up** - Only asks "why" if they preferred one version
7. **Reduced total from 32 to 29 questions** - More focused, less redundant

---

## VERIFICATION CHECKLIST:

After implementing, verify:
- [ ] Section 4 has 6 questions (not 7)
- [ ] All Section 4 questions assume users completed testing
- [ ] Section 5 Q2 is conditional (only shows if issues reported)
- [ ] Section 6 includes file upload for screen recording
- [ ] Section 7 (Background) is BEFORE Section 8 (Improvements)
- [ ] Progress bar is enabled
- [ ] All questions are marked "Required" except email (if added)
- [ ] Form title is clear: "AI Risk Repository Chatbot - User Testing Evaluation"

---

**Total question count: 29** (16 existing + 13 new)
