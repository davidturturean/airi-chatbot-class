Repository ID: RID-02270
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
search_all_fields: AI Risk Domain: 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation AI Risk Database v3 ai_risk_domain_summary
file_type: ai_risk_domain_summary
domain: 1.1 > Unfair discrimination and misrepresentation
specific_domain: 1.1 > Unfair discrimination and misrepresentation
is_summary: True
scqa_confidence: 1.0
rid: RID-02270
scqa_content_type: case_study
search_low_priority: AI Risk Database v3 ai_risk_domain_summary
search_high_priority: AI Risk Domain: 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation
search_medium_priority: 1.1 > Unfair discrimination and misrepresentation
entry_count: 75
scqa_complication: riables, like race or gender, but there is no transparency to this effect, making them impossible to challenge. These situations are typically only ident
content_preview: AI Risk Domain: 1.1 > Unfair discrimination and misrepresentation\n\nThis domain contains 75 risk entries from the AI Risk Repository:\n\nRisk Entry 1:\nTitle: Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 ...
summary_type: domain_aggregation
scqa_question: If the data contains bias (and much data does), then the AI will manifest that bias, too?
sheet: AI Risk Database v3
scqa_answer: generate He with a higher probability [4], [102]. Furthermore, stereotypical bias [103] which refers to overgeneralized beliefs about a particular gr
title: AI Risk Domain: 1.1 > Unfair discrimination and misrepresentation
scqa_situation: discrimination and misrepresentation\n\nThis domain contains 75 risk entries from the AI Risk Repository:\n\nRisk Entry 1:\nTitle: Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 > Unfair discrimination and misrepresentatio
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx

Content:
may involve demographic word prevalence and stereotypical contents. Concretely, in massive corpora, the prevalence of different pronouns and identities could influence an LLMâ€™s tendency about gender, nationality, race, religion, and culture [4]. For instance, the pronoun He is over-represented compared with the pronoun She in the training corpora, leading LLMs to learn less context about She and thus generate He with a higher probability [4], [102]. Furthermore, stereotypical bias [103] which refers to overgeneralized beliefs about a particular group of people, usually keeps incorrect values and is hidden in the large-scale benign contents. In effect, defining what should be regarded as a stereotype in the corpora is still an open problem."\nEntity: 2 - AI\nIntent: 2 - Unintentional\nTiming: 1 - Pre-deployment\n\nRisk Entry 4:\nTitle: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 > Unfair discrimination and