Repository ID: RID-00279
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
content_preview: Title: The Risks of Machine Learning Systems\nRisk Category: First-Order Risks\nRisk Subcategory: Application\nAdditional Evidence: "Accountability mechanisms: From an organizational perspective, mechanisms that hold the actors accountable for the systems they build reduce the likelihood of negative...
domain: Unspecified
entity: 
file_type: ai_risk_entry
intent: 
rid: RID-00279
risk_category: First-Order Risks
row: 256
scqa_answer: , engineers being rewarded more for increased user engagement than meeting an acceptable bias threshold)
scqa_complication: when the system falls short. However, this will only work when acceptance criteria are not in conflict (e.g., engineers being rewarded more for increased use
scqa_confidence: 1.0
scqa_content_type: case_study
scqa_question: What are the implications of this risk?
scqa_situation: Title: The Risks of Machine Learning Systems\nRisk Category: First-Order Risks\nRisk Subcategory: Application\nAdditional Evidence: "Accountability mechanisms: From an organizational perspective, mechanisms that hold the actors accountable for the systems they build reduce the likelihood of negative consequences
search_all_fields: The Risks of Machine Learning Systems Unspecified First-Order Risks Unspecified AI Risk Database v3 ai_risk_entry
search_high_priority: The Risks of Machine Learning Systems Unspecified First-Order Risks
search_low_priority: AI Risk Database v3 ai_risk_entry
search_medium_priority: Unspecified
sheet: AI Risk Database v3
specific_domain: Unspecified
subdomain: 
timing: 
title: The Risks of Machine Learning Systems

Content:
Title: The Risks of Machine Learning Systems\nRisk Category: First-Order Risks\nRisk Subcategory: Application\nAdditional Evidence: "Accountability mechanisms: From an organizational perspective, mechanisms that hold the actors accountable for the systems they build reduce the likelihood of negative consequences. For example, an organization might create explicit acceptability criteria, such as comparable accuracy across social groups, reward engineers for meeting these criteria, and block deployment when the system falls short. However, this will only work when acceptance criteria are not in conflict (e.g., engineers being rewarded more for increased user engagement than meeting an acceptable bias threshold)."