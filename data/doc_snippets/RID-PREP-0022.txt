Repository ID: RID-PREP-0022
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 3. AI Risk Database coded with causal taxonomy: entity, intent, timing
Content Type: comparative_analysis
Word Count: 542

Content:
influence at scale. Advances in AI have made powerful dual-use technologies like voice cloning, deep fakes, content generation, and data-gathering tools cheaper, more efficient, and easier to use (Cunha & Estima, 2023). With modest hardware requirements, these technologies are now within the reach of a broader group of users, including those with malicious intent. Disinformation is already a serious issue (Hendrycks et al., 2023) and involves the deliberate propagation of false or misleading information, usually with the intent to cause harm, influence behavior, or achieve a financial or political advantage (Electronic Privacy Information Centre, 2023; Infocomm Media Development Authority, 2023). AI tools could be used to amplify the impact and scope of disinformation through more personalized, convincing, and far-reaching messaging (Gabriel et al., 2024; Hendrycks et al., 2023; Weidinger et al., 2021, 2022; Wirtz et al., 2020). For example, the use of advanced AI in phishing schemes enables cybercriminals to automate the creation of highly sophisticated image, video, and audio communications (Cunha & Estima, 2023; Gabriel et al., 2024; Habbal et al., 2024). These communications can be tailored to individual recipients (sometimes including the cloned voice of a loved one), making them more likely to be successful and harder for both users and anti-phishing tools to detect (Gabriel et al., 2024). In the realm of surveillance, AI could support and enhance the mass gathering of personal data (Gabriel et al., 2024; Weidinger et al., 2021, 2022). Historically, mass surveillance required extensive manual effort. Machine learning tools can now link and process large datasets much more efficiently and cheaply than human analysts and can make predictions and decisions without human intervention (Weidinger et al., 2022). Through microtargeting, actors could manipulate individual behavior more subtly and effectively using AI-derived insights from their personal data and online behavior. In the hands of nefarious state actors, such capabilities could be used to enhance the effectiveness of illegitimate domestic surveillance campaigns and to facilitate oppression and control (Gabriel et al., 2024). All of the capabilities mentioned above could converge to facilitate the large-scale manipulation and control of what people see, hear, and believe. A form of this is automated censorship in which AI systems are used to selectively suppress or block specific types of information, content, or voices deemed undesirable to those controlling the AI (Wirtz et al., 2022). AI can not only be used to silence voices but also to entrench specific agendas: Actors (be they political figures, organizations, or state actors) could use AI to distribute incorrect information about electoral systems and processes (Vidgen et al., 2024), produce persuasive propaganda (Allianz Global Corporate & Security, 2018; Liu et al., 2023), and systematically exert control over public opinion and political debates on a large scale (Wirtz et al., 2022). During the 2016 Brexit referendum, a network of over 10,000 AI-powered political bots were employed to distribute fake and hyperpartisan news (Allianz Global Corporate & Security, 2018; Bastos & Mercea, 2019). The selective visibility of information can lead to the formation of incorrect or incomplete beliefs about what is happening in the world. This ability to shape public discourse can maintain or increase the power of those in control while keeping the public in the dark about critical issues that may affect their lives and their society.