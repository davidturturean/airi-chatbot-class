Repository ID: RID-02212
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
content_preview: Title: AI Risk Atlas\nRisk Category: Output risks (Explainability)\nRisk Subcategory: Unexplainable output\nAdditional Evidence: "Foundation models are based on complex deep learning architectures, making explanations for their outputs difficult. Inaccessible training data could limit the types of e...
domain: Unspecified
entity: 
file_type: ai_risk_entry
intent: 
rid: RID-02212
risk_category: Output risks (Explainability)
row: 2189
scqa_answer: Wrong explanations might lead to over-trust
scqa_complication: Title: AI Risk Atlas\nRisk Category: Output risks (Explainability)\nRisk Subcategory: Unexplainable output\nAdditional Evidence: "Foun
scqa_confidence: 1.0
scqa_content_type: risk_description
scqa_question: What are the implications of this risk?
scqa_situation: Title: AI Risk Atlas\nRisk Category: Output risks (Explainability)\nRisk Subcategory: Unexplainable output\nAdditional Evidence: "Foundation models are based on complex deep learning architectures, making explanations for their outputs difficult
search_all_fields: AI Risk Atlas Unspecified Output risks (Explainability) Unspecified AI Risk Database v3 ai_risk_entry
search_high_priority: AI Risk Atlas Unspecified Output risks (Explainability)
search_low_priority: AI Risk Database v3 ai_risk_entry
search_medium_priority: Unspecified
sheet: AI Risk Database v3
specific_domain: Unspecified
subdomain: 
timing: 
title: AI Risk Atlas

Content:
Title: AI Risk Atlas\nRisk Category: Output risks (Explainability)\nRisk Subcategory: Unexplainable output\nAdditional Evidence: "Foundation models are based on complex deep learning architectures, making explanations for their outputs difficult. Inaccessible training data could limit the types of explanations a model can provide. Without clear explanations for model output, it is difficult for users, model validators, and auditors to understand and trust the model. Wrong explanations might lead to over-trust."