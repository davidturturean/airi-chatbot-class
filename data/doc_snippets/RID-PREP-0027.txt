Repository ID: RID-PREP-0027
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: comparative_analysis
Word Count: 797

Content:
at a speed and scale that surpasses humans, this may jeopardize the ability of creators to earn an income and stymie human innovation and creativity (Hagendorff, 2024; Weidinger et al., 2022, 2023). A particularly damaging case of this may be developers using AIs to request off-the-shelf computer code (Cunha & Estima, 2023). Although some authors are attempting to sue AI companies for the appropriation of their work (Cunha & Estima, 2023), this and similar issues fall into a legal “gray area” within which current frameworks do not offer a secure path to recourse (Electronic Privacy Information Centre, 2023; Hagendorff, 2024). Several synergistic risks arise from the widespread dissemination and use of AI-generated cultural products. Because AIs optimize for repeated patterns in their training data, it is possible that their works will lack the diversity and unpredictability often celebrated in human works (Nah et al., 2023). Where synthetic works are adopted on a large enough scale, this could homogenize cultural experiences. Similarly, AIs do not understand the contextual significance of the cultural elements that they use. If AI enables the extensive commodification of certain products, it may expropriate their cultural value (Weidinger et al., 2022). For example, an AI might use Australian Aboriginal or Torres Strait Islander artwork in its designs without acknowledging or respecting their symbolic meanings. 6.4 Competitive dynamics. AI technology has the potential to redefine power dynamics across economic, political, and social spheres. As a result, many countries and corporations are investing heavily in AI research and development with the goal of becoming leaders in the area. While market competition can lead to beneficial economic and consumer outcomes, it also presents various risks, particularly in the field of AI (Hendrycks et al., 2023). In intensely competitive markets, AI developers and deployers may have an incentive to prioritize short-term, internal goals (e.g., profit or influence) to “secure their positions and survive” (Hendrycks et al., 2023), at the expense of external goals that encourage longer-term societal well-being (Hendrycks et al., 2023). A key concern is that AI companies may cut safety corners, releasing insecure and error-prone systems in a bid to stay ahead (McLean et al., 2023). These immature systems may present risks that are hard to identify and evaluate (Steimers & Schneider, 2022). Akin to the fossil fuel industry, profit-focused developers may allow their technologies to cause widespread externalities, such as “pollution, resource depletion, mental illness, misinformation, or injustice” (Critch & Russell, 2023). Countries or other state-like actors may engage in an AI-enabled military arms race, which could encourage the making of bad bets with a high potential for harm (Hendrycks et al., 2023; Wirtz et al., 2022). For example, they may give AI the autonomy to conduct cyberattacks, drone swarms, or disseminate propaganda and disinformation. 6.5 Governance failure. Governance failure refers to the risks and harms that arise when institutional, regulatory, and policy mechanisms fall short of effectively managing and overseeing the development and deployment of AI systems. Several issues make robust AI governance challenging to implement (Nah et al., 2023). First, it is difficult to determine who is responsible or liable when AI systems fail or make decisions that result in negative consequences (Allianz Global Corporate & Security, 2018; Electronic Privacy Information Centre, 2023; Saghiri et al., 2022; Wirtz et al., 2022). At present, there exists no comprehensive framework specifically designed to assign legal responsibility to AI agents (Meek et al., 2016). Traditional legal principles are based on human actors, whose intentions and actions can generally be identified and judged. AI’s decision-making, on the other hand, is often unpredictable, opaque, and involves complex interactions between millions of parameters (Nah et al., 2023). This complexity makes understanding how an AI arrived at a decision, and consequently who is responsible for the consequences of that decision, very difficult (Wirtz et al., 2020). In the absence of a regulatory or legal incentive to take safety engineering seriously, developers may release poorly designed AI systems (Meek et al., 2016), and people harmed by those systems may be left without recourse (Teixeira et al., 2022). A second challenge for effective AI governance is the rapid pace at which AI systems evolve. Typical governance and policy processes are inherently slow. Developing, proposing, debating, and implementing new regulations often involves multiple stakeholders, including government bodies, industry experts, and consultations with the public. The mismatch between the speed of AI advancements and their regulation may result in immature regulations that overlook important aspects of AI governance (Wirtz et al., 2022). The “great scope and ubiquity” of AI increases the difficulty of comprehensive governance (Wirtz et al., 2022). At present, many emerging aspects of AI-generated content are not explicitly addressed in copyright laws (Nah et al., 2023). Regulatory lags such as this could become increasingly dangerous as AI systems develop more harmful capabilities.