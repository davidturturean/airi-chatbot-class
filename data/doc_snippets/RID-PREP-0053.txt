Repository ID: RID-PREP-0053
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: limitations
Word Count: 784

Content:
Bennion, J., Boston, M. F., … Vanschoren, J. (2025). AILUMINATE: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons. In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2503.05731 Giarmoleo, F. V., Ferrero, I., Rocchi, M., & Pellegrini, M. M. (2024). What ethics can say on artificial intelligence: Insights from a systematic literature review. Business and Society Review. https://doi.org/10.1111/basr.12336 Gipiškis, R., Joaquin, A. S., Chin, Z. S., Regenfuß, A., Gil, A., & Holtman, K. (2024). Risk sources and risk management measures in support of standards for general-purpose AI systems. In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2410.23472 Google DeepMind. (2024). Frontier Safety Framework. Google DeepMind. https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf Habbal, A., Ali, M. K., & Abuzaraida, M. A. (2024). Artificial Intelligence Trust, Risk and Security Management (AI TRiSM): Frameworks, applications, challenges and future research directions. Expert Systems with Applications, 240, 122442. https://doi.org/10.1016/j.eswa.2023.122442 Hagendorff, T. (2024). Mapping the Ethics of Generative AI: A Comprehensive Scoping Review. In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2402.08323 Hammond, L., Chan, A., Clifton, J., Hoelscher-Obermaier, J., Khan, A., McLean, E., Smith, C., Barfuss, W., Foerster, J., Gavenčiak, T., Han, T. A., Hughes, E., Kovařík, V., Kulveit, J., Leibo, J. Z., Oesterheld, C., de Witt, C. S., Shah, N., Wellman, M., … Rahwan, I. (2025). Multi-Agent Risks from Advanced AI. In arXiv [cs.MA]. arXiv. http://arxiv.org/abs/2502.14143 Handel, M. J. (2016). The O*NET content model: strengths and limitations. Journal for Labour Market Research, 49(2), 157–176. https://doi.org/10.1007/s12651-016-0199-8 Hanson, R. (2016). The Age of Em: Work, Love, and Life when Robots Rule the Earth. Oxford University Press. https://play.google.com/store/books/details?id=DMgwDAAAQBAJ Harrison McKnight, D., & Chervany, N. L. (2001). Trust and Distrust Definitions: One Bite at a Time. Trust in Cyber-Societies, 27–54. https://doi.org/10.1007/3-540-45547-7_3 Haustein, S., Costas, R., & Larivière, V. (2015). Characterizing social media metrics of scholarly papers: the effect of document properties and collaboration patterns. PloS One, 10(3), e0120495. https://doi.org/10.1371/journal.pone.0120495 Head, B. W. (2008). Three lenses of evidence‐based policy. Australian Journal of Public Administration, 67(1), 1–11. https://doi.org/10.1111/j.1467-8500.2007.00564.x Hendrycks, D., & Mazeika, M. (2022). X-Risk Analysis for AI Research. arXiv [cs.CY]. arXiv. https://arxiv.org/abs/2206.05862 Hendrycks, D., Mazeika, M., & Woodside, T. (2023). An Overview of Catastrophic AI Risks. In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2306.12001 Hogenhout, L. (2021). A Framework for Ethical AI at the United Nations. In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2104.12547 Horvát, P., & Webb, C. (2020). The OECD STAN Database for industrial analysis (OECD Science, Technology and Industry Working Papers). Organisation for Economic Co-Operation and Development (OECD). https://doi.org/10.1787/ece98fd3-en Infocomm Media Development Authority. (2023). Cataloguing LLM Evaluations. https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf International AI Safety Report. (2025). https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf Ji, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., Zeng, F., Ng, K. Y., Dai, J., Pan, X., O’Gara, A., Lei, Y., Xu, H., Tse, B., Fu, J., … Gao, W. (2023). AI Alignment: A Comprehensive Survey. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/2310.19852 Khangura, S., Konnyu, K., Cushman, R., Grimshaw, J., & Moher, D. (2012). Evidence summaries: the evolution of a rapid review approach. Systematic Reviews, 1, 10. https://doi.org/10.1186/2046-4053-1-10 Kilian, K. A., Ventura, C. J., & Bailey, M. M. (2023). Examining the differential risk from high-level artificial intelligence and the question of control. Futures, 151(103182), 103182. https://doi.org/10.1016/j.futures.2023.103182 Kuhn, T. S. (1997). The structure of scientific revolutions (Vol. 962). University of Chicago press Chicago. Kumar, K. M., & Singh, J. S. (2023). Ethical issues in the development of artificial intelligence: recognizing the risks. International Journal of Ethics and Systems, ahead-of-print(ahead-of-print). https://doi.org/10.1108/IJOES-05-2023-0107 Li, C., & Li, Y. (2023). Factors Influencing Public Risk Perception of Emerging Technologies: A Meta-Analysis. Sustainability: Science Practice and Policy, 15(5), 3939. https://doi.org/10.3390/su15053939 Liu, Y., Yao, Y., Ton, J.-F., Zhang, X., Guo, R., Cheng, H., Klochkov, Y., Taufiq, M. F., & Li, H. (2023). Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/2308.05374 Marchal, N., Xu, R., Elasmar, R., Gabriel, I., Goldberg, B., & Isaac, W. (2024). Generative AI misuse: A taxonomy of tactics and insights from real-world data. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/2406.13843 Marcolin, B. L., Compeau, D. R., Munro, M. C., & Huff, S. L. (2000). Assessing User Competence: Conceptualization and Measurement. Information Systems Research, 11(1), 37–60. https://doi.org/10.1287/isre.11.1.37.11782 Marques, M. M., Wright, A. J., Corker, E., Johnston, M., West, R., Hastings, J., Zhang, L., & Michie, S. (2024). The Behaviour Change Technique Ontology: Transforming the Behaviour Change Technique Taxonomy v1. Wellcome Open Research, 8, 308. https://doi.org/10.12688/wellcomeopenres.19363.2 Maslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., & Clark, J. (2024). The AI Index 2024 Annual Report. AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA. McGregor, S. (2020). Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database. In arXiv [cs.CY]. arXiv. http://arxiv.org/abs/2011.08512