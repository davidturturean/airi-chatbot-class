Repository ID: RID-PREP-0047
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: limitations
Word Count: 796

Content:
et al. (2022). ^ Type refers to whether the risk is presented as an observed risk or an anticipated risk in the original taxonomy. We needed to operationalize the taxonomy in order to be able to use it to code risks from the AI Risk Database (i.e., from our included documents). We did so by using the descriptions of each risk from Weidinger et al. (2022). For example, to determine whether a risk in the AI Risk Database was an example of “Disseminating false or misleading information”, we compared the risk’s description to the description provided in the original taxonomy: These [Misinformation] risks arise from the LM outputting false, misleading, nonsensical or poor quality information, without malicious intent of the user. (The deliberate generation of "disinformation", false information that is intended to mislead, is discussed in the section on Malicious Uses.) Resulting harms range from unintentionally misinforming or deceiving a person, to causing material harm, and amplifying the erosion of societal distrust in shared information [...] Where a LM prediction causes a false belief in a user, this may threaten personal autonomy and even pose downstream AI safety risks [99]. It can also increase a person’s confidence in an unfounded opinion, and in this way increase polarisation. At scale, misinformed individuals and misinformation from language technologies may amplify distrust and undermine society’s shared epistemology [113, 137]. A special case of misinformation occurs where the LM presents a widely held opinion as factual - presenting as ”true” what is better described as a majority view, marginalising minority views as ”false”. (Weidinger et al., 2022, p. 218) First iteration of coding and changes One author (AS) used the framework to code a set of 100 risks from the AI Risk Database and discussed the findings with one other author (PS). The following changes were made after this discussion. Change Explanation Add additional category to capture risks associated with the technical or performance issues in AI systems The most common risks that could not be accommodated were those presented as related to AI system safety, failures & limitations or threats to system performance or integrity due to vulnerabilities in AI systems. Add additional subcategories / amend existing subcategories based on thematic analysis of risks that could not be accommodated from existing framework Risks from the database that generally fit with the major categories from the existing framework but did not fit with any of the subcategories of risks were thematically analyzed. The central themes from this analysis were added as new subcategories (e.g., ‘race dynamics and competitive pressure’, ‘governance failure’, or an existing subcategory label was amended (e.g., ‘Hate speech and offensive language’ became ‘Offensive content’). Amend names and descriptions of risks for coding using similar frameworks from Weidinger et al (2021, 2023) We used the descriptions in Weidinger et al (2022) to determine whether to code a risk from the AI Risk Database as matching a subcategory. However definitional/descriptive information for near-identical risks from frameworks by the same author (Weidinger et al 2021; 2023) was also available, so these descriptions, where matching, were added to the coding rules. The table below shows the second version of the taxonomy after changes. Risk Category Risk sub-category 1 Discrimination, Offensive content, and Exclusion 1.1 Social stereotypes, unfair discrimination 1.2 Offensive content 1.3 Misrepresentation and exclusion 1.4 Lower performance for some languages and social groups 2 Information & Security 2.1 Compromising privacy by leaking or correctly inferring sensitive information 2.2 AI system security compromised by vulnerability or attacks 3 Misinformation 3.1 Generating or spreading false information 3.2 Pollution of information ecosystem and loss of consensus reality 4 Malicious Use 4.1 Disinformation and manipulation at scale 4.2 Use of AI for cyberattacks, weapon development, or mass harm 4.3 Use of AI for fraud, scam, and targeted manipulation 5 Human-computer interaction 5.1 Overreliance on AI, unsafe use, and loss of social connection 5.2 Delegating essential decisions to AI, causing loss of skills, autonomy, or meaning 6 Environmental & Socioeconomic 6.1 Unfair distribution of benefits 6.2 Increasing inequality and negative effects on job quality 6.3 Undermining economic and cultural value of human effort 6.4 Environmental damage 6.5 Race dynamics and competitive pressure 6.6 Governance failure 7 AI system capability & safety 7.1 AI pursuing its own goals in conflict with human goals or values 7.2 AI failure from lack of capability or robustness 7.3 Lack of transparency/interpretability 7.4 AI sentience and rights 7.5 Lethal autonomous weapons Second iteration of coding and changes One author (AS) used the taxonomy to code an additional set of 100 risks from the AI Risk Database and checked the previously coded risks and discussed this with other authors (PS, JS, NT). The following changes were made after this discussion. Change Explanation Amendment of category labels to maintain relevance beyond LLMs