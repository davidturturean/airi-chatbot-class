Repository ID: RID-PREP-0024
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: comparative_analysis
Word Count: 759

Content:
on a much larger scale than would otherwise be possible. AI can manage multiple attack vectors simultaneously, coordinating them to maximize disruption and harm. Malicious actors may intentionally cause mass harm through terrorism or the disruption of law enforcement (Critch & Russell, 2023). For example, AI could automate the process of finding and exploiting vulnerabilities in software used by millions of people (Gabriel et al., 2024). AI could also be used to identify vulnerabilities in national power grids and strategically target key components to cause outages or to determine optimal release points for biological agents to maximize impact and spread. 4.3 Fraud, scams, and targeted manipulation. AI capabilities have the potential to be exploited for personal gain at the expense of others via deception and manipulation. This can take various forms including cheating, fraud, scams, and the use of deepfakes for blackmail or humiliation. It is currently very difficult to distinguish human text from text that is AI-generated (Hagendorff, 2024). This increases opportunities for cheating in settings where rewards depend on the communication of original thought. In academia, students may use AI to quickly generate essays or other coursework and claim it as their own (Cui et al., 2024; Hagendorff, 2024; Nah et al., 2023). If students’ regularly and inappropriately rely on AI for their schooling, this could undermine academic integrity and genuine intellectual development (Hagendorff, 2024). In science, researchers could use AI unscrupulously to produce professional outputs (Cui et al., 2024). If widely adopted, this practice could dilute the overall quality of scientific discourse (Hagendorff, 2024). Generative AI products may also be used to increase the reach and potency of various dishonest schemes. Advanced AI assistants can produce HTML, CSS, and other web development languages, allowing for the rapid creation of convincing fraudulent websites and applications at scale (Gabriel et al., 2024). In the context of social media, generative adversarial networks (GANs) have been used to create images of human faces that look authentic (Gabriel et al., 2024). These images can be uploaded as profile pictures to fake accounts to make them seem more trustworthy. AI models can also be trained on speech or writing data from a specific individual. This allows the model to impersonate someone very convincingly without consent. Scammers could use this capability to request sensitive information or financial aid by pretending to be a trusted contact (Weidinger et al., 2021, 2022). AI has recently advanced in generating realistic deep fakes which have enabled new forms of targeted harassment and extortion (Hogenhout, 2021). A particularly damaging type of abuse facilitated by deep fakes involves creating non-consensual sexual imagery with the intent to cause a subject social injury or manipulate them into performing desired actions (Electronic Privacy Information Centre, 2023; Liu et al., 2023; Shelby et al., 2023; Weidinger et al., 2023). Even if a deep fake is exposed as inauthentic, it can continue to impact a person’s life in significant ways (Electronic Privacy Information Centre, 2023) through the loss of job opportunities, social isolation, and ongoing harassment or defamation. Domain 5: Human-computer interaction 5.1 Overreliance and unsafe use. Users may come to trust or rely on AI systems beyond their actual capabilities or to anthropomorphize AI systems, which can lead to emotional or material dependence and inappropriate relationships with or expectations of AI systems. Users who develop trust in an AI may be harmed if this trust is miscalibrated, such as relying on an AI to provide advice, make decisions, or otherwise act in complex, risky situations for which the AI is only superficially equipped (Gabriel et al., 2024). For example, a user experiencing a mental health crisis may request psychotherapy from an AI with whom they have formed a connection. Were the AI to respond with insensitive or destructive advice, this could put the person in immediate danger (Gabriel et al., 2024). When people interact with AIs that use convincing natural language, they may start to perceive them as having human-like attributes and invest undue confidence in their capabilities (Weidinger et al., 2021, 2022). Anthropomorphic perceptions of AIs may encourage users to develop emotional trust in the systems (Hagendorff, 2024), which can make users more likely to follow suggestions, accept advice, and disclose personal information (Weidinger et al., 2021, 2022). This trust could be exploited by manipulative actors who wish to harvest user’s sensitive data or influence their decisions and actions for purposes which are unlikely to be in the user’s best interests (Weidinger et al., 2022). For example, AI systems could be used to power increasingly manipulative recommendation algorithms (Weidinger et al., 2023).