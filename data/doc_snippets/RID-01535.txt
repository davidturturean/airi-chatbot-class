Repository ID: RID-01535
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_answer: ing from AI actions. They could derive from a misaligned model pursuing dangerous goals, such as gather power, or from unintended side effects."
intent: 
sheet: AI Risk Database v3
search_medium_priority: Unspecified
search_all_fields: Future Risks of Frontier AI Unspecified Capabilities that increase the likelihood of existential risk Unspecified AI Risk Database v3 ai_risk_entry
search_high_priority: Future Risks of Frontier AI Unspecified Capabilities that increase the likelihood of existential risk
risk_category: Capabilities that increase the likelihood of existential risk
rid: RID-01535
scqa_complication: Title: Future Risks of Frontier AI\nRisk Category: Capabilities that increase the likelihood of existential risk\nAdditional Evidence: "Th
subdomain: 
scqa_situation: e an existential risk, a model must be given or gain some control over systems with significant impacts, such as weapons or financial systems. That model would then need the capa
search_low_priority: AI Risk Database v3 ai_risk_entry
timing: 
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
entity: 
domain: Unspecified
row: 1512
content_preview: Title: Future Risks of Frontier AI\nRisk Category: Capabilities that increase the likelihood of existential risk\nAdditional Evidence: "This debate is unlikely to be resolved soon. To pose an existential risk, a model must be given or gain some control over systems with significant impacts, such as ...
file_type: ai_risk_entry
title: Future Risks of Frontier AI
scqa_content_type: case_study
scqa_question: What are the implications of this risk?
scqa_confidence: 1.0
specific_domain: Unspecified

Content:
Title: Future Risks of Frontier AI\nRisk Category: Capabilities that increase the likelihood of existential risk\nAdditional Evidence: "This debate is unlikely to be resolved soon. To pose an existential risk, a model must be given or gain some control over systems with significant impacts, such as weapons or financial systems. That model would then need the capability to manipulate these systems while rendering mitigations ineffective. These effects could be direct or indirect, for example the consequences of conflict resulting from AI actions. They could derive from a misaligned model pursuing dangerous goals, such as gather power, or from unintended side effects."