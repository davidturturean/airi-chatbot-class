Repository ID: RID-01535
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
search_low_priority: AI Risk Database v3 ai_risk_entry
rid: RID-01535
file_type: ai_risk_entry
risk_category: Capabilities that increase the likelihood of existential risk
subdomain: 
intent: 
search_medium_priority: Unspecified
specific_domain: Unspecified
timing: 
entity: 
scqa_answer: ing from AI actions. They could derive from a misaligned model pursuing dangerous goals, such as gather power, or from unintended side effects."
title: Future Risks of Frontier AI
scqa_complication: Title: Future Risks of Frontier AI\nRisk Category: Capabilities that increase the likelihood of existential risk\nAdditional Evidence: "Th
sheet: AI Risk Database v3
content_preview: Title: Future Risks of Frontier AI\nRisk Category: Capabilities that increase the likelihood of existential risk\nAdditional Evidence: "This debate is unlikely to be resolved soon. To pose an existential risk, a model must be given or gain some control over systems with significant impacts, such as ...
scqa_situation: e an existential risk, a model must be given or gain some control over systems with significant impacts, such as weapons or financial systems. That model would then need the capa
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_question: What are the implications of this risk?
row: 1512
search_high_priority: Future Risks of Frontier AI Unspecified Capabilities that increase the likelihood of existential risk
scqa_content_type: case_study
search_all_fields: Future Risks of Frontier AI Unspecified Capabilities that increase the likelihood of existential risk Unspecified AI Risk Database v3 ai_risk_entry
domain: Unspecified
scqa_confidence: 1.0

Content:
Title: Future Risks of Frontier AI\nRisk Category: Capabilities that increase the likelihood of existential risk\nAdditional Evidence: "This debate is unlikely to be resolved soon. To pose an existential risk, a model must be given or gain some control over systems with significant impacts, such as weapons or financial systems. That model would then need the capability to manipulate these systems while rendering mitigations ineffective. These effects could be direct or indirect, for example the consequences of conflict resulting from AI actions. They could derive from a misaligned model pursuing dangerous goals, such as gather power, or from unintended side effects."