Repository ID: RID-PREP-0043
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: future_work
Word Count: 790

Content:
better represented as involving several stages. Our categorization frameworks do not capture several variables that may be important for audiences seeking to mitigate risks or balance benefits with risks, such as threat vectors (e.g., bio, cyber), types of AI systems (e.g., reinforcement learning, large language models), open vs. closed source, organizational types (e.g., big tech, startups), temporal aspects (near-term vs. long-term), or types of harm (e.g., economic loss, deaths). Future work should consider adding new dimensions or developing more granular categorizations. We have therefore shared our database openly and encourage others to build upon it. Other opportunities for future research We found overlapping clusters of risk concepts and categories in our included documents, but the specificity, consistency, and coherence of the definitions of these concepts could be significantly improved beyond our attempt in developing the Domain Taxonomy and describing each subdomain in detail. This could help to foster greater shared understanding through the use of component terms with consistent and shared definitions, such as in an ontology (Marques et al., 2024). The effect of shared understanding on research and practice can be subtle but should not be ignored. As Kuhn (1997) notes, shared paradigms are prerequisites for “the genesis and continuation of a particular research tradition" (p. 11). Mueller (2004) reinforces this, asserting that “the most fruitful research programs [...] are those in which the key concepts are agreed on and defined the same way by all” (p. 62). Several areas of risk seem underexplored relative to the wider literature and their importance. We found that most existing frameworks focus on language models (LLMs) rather than on broader AI contexts. This suggests that other areas, such as AI agents, may warrant greater consideration, a topic explored in two included documents (Gabriel et al., 2024; McLean et al., 2023). Agentic AI may be particularly important to consider as it presents new classes of risks associated with the possession and use of dangerous capabilities, such as recursive self-improvement (e.g., Shavit et al., n.d.). One paper added through the ongoing expert consultation grappled with multi-agent risks (Hammond et al., 2025). Relatively few documents discussed pre-deployment risks from humans. This may be important to address; there are emerging concerns about the potential for bad actors to create dangerous or unethical AI (e.g., Althaus & Baumann, 2020; Ferrara, 2024; Marchal et al., 2024). Only two documents discussed AI welfare and rights (Meek et al., 2016; Uuk et al., 2025); this issue may be deserving of greater acknowledgement and attention (e.g., Hanson, 2016). Conclusion This paper and the associated products (i.e., website and database) provided a comprehensive and accessible resource for understanding and addressing the risks associated with AI. This resource is not presented as a definitive source of truth but as a common foundation for constructive engagement and critique and a starting point for a common frame of reference to understand and address risks from AI. It provides a way to help people to understand and debate the risks from AI and decide which ones they want to tackle (e.g., through research, risk frameworks, regulations, etc.). It presents as a catalog of risks from AI, rather than an argument for why any of these risks are more or less important. Our methodology involved a systematic review of existing literature, leading to the creation of a living database of AI risks and the development of two frameworks to navigate it. Our AI Risk Database provides a comprehensive overview of the AI risk landscape, which can be focused on specific risks types to support targeted mitigation, research, and policy development. It contains detailed records of AI-related risks extracted from a variety of sources, categorized into high-level and mid-level taxonomies. The high-level Causal Taxonomy includes attributes such as the entity responsible for the risk (human, AI, or other), the intent (intentional, unintentional, or other), and the timing (pre-deployment, post-deployment, or other). The mid-level Domain Taxonomy categorizes risks into 24 specific domains like discrimination, misinformation, malicious use, and human-computer interaction issues. Each entry includes detailed metadata, such as the source of the risk, specific descriptions, and additional evidence where relevant. We used this synthesis to evaluate the AI risk landscape and the relative attention devoted to specific AI risk domains. Our work makes several contributions. To the best of our knowledge, this is the first attempt to comprehensively review and synthesize AI risk frameworks into a database, develop taxonomies from the database, and create supporting products to use and improve them. The resultant AI Risk Repository is uniquely comprehensive and extensible; it includes and indexes a large range of risks and sources and makes all data accessible for further adaptation and use. This makes our Repository uniquely positioned to support a wide range of future activities and research endeavors.