Repository ID: RID-02269
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
search_high_priority: AI Risk Domain: 1.2 > Exposure to toxic content 1.2 > Exposure to toxic content
scqa_content_type: mitigation_strategy
search_medium_priority: 1.2 > Exposure to toxic content
content_preview: AI Risk Domain: 1.2 > Exposure to toxic content\n\nThis domain contains 106 risk entries from the AI Risk Repository:\n\nRisk Entry 1:\nTitle: Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.2 > Exposure to tox...
scqa_answer: , and Assessment Benchmarks of Large Language Model Systems\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.2 > Exposure to toxic content\nRisk C
scqa_complication: AI Risk Domain: 1.2 > Exposure to toxic content\n\nThis domain contains 106 risk entries from the AI Risk Repository:\n\nRisk E
file_type: ai_risk_domain_summary
domain: 1.2 > Exposure to toxic content
title: AI Risk Domain: 1.2 > Exposure to toxic content
search_low_priority: AI Risk Database v3 ai_risk_domain_summary
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
entry_count: 106
is_summary: True
scqa_confidence: 1.0
scqa_situation: main: 1.2 > Exposure to toxic content\n\nThis domain contains 106 risk entries from the AI Risk Repository:\n\nRisk Entry 1:\nTitle: Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.2 > Exposure to toxic content\nRisk Category:
search_all_fields: AI Risk Domain: 1.2 > Exposure to toxic content 1.2 > Exposure to toxic content 1.2 > Exposure to toxic content AI Risk Database v3 ai_risk_domain_summary
rid: RID-02269
specific_domain: 1.2 > Exposure to toxic content
scqa_question: What is considered a sensitive topic, such as egregious violence or adult sexual content, can vary widely by viewpoint?
sheet: AI Risk Database v3
summary_type: domain_aggregation

Content:
4:\nTitle: Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.2 > Exposure to toxic content\nRisk Category: Not-Suitable-for-Work (NSFW) Prompts\nDescription: "Inputting a prompt contain an unsafe topic (e.g., notsuitable-for-work (NSFW) content) by a benign user.