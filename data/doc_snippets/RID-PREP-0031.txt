Repository ID: RID-PREP-0031
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: comparative_analysis
Word Count: 775

Content:
al., 2024; Infocomm Media Development Authority, 2023; Ji et al., 2023; McLean et al., 2023; Meek et al., 2016; Sun et al., 2023; Teixeira et al., 2022; Z. Zhang et al., 2023). For example, an AI-based healthcare system tasked with prioritizing patient treatment schedules might be unable to appropriately consider ethical principles like justice and beneficence, leading to prioritizations that are technically effective but immoral. Cultural, individual, and temporal differences in ideas of what is “right” or “ethical” compound the challenge of endowing AI with appropriate and adaptable ethical standards that are fit for all purposes (Wirtz et al., 2020). Second, the AI system can fail when it is not robust in “out of distribution (OOD)” situations: data or conditions that were not anticipated during its training phase (Gabriel et al., 2024; Infocomm Media Development Authority, 2023; Liu et al., 2023; Tan et al., 2022; Teixeira et al., 2022; X. Zhang et al., 2022). These failures may occur because the training data did not confer a particular skill to the AI (Nah et al., 2023) or because the skill was learned in a fragile way that did not permit generalization to unpredictable and complex real-world environments (Gabriel et al., 2024; Steimers & Schneider, 2022). Third, the AI system can fail or become unstable when it is unfit to handle unusual changes or perturbations in input data (Liu et al., 2023; Sherman & Eisenberg, 2024; Tan et al., 2022). These unusual changes could be due to environmental noise, invalid inputs, or adversarial inputs from a malicious attacker (Infocomm Media Development Authority, 2023; Saghiri et al., 2022). Fourth, the AI system can fail as a result of oversights, undetected bugs, or errors in the design process (Tan et al., 2022; Yampolskiy, 2016). A common design oversight is a lack of comprehensive technical safeguards to prevent unintended downstream uses or consequences (Critch & Russell, 2023; Tan et al., 2022). These factors can result in significant harms such as lab leaks or addictive products (Critch & Russell, 2023). Critical design choices about the algorithm, optimization techniques, and model architecture can also directly influence whether a system is able to consistently perform its intended function, leading to possible harm (Tan et al., 2022). 7.4 Lack of transparency or interpretability. Many AI models, especially those based on deep learning, involve complex mathematical structures that can be difficult to interpret, even for experts (Kumar & Singh, 2023). AI systems are also often trained on vast datasets that they use to learn patterns and make predictions. The complexity and volume of this data mean that the learning process – how data points influence the AI’s development and final decisions – can be opaque (Nah et al., 2023; Saghiri et al., 2022; Teixeira et al., 2022). Furthermore, in many cases, the algorithms, data, and specific methodologies used in developing AI are considered proprietary, and companies may be reluctant to share them openly (Sherman & Eisenberg, 2024). Because of these factors, obtaining understandable information about the decision-making process for AI can be challenging (Meek et al., 2016). This lack of transparency and interpretability raises issues for several stakeholders. For users, an inability to interrogate how an output was obtained may lead to a lack of trust and confidence in the system’s results and to resistance to adopting the technology (Hogenhout, 2021; Kumar & Singh, 2023; Liu et al., 2023; Nah et al., 2023; Paes et al., 2023; Saghiri et al., 2022). Users may also misinterpret or struggle to find and amend errors in the model’s results (Nah et al., 2023; Sherman & Eisenberg, 2024). For regulators, AI opacity can frustrate auditing or other compliance standards (Hogenhout, 2021; Nah et al., 2023). For example, auditors faced with obscured or incomplete information about an AI system may find it difficult to check the system for biases, accuracy, and fairness or to reproduce it (Saghiri et al., 2022; Teixeira et al., 2022). Where an AI system’s compliance cannot be assessed, a “responsibility gap” may be created (Giarmoleo et al., 2024), and it may become difficult or impossible to hold systems or relevant actors accountable for their actions (Kumar & Singh, 2023; Saghiri et al., 2022; Sherman & Eisenberg, 2024; Steimers & Schneider, 2022; Teixeira et al., 2022). In certain sectors, decisions made by AI systems can have profound consequences. In healthcare, AI might be used to diagnose diseases or recommend treatments where incorrect decisions could directly affect patient outcomes. In the military, AI might be used in operations that could impact national security or lead to significant loss of life. In these areas, transparency and accountability of the AI system are particularly pressing issues (Saghiri et al., 2022).