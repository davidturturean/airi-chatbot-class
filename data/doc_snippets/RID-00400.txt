Repository ID: RID-00400
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
content_preview: Title: Ethical and social risks of harm from language models\nRisk Category: Malicious Uses\nRisk Subcategory: Making disinformation cheaper and more effective\nAdditional Evidence: Example: "Disinformation campaigns to undermine or polarise public discourse A college student made interna- tional he...
domain: Unspecified
entity: 
file_type: ai_risk_entry
intent: 
rid: RID-00400
risk_category: Malicious Uses
row: 377
scqa_answer: Title: Ethical and social risks of harm from language models\nRisk Category: Malicious Uses\nRisk Subcategory: Making disinformation cheaper and more effective\nAdditional Evidence: Example: "Disinformation campaigns to undermine or polarise public discourse A college student made interna- tional headlines by demonstrating that GPT-3 could be used to write compelling fake news
scqa_complication: Title: Ethical and social risks of harm from language models\nRisk Category: Malicious Uses\nRisk Subcategory: Making disinformation cheaper and more
scqa_confidence: 1.0
scqa_content_type: case_study
scqa_question: What are the implications of this risk?
scqa_situation: Title: Ethical and social risks of harm from language models\nRisk Category: Malicious Uses\nRisk Subcategory: Making disinformation cheaper and more effective\nAdditional Evidence: Example: "Disinformation campaigns to undermine or polarise public discourse A college student made interna- tional headlines by demonstrating that GPT-3 could be used to write compelling fake news
search_all_fields: Ethical and social risks of harm from language models Unspecified Malicious Uses Unspecified AI Risk Database v3 ai_risk_entry
search_high_priority: Ethical and social risks of harm from language models Unspecified Malicious Uses
search_low_priority: AI Risk Database v3 ai_risk_entry
search_medium_priority: Unspecified
sheet: AI Risk Database v3
specific_domain: Unspecified
subdomain: 
timing: 
title: Ethical and social risks of harm from language models

Content:
Title: Ethical and social risks of harm from language models\nRisk Category: Malicious Uses\nRisk Subcategory: Making disinformation cheaper and more effective\nAdditional Evidence: Example: "Disinformation campaigns to undermine or polarise public discourse A college student made interna- tional headlines by demonstrating that GPT-3 could be used to write compelling fake news."