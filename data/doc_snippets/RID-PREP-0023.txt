Repository ID: RID-PREP-0023
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 3. AI Risk Database coded with causal taxonomy: entity, intent, timing
Content Type: comparative_analysis
Word Count: 650

Content:
public opinion and political debates on a large scale (Wirtz et al., 2022). During the 2016 Brexit referendum, a network of over 10,000 AI-powered political bots were employed to distribute fake and hyperpartisan news (Allianz Global Corporate & Security, 2018; Bastos & Mercea, 2019). The selective visibility of information can lead to the formation of incorrect or incomplete beliefs about what is happening in the world. This ability to shape public discourse can maintain or increase the power of those in control while keeping the public in the dark about critical issues that may affect their lives and their society. 4.2 Cyberattacks, weapon development or use, and mass harm. AI may be used to gain a political or strategic advantage or to cause harm at scale through cyber operations or the development and use of weapons. Advancements in AI have provided malicious actors with powerful tools that can lead to more frequent, more severe, and more precise cyber attacks (Gabriel et al., 2024; Hendrycks & Mazeika, 2022; Hogenhout, 2021; Kilian et al., 2023; Liu et al., 2023; Weidinger et al., 2022; Wirtz et al., 2022). Hackers could use the coding abilities of AI assistants to develop malicious malware more effectively and at lower cost (Allianz Global Corporate & Security, 2018; Hagendorff, 2024; Hogenhout, 2021; Sherman & Eisenberg, 2024; Weidinger et al., 2022). With AI, even those with limited coding and technical experience (Electronic Privacy Information Centre, 2023; Gabriel et al., 2024) could teach a model to produce and optimize malware code that discovers and exploits system vulnerabilities, including both self-replicating (Allianz Global Corporate & Security, 2018; Infocomm Media Development Authority, 2023) and automated software (Cui et al., 2024; Shevlane et al., 2023). The development and application of weapons could also be sped up and intensified through AI. For example, AIs with specialized knowledge of bioengineering could make it easier for more actors to design new bioweapons (Hendrycks et al., 2023; Infocomm Media Development Authority, 2023). For example, in 2022 a small pharmaceutical company used generative AI to develop 40,000 chemical nerve agents in less than six hours (Sohn, 2022). AI could also enable autonomous devices, such as drones, to be used as weapons (Allianz Global Corporate & Security, 2018). In fact, AI has already assisted in the development and application of Lethal Autonomous Weapons Systems (LAWS) – weapons that can operate without human oversight and use computer algorithms to identify and attack targets (Habbal et al., 2024; Hogenhout, 2021). Autonomous weapons may fail in ways that other AI systems do, such as through a lack of capability, robustness, or loss of control, meaning that they would cause harm that was not intended by their developers or operators. In these circumstances, the risks from LAWS would not be limited to malicious actors or misuse. However, in most cases, we conceptualize risks from LAWS as relating to purposeful decisions made by humans controlling the weapons. AIs deployed by states in conflict could also be integrated in conventional defense or mass-casualty weapons (Teixeira et al., 2022; Vidgen et al., 2024). These integrations could range from AI-controlled aerial combat to the operation of AI as part of a country's nuclear arsenal as a “fail-safe” mechanism (Hendrycks & Mazeika, 2022). Overall, AI’s ability to process vast amounts of data quickly may empower actors to act on a much larger scale than would otherwise be possible. AI can manage multiple attack vectors simultaneously, coordinating them to maximize disruption and harm. Malicious actors may intentionally cause mass harm through terrorism or the disruption of law enforcement (Critch & Russell, 2023). For example, AI could automate the process of finding and exploiting vulnerabilities in software used by millions of people (Gabriel et al., 2024). AI could also be used to identify vulnerabilities in national power grids and strategically target key components to cause outages or to determine optimal release points for biological agents to maximize impact and spread.