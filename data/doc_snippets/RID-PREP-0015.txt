Repository ID: RID-PREP-0015
Source: AI_Risk_Repository_Preprint.docx
Section: Introduction	15
Content Type: statistical_analysis
Word Count: 679

Content:
the risk arises from decisions or actions made by the AI system itself, such as generating harmful content or disempowering humans. Conversely, when humans are seen as the source, the risks are implied to be due to human actions like choosing poor training data, intentional malicious design, or improper use of AI systems. The "Other" category captures cases where the focal entity is not a human or AI or is ambiguous. For example, “The software development toolchain of LLMs is complex and could bring threats to the developed LLM,” implies that the toolchain could be exploited by humans or AI. The Intent variable captures whether the risk is presented as occurring as an expected or unexpected outcome from pursuing a goal. This variable has three levels: Intentional, Unintentional, and Other. Intentional risks are those that occur as expected outcomes from pursuing a specific goal, such as a case where AI is intentionally programmed to act deceptively or to exhibit bias. Unintentional risks reflect unintended consequences, such as an AI system inadvertently developing biases due to incomplete training data. The "Other" category captures risks where the intent is not clearly specified; for example, “The external tools (e.g., web APIs) present trustworthiness and privacy issues to LLM-based applications.” This includes cases where the risk may occur intentionally and unintentionally, such as "The potential for the AI system to infringe upon individuals' rights to privacy, through the data it collects, how it processes that data, or the conclusions it draws." The Timing variable captures the stage in the AI lifecycle at which the risk is presented as occurring. The levels within this variable include Pre-deployment, Post-deployment, and Other. Pre-deployment risks are those that arise before the AI system is fully developed and put into use, such as vulnerabilities in the model due to coding errors. Post-deployment risks arise after the AI has been deployed, including issues like the misuse of AI for harmful purposes. Deployment is not defined in Yampolskiy (2016); we therefore interpreted it to mean when a product is being used by end users rather than just by developers. The "Other" category is used for risks that do not have a clearly defined time of occurrence (e.g., ”Resilience against adversarial attacks and distribution shift”). This includes cases where the presented risk may occur both before and after deployment; for example, "Generative models are known for their substantial energy requirements, necessitating significant amounts of electricity, cooling water, and hardware containing rare metals.” Most common causal factors for AI risk Table 3 shows how the risks were coded against each category of causal factors. A majority of the risks were presented by authors of the documents as due to a decision or action by an artificial intelligence system (40%). Slightly more risks were presented as Unintentional (34%) compared to intentional (33%). Most of the risks were presented as occurring post-deployment (61%). Table 3. AI Risk Database coded with causal taxonomy: entity, intent, timing Category Level Proportion Entity Human 39% AI 41% Other 20% Intent Intentional 34% Unintentional 35% Other 31% Timing Pre-deployment 13% Post-deployment 62% Other 25% Note. Totals may not match due to rounding. Table 4 shows how the risks intersect across our three causal factors. The most common triads of causal conditions under which an AI risk was presented as occurring were Entity = Human, Intention = Intentional, Timing = Post-deployment (18% of all risks). This was followed by Entity = AI, Intention = Unintentional, Timing = Post-deployment (15% of all risks). Table 4. AI Risk Database coded with Causal Taxonomy: entity x intent x timing Intent Timing Entity Intentional Unintentional Other Pre-deployment Human 2% 4% • AI • 2% • Other • • • Post-deployment Human 18% 5% 3% AI 5% 15% 9% Other 2% 2% 4% Other Human 3% 2% 2% AI 3% 4% 2% Other • 2% 7% Note. Taxonomy categories with a prevalence ≥ 10% are highlighted. Categories with a prevalence less than 2% in the AI Risk Database are shown as • for ease of interpretation. Causal factors of AI risk examined by included documents