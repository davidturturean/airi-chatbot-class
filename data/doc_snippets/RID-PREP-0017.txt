Repository ID: RID-PREP-0017
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: comparative_analysis
Word Count: 783

Content:
Yampolskiy (2016) X X X X X X X X 41 Allianz (2018) X X X X X X X 42 Teixeira (2022) X X X X X X X X X 43 InfoComm (2023) X X X X X X 44 Coghlan (2023) X X X X X X X X X 45 TC260 (2024) X X X X X X X X X 46 Ferrara (2023) X X X X X X X 47 G'sell (2024) X X X X X X X X X 48 NIST (2024) X X X X X X X X X 49 Bengio (2024) X X X X X X X X 50 Zeng (2024) X X X X X X X X 51 Everitt (2018) X X X X X X X X X 52 Maham (2023) X X X X X X X X X 53 Maas (2023) X X X X X X X X X 54 Leech (2024) X X X X X X X X X 55 Clarke (2022) X X X X X X X X X 56 GOS (2023) X X X X X X X X X 57 Ghosh (2024) X X X 58 Abercrombie (2024) X X X X X X X X 59 Schnitzer (2024) X X X X X X X X X 60 Bengio (2025) X X X X X X X X X 61 Uuk (2025) X X X X X X X X X 62 Gipiškis (2024) X X X X X X X X X 63 Hammond (2025) X X X X X X X X X 64 Marchal (2024) X X X X X 65 IBM (2025) X X X X X X X Domain Taxonomy of AI Risks As a result of a best-fit framework synthesis, which selected, adapted, and iterated on a mid-level taxonomy of categories of AI risk (Weidinger et al., 2022), we developed the Domain Taxonomy of AI Risks. This taxonomy catalogs hazards and harms associated with AI and was used to classify risks in the AI Risk Database. We coded 1409 (87%) of the 1612 risks extracted from our documents against the Domain Taxonomy. In Table 6, we present the Domain Taxonomy including subdomains and short descriptions of each subdomain. In the following sections we present a detailed description of each subdomain using information from included documents in the AI Risk Database, as well as analysis of the distribution of risks and domains across the database. Table 6. Domain Taxonomy of AI Risks Domain / Subdomain Description 1 Discrimination & toxicity 1.1 Unfair discrimination and misrepresentation Unequal treatment of individuals or groups by AI, often based on race, gender, or other sensitive characteristics, resulting in unfair outcomes and unfair representation of those groups. 1.2 Exposure to toxic content AI that exposes users to harmful, abusive, unsafe or inappropriate content. May involve providing advice or encouraging action. Examples of toxic content include hate speech, violence, extremism, illegal acts, or child sexual abuse material, as well as content that violates community norms such as profanity, inflammatory political speech, or pornography. 1.3 Unequal performance across groups Accuracy and effectiveness of AI decisions and actions is dependent on group membership, where decisions in AI system design and biased training data lead to unequal outcomes, reduced benefits, increased effort, and alienation of users. 2 Privacy & security 2.1 Compromise of privacy by obtaining, leaking, or correctly inferring sensitive information AI systems that memorize and leak sensitive personal data or infer private information about individuals without their consent. Unexpected or unauthorized sharing of data and information can compromise user expectation of privacy, assist identity theft, or cause loss of confidential intellectual property. 2.2 AI system security vulnerabilities and attacks Vulnerabilities that can be exploited in AI systems, software development toolchains, and hardware, resulting in unauthorized access, data and privacy breaches, or system manipulation causing unsafe outputs or behavior. 3 Misinformation 3.1 False or misleading information AI systems that inadvertently generate or spread incorrect or deceptive information, which can lead to inaccurate beliefs in users and undermine their autonomy. Humans that make decisions based on false beliefs can experience physical, emotional, or material harms 3.2 Pollution of information ecosystem and loss of consensus reality Highly personalized AI-generated misinformation that creates “filter bubbles” where individuals only see what matches their existing beliefs, undermining shared reality and weakening social cohesion and political processes. 4 Malicious actors & misuse 4.1 Disinformation, surveillance, and influence at scale Using AI systems to conduct large-scale disinformation campaigns, malicious surveillance, or targeted and sophisticated automated censorship and propaganda, with the aim of manipulating political processes, public opinion, and behavior. 4.2 Cyberattacks, weapon development or use, and mass harm