Repository ID: RID-PREP-0039
Source: AI_Risk_Repository_Preprint.docx
Section: Domain Taxonomy
Content Type: limitations
Word Count: 799

Content:
taxonomies to classify these risks: the Causal Taxonomy of AI Risks for understanding how, when, or why risks from AI may emerge, and the Domain Taxonomy of AI Risks to classify commonly discussed hazards and harms associated with AI. The database and taxonomies are then used to evaluate the curated literature and provide a range of insights into the state of this literature. In this section, we discuss i) insights into the “AI risk landscape,” ii) specific implications for policymaker, auditor, academic research, and industry audiences, and iii) limitations and opportunities for future research. Insights into the “AI risk landscape” Our findings present several implications for the collective understanding of how the landscape of AI risks is constructed. Before discussing these, we emphasize that our findings involve a particular lens of analysis and therefore necessarily reveal and obscure different aspects of the more complex system (Head, 2008; Nilsen, 2015; Sovacool & Hess, 2017). What follows should therefore not be regarded as a complete reflection of the actual or ideal landscape but as several lenses which may offer insights into future policy, research, or practical work to understand and address AI risks. Insights from the AI Risk Database and included documents Most of the documents we found that presented structured taxonomies or classifications of AI risks were very recent (post 2020). 25 were peer-reviewed articles, 22 were pre-prints (typically hosted on ArXiv), 6 were conference papers and 12 were industry reports. In addition, most papers presented a non-systematic or narrative review (also called a survey of research) and did not describe their methodology in detail. This suggests that AI risk research is characterized by a focus on rapid knowledge dissemination, possibly to keep pace with the accelerating progress and investment in AI capabilities and applications. However, this focus on rapid work and dissemination poses challenges for coordination and standardization of research and for the practical work needed to understand and address risks from AI. This issue is more stark when considering the absence of documents from the most influential AI companies that are developing and deploying the largest and most capable AI models (Epoch AI, 2024). Some notable exceptions include Google DeepMind, with several included documents (Gabriel et al., 2024; Weidinger et al., 2021, 2022, 2023), and ByteDance, with one included document (Liu et al., 2023). In addition, one document led by Google DeepMind included co-authors from OpenAI & Anthropic (Shevlane et al., 2023). Large Language models (LLMs) are the type of AI most commonly assessed for risks; of the 35 documents examined that specified a type of AI, eleven focused on risks from LLMs, nine on generative AI, eight on general-purpose AI, four on Artificial General Intelligence and three on machine learning. One each examined i) AI assistants, ii) frontier AI, iii) algorithmic systems, and iv) AI and machine learning (AI/ML) v) advanced AI. Most of the risks in the AI Risk Database can be coded using the Causal Taxonomy or the Domain Taxonomy; of the 1612 risks we extracted; we were able to classify 86% of the extracted risks using the Causal Taxonomy and 87% using the Domain Taxonomy. Insights from the Causal Taxonomy Our Causal Taxonomy describes three categories of causal factors to understand how AI risks occur: the Entity (whether a Human or an AI system causes the risk), the Intent (whether the risk emerges as an expected outcome [Intentional] or unexpected outcome [Unintentional] from pursuing a goal), and the Timing of the risk (before the AI system is deployed [Pre-deployment] or after the AI model has been trained and deployed [Post-deployment]). These causal factors combine to help understand how, when, or why risks from AI emerge. Table 12 summarizes several key insights from the application of the Causal Taxonomy to the AI Risk Database and included documents. Table 12. Insights from Causal Taxonomy of AI Risks Insight Supporting evidence AI risks tend to be equally attributed to decisions or actions taken by AI and humans Slightly more risks were presented with the causal Entity as an AI (41%) compared with humans (39%) AI risk is seen as roughly equally the result of intentional and unintentional actions A similar proportion of risks were presented as the result of intentional action (34%) as with unintentional action (35%) There is more focus on risks emerging after AI is deployed than during development Approximately 5 times as many risks were presented as occurring after the AI model has been trained and deployed (62%) than before deployment (13%). Human-caused risks were most likely to be seen as intentional; AI-caused risks were most likely to be seen as unintentional or ambiguous Each combination of Entity, Intent, and Timing included 0-9% of risks in the database, but Human-caused intentional risks included 18% of risks; and AI-caused unintentional risks included 15% of risks.