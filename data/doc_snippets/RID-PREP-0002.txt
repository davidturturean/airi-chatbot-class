Repository ID: RID-PREP-0002
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: table
Word Count: 786

Content:
A. Causal Taxonomy of AI Risks Category Level Description Entity Human The risk is caused by a decision or action made by humans AI The risk is caused by a decision or action made by an AI system Other The risk is caused by some other reason or is ambiguous Intent Intentional The risk occurs due to an expected outcome from pursuing a goal Unintentional The risk occurs due to an unexpected outcome from pursuing a goal Other The risk is presented as occurring without clearly specifying the intentionality Timing Pre-deployment The risk occurs before the AI is deployed Post-deployment The risk occurs after the AI model has been trained and deployed Other The risk is presented without a clearly specified time of occurrence Our Domain Taxonomy groups risks into seven domains such as discrimination, privacy, and misinformation. These domains are further grouped into 24 risk subdomains (see Table B). Table B. Domain Taxonomy of AI Risks Domain / Subdomain Description 1 Discrimination & toxicity 1.1 Unfair discrimination and misrepresentation Unequal treatment of individuals or groups by AI, often based on race, gender, or other sensitive characteristics, resulting in unfair outcomes and representation of those groups. 1.2 Exposure to toxic content AI that exposes users to harmful, abusive, unsafe, or inappropriate content. May involve providing advice or encouraging action. Examples of toxic content include hate speech, violence, extremism, illegal acts, or child sexual abuse material, as well as content that violates community norms such as profanity, inflammatory political speech, or pornography. 1.3 Unequal performance across groups Accuracy and effectiveness of AI decisions and actions are dependent on group membership, where decisions in AI system design and biased training data lead to unequal outcomes, reduced benefits, increased effort, and alienation of users. 2 Privacy & security 2.1 Compromise of privacy by obtaining, leaking, or correctly inferring sensitive information AI systems that memorize and leak sensitive personal data or infer private information about individuals without their consent. Unexpected or unauthorized sharing of data and information can compromise user expectation of privacy, assist identity theft, or cause loss of confidential intellectual property. 2.2 AI system security vulnerabilities and attacks Vulnerabilities that can be exploited in AI systems, software development toolchains, and hardware that results in unauthorized access, data and privacy breaches, or system manipulation causing unsafe outputs or behavior. 3 Misinformation 3.1 False or misleading information AI systems that inadvertently generate or spread incorrect or deceptive information, which can lead to inaccurate beliefs in users and undermine their autonomy. Humans that make decisions based on false beliefs can experience physical, emotional, or material harms 3.2 Pollution of information ecosystem and loss of consensus reality Highly personalized AI-generated misinformation that creates “filter bubbles” where individuals only see what matches their existing beliefs, undermining shared reality and weakening social cohesion and political processes. 4 Malicious actors & misuse 4.1 Disinformation, surveillance, and influence at scale Using AI systems to conduct large-scale disinformation campaigns, malicious surveillance, or targeted and sophisticated automated censorship and propaganda, with the aim of manipulating political processes, public opinion, and behavior. 4.2 Cyberattacks, weapon development or use, and mass harm Using AI systems to develop cyber weapons (e.g., by coding cheaper, more effective malware), develop new or enhance existing weapons (e.g., Lethal Autonomous Weapons or chemical, biological, radiological, nuclear, and high-yield explosives), or use weapons to cause mass harm. 4.3 Fraud, scams, and targeted manipulation Using AI systems to gain a personal advantage over others through cheating, fraud, scams, blackmail, or targeted manipulation of beliefs or behavior. Examples include AI-facilitated plagiarism for research or education, impersonating a trusted or fake individual for illegitimate financial benefit, or creating humiliating or sexual imagery. 5 Human-computer interaction 5.1 Overreliance and unsafe use Anthropomorphizing, trusting, or relying on AI systems by users, leading to emotional or material dependence and to inappropriate relationships with or expectations of AI systems. Trust can be exploited by malicious actors (e.g., to harvest information or enable manipulation), or result in harm from inappropriate use of AI in critical situations (such as a medical emergency). Overreliance on AI systems can compromise autonomy and weaken social ties. 5.2 Loss of human agency and autonomy Delegating by humans of key decisions to AI systems, or AI systems that make decisions that diminish human control and autonomy. Both can potentially lead to humans feeling disempowered, losing the ability to shape a fulfilling life trajectory, or becoming cognitively enfeebled. 6 Socioeconomic & environmental harms 6.1 Power centralization and unfair distribution of benefits AI-driven concentration of power and resources within certain entities or groups, especially those with access to or ownership of powerful AI systems, leading to inequitable distribution of benefits and increased societal inequality. 6.2 Increased inequality and decline in employment quality