Repository ID: RID-PREP-0011
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: limitations
Word Count: 696

Content:
as our initial best-fit framework for developing a causal taxonomy for AI risk. We selected Yampolskiy’s taxonomy as it was highly cited (116 citations, fifth most highly cited from the set of identified papers), simple, comprehensive, and provided sufficient definitions for each category. Yampolskiy’s taxonomy systematically classifies the ways in which an AI system might become dangerous based on two main factors: Timing - whether the AI became dangerous at the pre-deployment or post-deployment stage, and Cause - whether the danger arose from External Causes (On Purpose, By Mistake, Environment) or Internal Causes originating from the AI system itself (Independently). Yampolskiy proposes that this taxonomy covers scenarios ranging from AI being purposely designed to be dangerous, to becoming dangerous by accident during development or after deployment, to turning dangerous due to environmental factors outside its control, or evolving to become dangerous through recursive self-improvement. Each “pathway” represents a set of causal conditions that lead to AI causing harm, e.g., a person using a large language model (LLM) to generate fake news for political gain would be classified under Path B (“Timing: post-deployment; External cause: on purpose”). Coding and iteration process We started by using Yampolskiy's taxonomy to categorize a sample of risks from our database. We then identified themes in the AI Risk Database that didn't fit into Yampolskiy's taxonomy. We updated the taxonomy categories, criteria, and descriptions, then coded a further sample of risks. This process was repeated over three iterations until the taxonomy categories, criteria, and descriptions were stable. We describe this iteration process in detail in Appendix A. Final taxonomy The final version of the taxonomy, which we named the Causal Taxonomy of AI Risks, included three categories of causal factors that specify how, why, or when an AI risk might emerge. The first category, Entity, classified the entity (e.g., AI system, Human) that was presented as causing the risk to occur due to a decision or action taken by that entity. The second category, Intent, classified whether the risk was presented as an expected outcome or unexpected outcome of an entity pursuing a goal. The third category, Timing, classified the stage in the AI lifecycle that the risk is presented as occurring (e.g., pre-deployment, post-deployment). Each of these categories includes a third option, "Other," which captures risks that are not clearly categorizable within the primary options. Each of the categories is therefore mutually exclusive; each risk is classified under only one option within each category. The Causal Taxonomy is presented and described in more detail in the Results section. Development of mid-level Domain Taxonomy of AI Risks Best-fit taxonomy: Weidinger (2022) We chose Weidinger et al (2022) “Taxonomy of Risks posed by Language Models” as our initial mid-level best-fit framework because it and its related papers (Weidinger et al., 2021, 2023) are among the highest cited in our review. Although this taxonomy was focused on language models, its set of categories was one of the most comprehensive, and it has been iterated upon over several publications. It included six areas of risks from language models: (1) Discrimination, Hate speech and Exclusion; (2) Information Hazards; (3) Misinformation Harms; (4) Malicious Uses; (5) Human-computer interaction Harms; and (6) Environmental and Socioeconomic Harms. Each area of risk described several subcategories of risk. Coding and iteration process We applied this taxonomy by coding as many of the included risks as possible using the Weidinger (2022) taxonomy. We operationalised the taxonomy by using the definitions or descriptions for each category from Weidinger (2022). Because several similar taxonomies were included in the set identified by the systematic literature review (e.g., Weidinger et al., 2021, 2022, 2023), we considered descriptions and definitions from any of these taxonomies in our initial coding. We iterated on the taxonomy to accommodate risks that could not be coded against the existing Weidinger (2022) taxonomy. The most common risks that could not be accommodated were those related to AI system safety, failures and limitations; AI system security vulnerabilities and attacks; and competitive dynamics or other failures of governance to manage the development and deployment of AI systems. We describe this iteration process in detail in Appendix A. Final taxonomy: Domain Taxonomy of AI Risks