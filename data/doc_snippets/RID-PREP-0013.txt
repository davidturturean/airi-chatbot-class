Repository ID: RID-PREP-0013
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: future_work
Word Count: 800

Content:
(92%) of the included documents published later than 2020. We coded the corresponding author’s country and found that most of the included documents were from the USA (n = 18), United Kingdom (n = 11), China (n = 9) and Germany (n = 8). Other countries included Australia, Singapore, Portugal, Canada, Iran, Lithuania, Scotland, Netherlands, India, Spain, Türkiye, Brazil and Switzerland. Most of the included documents had a corresponding author affiliated with a University (n = 35), followed by industry organization (n = 15), with the remainder from government, international organizations (i.e., United Nations), or non-government organizations (n = 15). The most common affiliation was DeepMind / Google DeepMind (n = 5). The most common described methodology was a narrative review or “survey” of existing literature (n = 20), followed by systematic review (n = 7) and expert consultation (n = 7). Many of the papers (n = 25) did not explicitly describe their methodology in any detail. The included documents varied in the type or scope of Artificial Intelligence they focused on. In most cases, the type of AI was not explicitly defined (n = 26). Large language model was the next most common (n = 11), followed by Generative AI (n = 9), General-purpose AI (n = 8), Artificial General Intelligence (n = 4) and Machine Learning (n = 3). Other terms included “AI and Machine Learning” (always described in the document as “AI/ML”), AI assistant, algorithmic systems, frontier AI and advanced AI. The framing of risk and AI risk differed significantly across documents. Only eight documents explicitly defined risk, describing it as presented below: “...the impact of uncertainty on objectives” (Steimers & Schneider, 2022), “the consequence of an event combined with its likelihood of occurrence” (Tan et al., 2022) “A Hazard is a source of danger with the potential to harm… Risk = Hazard × Exposure × Vulnerability” (Hendrycks & Mazeika, 2022). “the composite measure of an event’s probability(or likelihood) of occurring and the magnitude or degree of the consequences of the corresponding event”. (National Institute of Standards and Technology (US), 2024) “...the combination of the probability of an occurrence of harm and the severity of that harm” (Gipiškis et al., 2024) “...the combined function of an event’s probability and the severity of its potential consequences” (Ghosh et al., 2025) “...a function of the probability and severity associated with a specific hazard” (Schnitzer et al., 2023) “...combination of the probability and severity of a harm that arises from the development, deployment, or use of AI” (International AI Safety Report, 2025) The classifications, frameworks, and taxonomies used varied terms to describe risks, including: “risks of/from AI,” “harms of AI,” “AI ethics,” “ethical issues/concerns/challenges,” “social impacts/harms,” and others. Together, the included documents presented a total of 1612 risk categories (e.g., “Privacy risks”) or sub-categories (e.g., “Compromising privacy by leaking sensitive information”, Weidinger et al., 2022) of risk. Not all documents presented a framework with both categories and sub-categories. Not all documents presented eligible risk categories in sufficient detail to allow us to code them with our taxonomies; two included documents were not coded as having any distinct risk categories or framework (AI Verify Foundation, 2023; e.g., Sharma, 2024). These are therefore unrepresented in later outputs. Our supplementary online resources include a database of all risks and included documents & frameworks. The full set of included documents are also presented in Appendix B. The included documents contained a range of highly cited taxonomies, as shown in Table 1. Table 1. 20 most cited documents that present a taxonomy or classification of AI risks Title First author First author affiliation (country) Year Type Citations^ Citations^ / year Ethical and social risks of harm from language models Weidinger Deepmind (UK) 2021 Preprint 1106 277 Generative AI and ChatGPT: Applications, Challenges, and AI-Human Collaboration Nah City University of Hong Kong (China) 2023 Journal Article 933 467 Taxonomy of Risks posed by Language Models Weidinger Deepmind (UK) 2022 Conference Paper 668 223 The ethics of ChatGPT -- Exploring the ethical issues of an emerging technology Stahl University of Nottingham (UK) 2024 Journal Article 352 352 Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment Liu ByteDance Research (China) 2024 Preprint 318 318 The Dark Sides of Artificial Intelligence: An Integrated AI Governance Framework for Public Administration Wirtz German University of Administrative Sciences Speyer (Germany) 2020 Journal Article 310 62 AI Alignment: A Comprehensive Survey Ji Peking University (China) 2023 Preprint 267 134 Artificial Intelligence Trust, Risk and Security Management (AI TRiSM): Frameworks, Applications, Challenges and Future Research Directions Habbal Karabuk University (Turkiye) 2024 Journal Article 226 226 An Overview of Catastrophic AI Risks Hendrycks Center for AI Safety (USA) 2023 Preprint 225 113 The risks associated with Artificial General Intelligence: A systematic review McLean University Of The Sunshine Coast (Australia) 2023