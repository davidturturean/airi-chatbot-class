Repository ID: RID-PREP-0009
Source: AI_Risk_Repository_Preprint.docx
Section: Domain Taxonomy
Content Type: comparative_analysis
Word Count: 702

Content:
due to a bug with the model. Only three out of the four key papers (Critch & Russell, 2023; McLean et al., 2023; Steimers & Schneider, 2022; Weidinger et al., 2022) had appeared in the screening process, and the final key paper was scheduled to appear several thousand papers later. Because Stage 3 of the SAFE process aims to ensure that records are not missed due to the initial model, the screening team switched models to see if a new model would locate the relevant paper. Phase 3: Switch active-learning model and screen additional records until stopping rule is reached Based on a review of relevant literature (e.g., Campos et al., 2024; van de Schoot et al., 2021), we use the Oracle model with the following set-up: a fully connected neural network (2 hidden layers) model as the classifier and sBert as the feature extractor, maximum as the query strategy and dynamic resampling (double) as the balance strategy. The model was trained on the data that was labeled while using the previous model. Screening stopped when no extra relevant records were identified in the last 50 records. Both authors screened in the missing key paper within the first two records found by the new model. Phase 4: Evaluate quality For quality checks, the screening team screened records previously labeled as irrelevant using the Oracle model and the default model set-up (i.e., the same model that was used in the initial/main model phase). This model was trained using the 10 highest- and lowest-ranked records from the model switching phase. Both team members screened records to identify any relevant records that might have been falsely excluded. This continued until the stopping rule was met (no extra relevant records identified in the last 50 records). One member of the screening team screened the full text of all records that were included at the title/abstracts step. For calibration, 10% of the records were screened in duplicate, with 100% interrater reliability achieved. Conflicts were resolved by discussion for any remaining records. Stage 2: Forwards and backwards searching and expert consultation Following full-text screening, we undertook forwards and backwards searching using Scopus, Google Scholar, and various preprint servers hosting the included gray literature. Backwards searching involved identifying and reviewing all references from articles included in Stage 1, while forwards searching involved identifying and reviewing all articles that cited an included article. We also undertook an expert consultation, which involved sharing the preliminary set of included articles with their authors and other experts and requesting recommendations for relevant frameworks that had been overlooked. All records identified during forwards and backwards searching and expert consultation were screened by one author. Those which met inclusion criteria were added to the backlog for extraction. Extraction into living AI Risk Database Five authors were involved in data extraction. A template data extraction spreadsheet was developed to capture various details from the studies, including title, abstract, author, year, source/outlet, risk category name, risk category description, risk subcategory name, risk subcategory description, and page number. This spreadsheet was refined over several rounds of pilot testing and extractor calibration on subsets of randomly selected articles. Data extraction was then conducted individually, with regular meetings for discussion and conflict resolution. Based on the recommendations of grounded theory, we aimed to capture the studied phenomena directly from the data rather than impose our interpretations (cf. Charmaz, 2006; Corbin & Strauss, 2014). Consequently, we extracted risks based on how the authors presented them, maintaining fidelity to their original categorizations and descriptions. Best fit framework synthesis approach Seven authors were involved in data synthesis. We used a “best fit” framework synthesis approach to develop two AI risk taxonomies. Best-fit framework synthesis is a method for rapidly, clearly, and practically understanding the relationships and structures between concepts in a topic area (Carroll et al., 2011, 2013). It combines the strengths of framework synthesis (Ritchie & Spencer, 2002), which is a “top down” positivist method where concepts are coded against a pre-existing structure, and thematic synthesis (Thomas & Harden, 2008), which is a “bottom up” interpretative method where concepts are iteratively analyzed to identify patterns and structure. We outline the process in Figure 2. Figure 2. Methodology for Best Fit Framework Synthesis