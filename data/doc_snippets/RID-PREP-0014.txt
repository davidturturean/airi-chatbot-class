Repository ID: RID-PREP-0014
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: future_work
Word Count: 685

Content:
318 The Dark Sides of Artificial Intelligence: An Integrated AI Governance Framework for Public Administration Wirtz German University of Administrative Sciences Speyer (Germany) 2020 Journal Article 310 62 AI Alignment: A Comprehensive Survey Ji Peking University (China) 2023 Preprint 267 134 Artificial Intelligence Trust, Risk and Security Management (AI TRiSM): Frameworks, Applications, Challenges and Future Research Directions Habbal Karabuk University (Turkiye) 2024 Journal Article 226 226 An Overview of Catastrophic AI Risks Hendrycks Center for AI Safety (USA) 2023 Preprint 225 113 The risks associated with Artificial General Intelligence: A systematic review McLean University Of The Sunshine Coast (Australia) 2023 Journal Article 199 100 Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction Shelby JusTech Lab, Google Research (USA) 2023 Conference Paper 194 97 Model Evaluation for Extreme Risks Shevlane Google Deepmind (UK) 2023 Preprint 173 87 GenAI against humanity: nefarious applications of generative artificial intelligence and large language models Ferrara University of Southern California (USA) 2023 Journal Article 169 85 SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions Zhang Tsinghua University (China) 2023 Preprint 164 82 AGI Safety Literature Review Everitt Australian National University (Australia ) 2018 Preprint 164 23 Sociotechnical Safety Evaluation of Generative AI Systems Weidinger Google Deepmind (UK) 2023 Preprint 139 70 Taxonomy of Pathways to Dangerous Artificial Intelligence Yampolskiy University of Louisville (USA) 2016 Journal Article 138 15 Safety Assessment of Chinese Large Language Models Sun Tsinghua University (China) 2023 Preprint 130 65 Evaluating the Social Impact of Generative AI Systems in Systems and Society Solaiman Hugging Face (USA) 2023 Preprint 126 63 Governance of artificial intelligence: A risk and guideline-based integrative framework Wirtz German University of Administrative Sciences Speyer (Germany) 2022 Journal Article 122 41 Note. ^ collected from Google Scholar on 26th March 2025. Seven organizational/industry reports (AI Verify Foundation, 2023; Allianz Global Corporate & Security, 2018; Electronic Privacy Information Centre, 2023) were not indexed on Google Scholar and are therefore not listed. Causal Taxonomy of AI Risks As a result of a best-fit framework synthesis, which selected, adapted, and iterated on a high-level taxonomy of causal factors (Yampolskiy, 2016) identified in our systematic search, we developed the Causal Taxonomy of AI Risks (Table 2). This taxonomy uses Entity, Intent, and Timing to classify risks in the AI Risk Database. We coded 1390 of the 1612 (86%) risks extracted from our documents against the Causal Taxonomy. 135 did not present sufficient information to assess the Entity, Intent, or Timing, and 87 were discarded as they did not fit our definition of risk. Table 2. Causal Taxonomy of AI Risks Category Level Description Entity Human The risk is caused by a decision or action made by humans AI The risk is caused by a decision or action made by an AI system Other The risk is caused by some other reason or is ambiguous Intent Intentional The risk occurs due to an expected outcome from pursuing a goal Unintentional The risk occurs due to an unexpected outcome from pursuing a goal Other The risk is presented as occurring without clearly specifying the intentionality Timing Pre-deployment The risk occurs before the AI is deployed Post-deployment The risk occurs after the AI model has been trained and deployed Other The risk is presented without a clearly specified time of occurrence The Entity variable captures which, if any, entity is presented as the main cause of the risk. It includes three levels: AI, Human, and Other. When the risk is attributed to AI, it means that the risk arises from decisions or actions made by the AI system itself, such as generating harmful content or disempowering humans. Conversely, when humans are seen as the source, the risks are implied to be due to human actions like choosing poor training data, intentional malicious design, or improper use of AI systems. The "Other" category captures cases where the focal entity is not a human or AI or is ambiguous. For example, “The software development toolchain of LLMs is complex and could bring threats to the developed LLM,” implies that the toolchain could be exploited by humans or AI.