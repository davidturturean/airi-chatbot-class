Repository ID: RID-PREP-0032
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 5. Included documents coded with causal taxonomy
Content Type: statistical_analysis
Word Count: 800

Content:
hold systems or relevant actors accountable for their actions (Kumar & Singh, 2023; Saghiri et al., 2022; Sherman & Eisenberg, 2024; Steimers & Schneider, 2022; Teixeira et al., 2022). In certain sectors, decisions made by AI systems can have profound consequences. In healthcare, AI might be used to diagnose diseases or recommend treatments where incorrect decisions could directly affect patient outcomes. In the military, AI might be used in operations that could impact national security or lead to significant loss of life. In these areas, transparency and accountability of the AI system are particularly pressing issues (Saghiri et al., 2022). 7.5 AI welfare and rights. At a sufficient level of complexity, it is possible that AI systems could acquire the ability to have subjective experiences, particularly pleasure and pain. Some consciousness researchers and philosophers consider the possibility of sentient AI theoretically feasible (Bourget & Chalmers, n.d.; Francken et al., 2022). Where AIs become sentient, they may deserve moral consideration and therefore a range of the rights currently afforded to many forms of human, animal, and environmental life (Meek et al., 2016). Systems may be mistreated or harmed if these rights are not implemented responsibly or we accidentally or intentionally treat AIs as non-sentient where they are sentient. As AI technology advances, it will become more challenging to assess whether an AI has developed the sentience, consciousness, or self-awareness that would grant it moral status. 7.6 Multi-agent risks. AI systems that interact autonomously with each other will form multi-agent systems (Hammond et al., 2025). Multi-agent systems are associated with unique risks beyond those posed by individual AI systems. These risks fall into three main failure modes depending on the objectives of the AI agent and how humans expect systems to behave: Miscoordination occurs when AI agents fail to cooperate effectively despite sharing the same goals. This can be caused by agents choosing incompatible strategies to achieve mutual ends. For example, driving models trained on United States vs Indian cultural conventions for yielding to emergency vehicles block traffic in 77.5% of scenarios despite their shared goal of clearing a path (Hammond et al., 2025). Conflict occurs when AI agents with different but overlapping goals compete in harmful ways. For example, by intensifying competition over shared resources or escalating military tensions. They could also make novel forms of conflict possible through more advanced and accessible methods of coercion and extortion. Collusion occurs when undesired cooperation emerges between AI agents, allowing them to circumvent safeguards or manipulate markets. For example, AI systems may be able to develop hidden communication channels without explicit training. (Motwani et al., 2024) show that advanced LLMs can covertly exchange steganographic messages undetected by equally capable oversight systems, using natural language cues and shared context. In market settings, AI systems may learn to collude because it is the most rewarding strategy. A range of risk factors contribute to miscoordination, conflict and collusion: information asymmetries between agents, network effects where small changes cascade through interconnected systems, selection pressures that reward problematic behaviours, destabilizing dynamics like feedback loops and unpredictability, commitment problems that prevent trust, emergent agency where new capabilities or goals arise at the collective level, and multi-agent security vulnerabilities. Unlike single-agent risks, multi-agent risks involve interactions across networks of agents that may be individually safe but collectively dangerous, and these risks could increase as AI systems become more numerous, autonomous, and capable of adapting to each other. Most common domains of AI risks Table 7 shows how the database of AI risks was coded against each subdomain and domain in the Domain Taxonomy, and the proportion of documents that presented a risk for each subdomain and domain. Table 7. AI Risk Database coded with Domain Taxonomy Domain / Subdomain Percentage of risks Percentage of documents 1 Discrimination & Toxicity 15% 70% 1.1 Unfair discrimination and misrepresentation 6% 63% 1.2 Exposure to toxic content 8% 33% 1.3 Unequal performance across groups 1% 17% 2 Privacy & Security 12% 68% 2.1 Compromise of privacy by leaking or correctly inferring sensitive information 5% 59% 2.2 AI system security vulnerabilities and attacks 7% 37% 3 Misinformation 4% 46% 3.1 False or misleading information 3% 37% 3.2 Pollution of information ecosystem and loss of consensus reality 1% 16% 4 Malicious actors & Misuse 16% 71% 4.1 Disinformation, surveillance, and influence at scale 6% 51% 4.2 Cyberattacks, weapon development or use, and mass harm 5% 57% 4.3 Fraud, scams, and targeted manipulation 5% 40% 5 Human-Computer Interaction 7% 49% 5.1 Overreliance and unsafe use 4% 32% 5.2 Loss of human agency and autonomy 3% 33% 6 Socioeconomic & Environmental 19% 76% 6.1 Power centralization and unfair distribution of benefits 4% 41% 6.2 Increased inequality and decline in employment quality 3% 41% 6.3 Economic and cultural devaluation of human effort 2% 35% 6.4 Competitive dynamics 1% 21%