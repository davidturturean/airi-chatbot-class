Repository ID: RID-PREP-0004
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: limitations
Word Count: 793

Content:
AI Risk Database Coded With Causal Taxonomy: Entity, Intent, Timing Category Level Proportion Entity Human 39% AI 41% Other 20% Intent Intentional 34% Unintentional 35% Other 31% Timing Pre-deployment 13% Post-deployment 62% Other 25% Note. Totals may not match due to rounding. As shown in Table D, the risk domains that were covered the most in previous documents were: AI system safety, failures & limitations - covered in 75% of documents. Socioeconomic & environmental harms - covered in 76% of documents. Discrimination & toxicity - covered in 70% of documents. Human-computer interaction (49%) and Misinformation (46%) were less frequently discussed. No document discussed risks from all 24 subdomains; the highest coverage was 19 out of 24 subdomains (70%; Gabriel et al., 2024). On average, documents mentioned 8 out of 24 (33%) of the AI risk subdomains, with a range of 1 to 19 subdomains mentioned. See Table 9 in the body of the paper for a full breakdown of subdomain coverage by paper. Some risk subdomains were discussed much more frequently than others, such as: Exposure to toxic content (8% of risks). AI pursuing its own goals in conflict with human goals or values (7% of risks). Lack of capability or robustness (9% of risks). AI system security vulnerabilities and attacks (7% of risks) Some risk subdomains are relatively underexplored, such as: AI welfare and rights (<1% of risks). Pollution of the information ecosystem and loss of consensus reality (1% of risks). Competitive dynamics (1% of risks). Table D. AI Risk Database Coded With Domain Taxonomy Domain / Subdomain Percentage of risks Percentage of documents 1 Discrimination & Toxicity 15% 70% 1.1 Unfair discrimination and misrepresentation 6% 63% 1.2 Exposure to toxic content 8% 33% 1.3 Unequal performance across groups 1% 17% 2 Privacy & Security 12% 68% 2.1 Compromise of privacy by leaking or correctly inferring sensitive information 5% 59% 2.2 AI system security vulnerabilities and attacks 7% 37% 3 Misinformation 4% 46% 3.1 False or misleading information 3% 37% 3.2 Pollution of information ecosystem and loss of consensus reality 1% 16% 4 Malicious actors & Misuse 16% 71% 4.1 Disinformation, surveillance, and influence at scale 6% 51% 4.2 Cyberattacks, weapon development or use, and mass harm 5% 57% 4.3 Fraud, scams, and targeted manipulation 5% 40% 5 Human-Computer Interaction 7% 49% 5.1 Overreliance and unsafe use 4% 32% 5.2 Loss of human agency and autonomy 3% 33% 6 Socioeconomic & Environmental 19% 76% 6.1 Power centralization and unfair distribution of benefits 4% 41% 6.2 Increased inequality and decline in employment quality 3% 41% 6.3 Economic and cultural devaluation of human effort 2% 35% 6.4 Competitive dynamics 1% 21% 6.5 Governance failure 4% 30% 6.6 Environmental harm 4% 38% 7 AI system safety, failures, & limitations 26% 75% 7.1 AI pursuing its own goals in conflict with human goals or values 7% 48% 7.2 AI possessing dangerous capabilities 4% 25% 7.3 Lack of capability or robustness 9% 56% 7.4 Lack of transparency or interpretability 3% 33% 7.5 AI welfare and rights <1% 3% 7.6 Multi-agent risks 3% 5% Note. Domain totals may not match subdomain sums due to rounding and domain-level coding of some risks. How to use the AI Risk Repository Our Database is free to copy and use. The Causal and Domain Taxonomies can be used separately to filter this database to identify specific risks, for instance, those focused on risks occurring pre-deployment or post-deployment or related to a specific risk domain such as Misinformation. The Causal and Domain Taxonomies can be used together to understand how each causal factor (i.e., entity, intent, and timing) relates to each risk domain or subdomain. For example, a user could filter for Discrimination & toxicity risks and use the causal filter to identify the intentional and unintentional variations of this risk from different sources. Similarly, they could differentiate between sources which examine Discrimination & toxicity risks where AI is trained on toxic content pre-deployment, and those which examine where AI inadvertently causes harm post-deployment by showing toxic content. We discuss some additional use cases below; see the full paper for more detail. General: Onboarding new people to the field of AI risks. A foundation to build on for complex projects. Informing the development of narrower or more specific taxonomies. (e.g., systemic risks, or EU-related misinformation risks). Using the taxonomy for prioritization (e.g., with expert ratings), synthesis (e.g, for a review) or comparison (e.g., exploring public concern across domains). Identifying underrepresented areas (e.g., AI welfare and rights). Specific: Policymakers: Regulation and shared standard development. Auditors: Developing AI system audits and standards. Academics: Identifying research gaps and developing education and training. Industry: Internally evaluating and preparing for risks, and developing related strategy, education and training. How to engage Access the Repository via our website: airisk.mit.edu