Repository ID: RID-00100
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
search_medium_priority: 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation
intent: 2 - Unintentional
rid: RID-00100
sheet: AI Risk Database v3
scqa_question: What are the implications of this risk?
scqa_answer: "\nEntity: 2 - AI\nIntent: 2 - Unintentional\nTiming: 2 - Post-deployment
title: Navigating the Landscape of AI Ethics and Responsibility
scqa_content_type: case_study
scqa_situation: Title: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1
risk_category: Broken systems
subdomain: 1.1 > Unfair discrimination and misrepresentation
search_low_priority: AI Risk Database v3 ai_risk_entry
search_high_priority: Navigating the Landscape of AI Ethics and Responsibility 1. Discrimination & Toxicity Broken systems
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_confidence: 1.0
row: 77
entity: 2 - AI
domain: 1. Discrimination & Toxicity
scqa_complication: riables, like race or gender, but there is no transparency to this effect, making them impossible to challenge. These situations are typically only ident
search_all_fields: Navigating the Landscape of AI Ethics and Responsibility 1. Discrimination & Toxicity Broken systems 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation AI Risk Database v3 ai_risk_entry
specific_domain: 1.1 > Unfair discrimination and misrepresentation
content_preview: Title: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 > Unfair discrimination and misrepresentation\nRisk Category: Broken systems\nDescription: "These are the most mentioned cases. They refer to situations where the algorithm or the t...
timing: 2 - Post-deployment
file_type: ai_risk_entry

Content:
Title: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 > Unfair discrimination and misrepresentation\nRisk Category: Broken systems\nDescription: "These are the most mentioned cases. They refer to situations where the algorithm or the training data lead to unreliable outputs. These systems frequently assign disproportionate weight to some variables, like race or gender, but there is no transparency to this effect, making them impossible to challenge. These situations are typically only identified when regulators or the press examine the systems under freedom of information acts. Nevertheless, the damage they cause to people’s lives can be dramatic, such as lost homes, divorces, prosecution, or incarceration. Besides the inherent technical shortcomings, auditors have also pointed out “insufficient coordination” between the developers of the systems and their users as a cause for ethical considerations to be neglected. This situation raises issues about the education of future creators of AI-infused systems, not only in terms of technical competence (e.g., requirements, algorithms, and training) but also ethics and responsibility. For example, as autonomous vehicles become more common, moral dilemmas regarding what to do in potential accident situations emerge, as evidenced in this MIT experiment. The decisions regarding how the machines should act divides opinions and requires deep reflection and maybe regulation."\nEntity: 2 - AI\nIntent: 2 - Unintentional\nTiming: 2 - Post-deployment