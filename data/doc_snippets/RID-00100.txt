Repository ID: RID-00100
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
search_high_priority: Navigating the Landscape of AI Ethics and Responsibility 1. Discrimination & Toxicity Broken systems
scqa_situation: Title: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1
timing: 2 - Post-deployment
risk_category: Broken systems
content_preview: Title: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 > Unfair discrimination and misrepresentation\nRisk Category: Broken systems\nDescription: "These are the most mentioned cases. They refer to situations where the algorithm or the t...
entity: 2 - AI
scqa_content_type: case_study
row: 77
scqa_answer: "\nEntity: 2 - AI\nIntent: 2 - Unintentional\nTiming: 2 - Post-deployment
file_type: ai_risk_entry
title: Navigating the Landscape of AI Ethics and Responsibility
sheet: AI Risk Database v3
search_medium_priority: 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation
rid: RID-00100
search_all_fields: Navigating the Landscape of AI Ethics and Responsibility 1. Discrimination & Toxicity Broken systems 1.1 > Unfair discrimination and misrepresentation 1.1 > Unfair discrimination and misrepresentation AI Risk Database v3 ai_risk_entry
intent: 2 - Unintentional
scqa_question: What are the implications of this risk?
scqa_complication: riables, like race or gender, but there is no transparency to this effect, making them impossible to challenge. These situations are typically only ident
subdomain: 1.1 > Unfair discrimination and misrepresentation
specific_domain: 1.1 > Unfair discrimination and misrepresentation
domain: 1. Discrimination & Toxicity
scqa_confidence: 1.0
search_low_priority: AI Risk Database v3 ai_risk_entry

Content:
Title: Navigating the Landscape of AI Ethics and Responsibility\nDomain: 1. Discrimination & Toxicity\nSub-domain: 1.1 > Unfair discrimination and misrepresentation\nRisk Category: Broken systems\nDescription: "These are the most mentioned cases. They refer to situations where the algorithm or the training data lead to unreliable outputs. These systems frequently assign disproportionate weight to some variables, like race or gender, but there is no transparency to this effect, making them impossible to challenge. These situations are typically only identified when regulators or the press examine the systems under freedom of information acts. Nevertheless, the damage they cause to people’s lives can be dramatic, such as lost homes, divorces, prosecution, or incarceration. Besides the inherent technical shortcomings, auditors have also pointed out “insufficient coordination” between the developers of the systems and their users as a cause for ethical considerations to be neglected. This situation raises issues about the education of future creators of AI-infused systems, not only in terms of technical competence (e.g., requirements, algorithms, and training) but also ethics and responsibility. For example, as autonomous vehicles become more common, moral dilemmas regarding what to do in potential accident situations emerge, as evidenced in this MIT experiment. The decisions regarding how the machines should act divides opinions and requires deep reflection and maybe regulation."\nEntity: 2 - AI\nIntent: 2 - Unintentional\nTiming: 2 - Post-deployment