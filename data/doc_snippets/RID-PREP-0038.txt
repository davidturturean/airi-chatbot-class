Repository ID: RID-PREP-0038
Source: AI_Risk_Repository_Preprint.docx
Section: Domain Taxonomy
Content Type: limitations
Word Count: 792

Content:
human goals or values, 7.2 AI possessing dangerous capabilities, 7.5 AI welfare and rights, 6.4 Competitive dynamics. This implies that these domains of risk may emerge or occur multiple times during development and deployment. Entity x Intent causal factors by each domain of AI risk As a preliminary investigation and demonstration, we explore here how multiple variables from the Causal Taxonomy can be combined to provide additional insights about risks in the Domain Taxonomy. This investigation is intended to illustrate how more in-depth assessment is possible using the AI Risk Database by selecting and combining causal factors and risk domains. Table 11 compares risk domains based on their presentation of Entity and Intent as causal factors. It shows that risks in the Malicious actors & misuse domain are consistently presented as involving the same Entity (i.e., Humans) and Intent (i.e., Intentional), and that other risks such as 1.1 Unfair discrimination and misrepresentation, 1.3 Unequal performance across groups, and 3.1 False or misleading information are generally presented as Unintentionally caused by an AI system. In contrast, other risk domains and subdomains show less consistency or coherence in what Entity is responsible and in the role of Intentionality. For example, 2.1 Compromise of privacy, 5.1 Overreliance and unsafe use, 5.2 Loss of human agency and autonomy, and 6.5 Governance failure are all presented as due to both Human and AI entities, and both Intentional and Unintentional action. This suggests that the representation of these risks in the literature is more contested and less coherent or consistent or that these subdomains of risk are more complex than others. Table 11. AI Risk Database Coded With Causal Taxonomy and Domain Taxonomy: Entity X Intent Entity x Intent Human AI Other Domain / Subdomain Intent. Unintent. Other Intent. Unintent. Other Intent. Unintent. Other 1 Discrimination & toxicity 1.1 Unfair discrimination and misrepresentation 3% 9% 3% 64% 7% 7% 8% 1.2 Exposure to toxic content 5% 5% 2% 6% 21% 55% 8% 1.3 Unequal performance across groups 6% 19% 56% 13% 6% 2 Privacy & security 2.1 Compromise of privacy by obtaining, leaking or correctly inferring sensitive information 12% 9% 6% 2% 39% 17% 5% 11% 2.2 AI system security vulnerabilities and attacks 67% 6% 4% 6% 6% 3% 7% 3 Misinformation 3.1 False or misleading information 2% 5% 66% 20% 2% 5% 3.2 Pollution of information ecosystem and loss of consensus reality 6% 22% 22% 22% 28% 4 Malicious actors & misuse 4.1 Disinformation, surveillance, and influence at scale 66% 4% 8% 4% 16% 3% 4.2 Cyberattacks, weapon development or use, and mass harm 75% 1% 9% 1% 4% 1% 7% 4.3 Fraud, scams, and targeted manipulation 75% 4% 1% 4% 6% 9% 5 Human-computer interaction 5.1 Overreliance and unsafe use 41% 4% 8% 4% 10% 4% 16% 14% 5.2 Loss of human agency and autonomy 8% 16% 5% 5% 13% 8% 3% 11% 32% 6 Socioeconomic & environmental harms 6.1 Power centralization and unfair distribution of benefits 38% 19% 15% 4% 4% 19% 6.2 Increased inequality and decline in employment quality 30% 5% 18% 9% 7% 16% 2% 14% 6.3 Economic and cultural devaluation of human effort 37% 3% 10% 17% 20% 13% 6.4 Competitive dynamics 42% 16% 11% 11% 16% 5% 6.5 Governance failure 4% 36% 18% 11% 7% 7% 18% 6.6 Environmental harm 10% 17% 4% 40% 8% 13% 6% 7 AI system safety, failures, and limitations 7.1 AI pursuing its own goals in conflict with human goals or values 1% 4% 3% 49% 7% 17% 2% 16% 7.2 AI possessing dangerous capabilities 2% 6% 67% 9% 15% 2% 7.3 Lack of capability or robustness 1% 16% 2% 4% 48% 14% 1% 6% 8% 7.4 Lack of transparency or interpretability 8% 13% 40% 13% 8% 20% 7.5 AI welfare and rights 33% 33% 33% 7.6 Multi-agent risks 2% 4% 22% 35% 13% 4% 7% 13% Note. The most common Entity x Intent causal factor is highlighted for each subdomain. Discussion This paper is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. In doing this, we use two taxonomies to classify these risks: the Causal Taxonomy of AI Risks for understanding how, when, or why risks from AI may emerge, and the Domain Taxonomy of AI Risks to classify commonly discussed hazards and harms associated with AI. The database and taxonomies are then used to evaluate the curated literature and provide a range of insights into the state of this literature. In this section, we discuss i) insights into the “AI risk landscape,” ii) specific implications for policymaker, auditor, academic research, and industry audiences, and iii) limitations and opportunities for future research. Insights into the “AI risk landscape”