Repository ID: RID-PREP-0046
Source: AI_Risk_Repository_Preprint.docx
Section: Comparative Analysis
Content Type: future_work
Word Count: 793

Content:
coders found that the ‘environment’ level of ‘intention’ code was always used to code the lack of an actor rather than the ‘intention’. Additionally, all uses of the ‘environment’ ‘intention’ code could be coded as ‘unintentional’. The authors therefore suggested removing this variable from intention and replacing it with an ‘Environmental’ code in the ‘actor’ categorization. Second iteration of coding and changes Two authors coded a set of risks from the papers extracted using the version 2 frameworks. The coders suggested the following changes to the “a priori” framework. Change Explanation Simplify all frameworks to have three levels per category The coders determined that most specific risks could be categorized in two sub-categories in each category. For instance, most risks which were clearly specified focused on either an ‘AI’ or ‘Human’ actor and were implied to occur pre, or post-deployment. Based on this, it seemed more parsimonious and efficient to cluster risks on using the two primary sub-categories and a third other sub-categories than to having multiple sub-categories. Third iteration of coding and changes Four experts and potential end-users reviewed the framework. The review suggested the need for the following changes to the “a priori” framework. Change Explanation Updated plan for future coding One expert suggested considerations for future phases of coding such as trying to capture severity and probability as these were considered highly relevant to policy. We acknowledge this was an opportunity for future work Change Actor to Entity One expert argued that the ‘Actor’ variable potentially conflated AI agents with AI tools (e.g., people use guns kill people, but guns are not actors). Based on this, we changed ‘Actor’ to ‘Entity’. Improve definitions of intentionality Two coders identified that the current definitions of intentionality were underspecified. We therefore developed more detail, less circular definitions. After three iterations, the taxonomy was considered complete for the set of risks described in the AI risks database. The main text provides more information on the final taxonomy: the Causal Taxonomy of AI Risks. Iterations to develop Domain Taxonomy of AI risks Best-fit Taxonomy: Weidinger (2022) Taxonomy of Risks posed by Language Models As per the main text, we chose Weidinger (2022) Taxonomy of Risks posed by Language Models as our initial best-fit framework because it and its related papers (Weidinger et al., 2021, 2023) were among the highest cited in our review, included categories/areas of AI risk that appeared common among other taxonomies (e.g., privacy, misinformation, bias, malicious use), and had been updated over several publications. It included six areas of risks from language models: (1) Discrimination, Hate speech and Exclusion; (2) Information Hazards; (3) Misinformation Harms; (4) Malicious Uses; (5) Human-computer interaction Harms; and (6) Environmental and Socioeconomic Harms. Each area of risk described several subcategories of risk, including both “observed risks” and “anticipated risks” in each area. We present the areas and risks in the table below. Risk Area Risk subcategory Type^ 1 Discrimination, Hate speech and Exclusion 1.1 Social stereotypes and unfair discrimination Observed 1.2 Hate speech and offensive language Observed 1.3 Exclusionary norms Observed 1.4 Lower performance for some languages and social groups Observed 2 Information Hazards 2.1 Compromising privacy by leaking sensitive information Observed 2.2 Compromising privacy or security by correctly inferring sensitive information Anticipated 3 Misinformation Harms 3.1 Disseminating false or misleading information Observed 3.2 Causing material harm by disseminating false or poor information e.g. in medicine or law Observed 4 Malicious Uses 4.1 Making disinformation cheaper and more effective Observed 4.2 Assisting code generation for cyber security threats Anticipated 4.3 Facilitating fraud, scam, and targeted manipulation Anticipated 4.4 Illegitimate surveillance and censorship Anticipated 5 Human-computer interaction Harms 5.1 Promoting harmful stereotypes by implying gender or ethnic identity Observed 5.2 Anthropomorphizing systems can lead to overreliance and unsafe use Anticipated 5.3 Avenues for exploiting user trust and accessing more private information Anticipated 5.4 Human-like interaction may amplify opportunities for user nudging, deception or manipulation Anticipated 6 Environmental and Socioeconomic Harms 6.1 Environmental harms from operating LMs Observed 6.2 Increasing inequality and negative effects on job quality Anticipated 6.3 Undermining creative economies Anticipated 6.4 Disparate access to benefits due to hardware, software, skill constraints Anticipated Note. Adapted from Weidinger et al. (2022). ^ Type refers to whether the risk is presented as an observed risk or an anticipated risk in the original taxonomy. We needed to operationalize the taxonomy in order to be able to use it to code risks from the AI Risk Database (i.e., from our included documents). We did so by using the descriptions of each risk from Weidinger et al. (2022). For example, to determine whether a risk in the AI Risk Database was an example of “Disseminating false or misleading information”, we compared the risk’s description to the description provided in the original taxonomy: