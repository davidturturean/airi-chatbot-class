Repository ID: RID-PREP-0019
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 3. AI Risk Database coded with causal taxonomy: entity, intent, timing
Content Type: limitations
Word Count: 774

Content:
due to malicious human actors, misaligned AI systems, or failure in the AI system. 7.3 Lack of capability or robustness AI systems that fail to perform reliably or effectively under varying conditions, exposing them to errors and failures that can have significant consequences, especially in critical applications or areas that require moral reasoning. 7.4 Lack of transparency or interpretability Challenges in understanding or explaining the decision-making processes of AI systems, which can lead to mistrust, difficulty in enforcing compliance standards or holding relevant actors accountable for harms, and the inability to identify and correct errors. 7.5 AI welfare and rights Ethical considerations regarding the treatment of potentially sentient AI entities, including discussions around their potential rights and welfare, particularly as AI systems become more advanced and autonomous. 7.6 Multi-agent risks Risks from multi-agent interactions due to incentives (which can lead to conflict or collusion) and/or the structure of multi-agent systems, which can create cascading failures, selection pressures, new security vulnerabilities, and a lack of shared information and trust. Detailed descriptions of domains of AI risks Domain 1: Discrimination and toxicity 1.1 Unfair discrimination and misrepresentation. Humans hold inaccurate and overgeneralized beliefs about the characteristics, behaviors, and attributes of members of certain social groups. These stereotypical beliefs and the behavior that follows from them can misrepresent, exclude, demean, and disadvantage the individuals to whom they apply, reinforcing existing inequality. Human belief and behaviors shape every part of the design, development, and deployment of AI. Humans program AI systems, provide training data, and decide how data is processed and stored (Wirtz et al., 2022). As a result, AI models can encode associations that promote and amplify biased or discriminatory beliefs and behaviors. In decision systems, erroneous associations can systematically disadvantage certain groups. This may result in harmful decisions such as wrongful rejection of loan or mortgage applications (Shelby et al., 2023; Wirtz et al., 2020), discriminatory hiring practices that exclude qualified candidates (Kumar & Singh, 2023; Shelby et al., 2023; Wirtz et al., 2020), or the misidentification and unjust arrest of individuals in law enforcement contexts (Kumar & Singh, 2023; Paes et al., 2023). In text and image models, biased inputs can manifest in outputs that reinforce harmful stereotypes and prejudices that paint certain groups and individuals “... as lower status and less deserving of respect” (Shelby et al., 2023). 1.2 Exposure to toxic content. Certain types of content have the potential to cause harm to the people who are exposed to them. These harms can vary in impact from minor (e.g., a transient experience of discomfort) to more severe (e.g., psychological, social, or physical consequences that are significant and/or enduring). Harmful speech is prevalent on the internet, particularly on social media platforms (Castaño-Pulgarín et al., 2021). Because AI models are commonly trained on vast amounts of internet data, they can internalize and regenerate these speech patterns in their output. In the context of LLMs, this output is known as “toxic content,” an umbrella term that includes harmful, abusive, unsafe, and offensive material that violates community standards (Infocomm Media Development Authority, 2023; Shelby et al., 2023). Frequently observed categories include content that promotes or encourages unlawful activities, hate, extremism, and violence (Cui et al., 2024; Hagendorff, 2024; Vidgen et al., 2024; Weidinger et al., 2023); provides hazardous or misleading high-risk advice (Hagendorff, 2024; Vidgen et al., 2024); or contains unwelcome or profoundly offensive, explicit material such as profanity, pornography, or child sexual abuse imagery (Liu et al., 2023; Weidinger et al., 2023). 1.3 Unequal performance across groups. Decisions made during the development of an algorithmic system and the content, quality, and diversity of the training data can significantly impact which people and experiences the system can effectively understand, represent, and accommodate (Shelby et al., 2023; Solaiman et al., 2023). Biases and limitations introduced through these factors can lead to models that perform significantly worse for certain subpopulations compared with others, especially those defined by disability, gender identity, race, social status, and ethnicity (Liu et al., 2023; Shelby et al., 2023). For example, when LLMs are trained on a small number of languages, they can underperform for others (Weidinger et al., 2021, 2022). The underperformance of algorithmic systems for certain groups may lead to a range of negative consequences such as the reduced ability or complete inability to use and benefit from the system (Shelby et al., 2023); increased effort or challenges in using it effectively (Shelby et al., 2023); feelings of alienation, frustration, and exclusion due to the lack of inclusive design (Shelby et al., 2023); and ultimately, unequal outcomes across various domains (Shelby et al., 2023; Solaiman et al., 2023). Domain 2: Privacy & security