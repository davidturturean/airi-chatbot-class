Repository ID: RID-PREP-0021
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 3. AI Risk Database coded with causal taxonomy: entity, intent, timing
Content Type: comparative_analysis
Word Count: 715

Content:
altering the model’s learning process or extracting its data. They include perturbations designed to deceive models into incorrect outputs, extraction attacks to steal model insights, and poisoning attacks to alter model behavior (Cui et al., 2024; Gabriel et al., 2024; Liu et al., 2023). Instruction-based attacks manipulate the way the model handles and responds to inputs (Hagendorff, 2024; Liu et al., 2023; Sun et al., 2023). Attackers deliberately craft prompts to induce models to produce biased or unsafe outputs (a.k.a. ‘jailbreaking’). This manipulation directly targets the operational aspects of AI systems with the intent to cause harm. Domain 3: Misinformation 3.1 False or misleading information. LLMs can sometimes generate content that is factually incorrect, misleading, poorly researched, or unintelligible (Cunha & Estima, 2023; Deng et al., 2023; Electronic Privacy Information Centre, 2023; Hagendorff, 2024; Nah et al., 2023; Shelby et al., 2023). Risks in this category occur accidentally and not as a result of humans intentionally trying to cause harm, as is the case with disinformation (Liu et al., 2023; Weidinger et al., 2022). Common sources of AI misinformation include noisy training data (Cui et al., 2024; Liu et al., 2023), sampling strategies that introduce randomness (Cui et al., 2024), outdated knowledge bases (Liu et al., 2023), and fine-tuning processes that encourage sycophantic behavior (Cui et al., 2024). Incorrect and misleading information generated by LLMs can result in a range of actual and anticipated negative outcomes. Individuals exposed to false information may form inaccurate beliefs and perceptions. This undermines their autonomy and ability to make free and informed choices (Weidinger et al., 2021, 2022). Where inaccuracies in LLM predictions influence an individual’s decisions and actions, the individual may experience indirect physical, emotional, or material harms (Gabriel et al., 2024; Weidinger et al., 2022) especially but not exclusively in high-stakes domains such as mental health (Sun et al., 2023; Z. Zhang et al., 2023), physical health (Nah et al., 2023; Sun et al., 2023; Z. Zhang et al., 2023), law (Weidinger et al., 2022), and finance (Vidgen et al., 2024). For example, an LLM that offers misleading information about medical drug use may cause a consumer to harm themselves or others (Sun et al., 2023). 3.2 Pollution of information ecosystems and loss of consensus reality. This subcategory covers the diverse effects of AI-driven personalisation and content-generation technologies on the information landscape. As AI systems become more adept at tailoring content to individual preferences, they risk creating “filter bubbles” (Hogenhout, 2021). These are informational cocoons where individuals are predominantly exposed to news and opinions that align with their pre-existing beliefs. AI-driven filter bubbles are likely to be more pervasive and intense than those driven by traditional internet browsing and recommendation algorithms: They adapt to individual preferences in a more sophisticated manner (e.g., through reinforcement learning and analysis of user behavioral data) (Gabriel et al., 2024), integrate seamlessly into daily life, and are more opaque. An overreliance on hyper-personalized AI information sources could lead to a “splintering” of shared reality, where different groups of people have vastly different understandings of what is true or important (Gabriel et al., 2024; Hogenhout, 2021). This is likely to be exacerbated by the proliferation of AI-enabled content generation technologies that spread misinformation at higher rates (e.g., clickbait), potentially making consumers generally distrustful of information and important institutions (Electronic Privacy Information Centre, 2023; Gabriel et al., 2024; Hendrycks & Mazeika, 2022). A shared sense of reality is fundamental to social solidarity. Where societal bonds are weakened, individuals may become more hostile towards opposing views. This can hinder constructive dialogue on critical collective issues like climate change and public health (Gabriel et al., 2024). Domain 4: Malicious actors and misuse 4.1 Disinformation, surveillance, and influence at scale. Advances in AI have made powerful dual-use technologies like voice cloning, deep fakes, content generation, and data-gathering tools cheaper, more efficient, and easier to use (Cunha & Estima, 2023). With modest hardware requirements, these technologies are now within the reach of a broader group of users, including those with malicious intent. Disinformation is already a serious issue (Hendrycks et al., 2023) and involves the deliberate propagation of false or misleading information, usually with the intent to cause harm, influence behavior, or achieve a financial or political advantage (Electronic Privacy Information Centre, 2023; Infocomm Media Development Authority, 2023).