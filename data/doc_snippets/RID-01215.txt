Repository ID: RID-01215
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
content_preview: Title: Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile\nRisk Category: Harmful Bias or Homogenization\nAdditional Evidence: "Harmful bias in GAI systems can also lead to harms via disparities between how a model performs for different subgroups or langua...
domain: Unspecified
entity: 
file_type: ai_risk_entry
intent: 
rid: RID-01215
risk_category: Harmful Bias or Homogenization
row: 1192
scqa_answer: s than if no GAI system were used. Disparate or reduced performance for lower-resource languages also presents challenges to model adoption, inclusion
scqa_complication: ). Such disparities can contribute to discriminatory decision-making or amplification of existing societal biases. In addition, GAI systems may be inappr
scqa_confidence: 1.0
scqa_content_type: risk_description
scqa_question: What are the implications of this risk?
scqa_situation: omogenization\nAdditional Evidence: "Harmful bias in GAI systems can also lead to harms via disparities between how a model performs for different subgroups or lang
search_all_fields: Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile Unspecified Harmful Bias or Homogenization Unspecified AI Risk Database v3 ai_risk_entry
search_high_priority: Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile Unspecified Harmful Bias or Homogenization
search_low_priority: AI Risk Database v3 ai_risk_entry
search_medium_priority: Unspecified
sheet: AI Risk Database v3
specific_domain: Unspecified
subdomain: 
timing: 
title: Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile

Content:
Title: Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile\nRisk Category: Harmful Bias or Homogenization\nAdditional Evidence: "Harmful bias in GAI systems can also lead to harms via disparities between how a model performs for different subgroups or languages (e.g., an LLM may perform less well for non-English languages or certain dialects). Such disparities can contribute to discriminatory decision-making or amplification of existing societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly across all subgroups, which could leave the groups facing underperformance with worse outcomes than if no GAI system were used. Disparate or reduced performance for lower-resource languages also presents challenges to model adoption, inclusion, and accessibility, and may make preservation of endangered languages more difficult if GAI systems become embedded in everyday processes that would otherwise have been opportunities to use these languages."