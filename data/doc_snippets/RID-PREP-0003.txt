Repository ID: RID-PREP-0003
Source: AI_Risk_Repository_Preprint.docx
Section: Domain Taxonomy
Content Type: limitations
Word Count: 744

Content:
of human agency and autonomy Delegating by humans of key decisions to AI systems, or AI systems that make decisions that diminish human control and autonomy. Both can potentially lead to humans feeling disempowered, losing the ability to shape a fulfilling life trajectory, or becoming cognitively enfeebled. 6 Socioeconomic & environmental harms 6.1 Power centralization and unfair distribution of benefits AI-driven concentration of power and resources within certain entities or groups, especially those with access to or ownership of powerful AI systems, leading to inequitable distribution of benefits and increased societal inequality. 6.2 Increased inequality and decline in employment quality Social and economic inequalities caused by widespread use of AI, such as by automating jobs, reducing the quality of employment, or producing exploitative dependencies between workers and their employers. 6.3 Economic and cultural devaluation of human effort AI systems capable of creating economic or cultural value through reproduction of human innovation or creativity (e.g., art, music, writing, coding, invention), destabilizing economic and social systems that rely on human effort. The ubiquity of AI-generated content may lead to reduced appreciation for human skills, disruption of creative and knowledge-based industries, and homogenization of cultural experiences. 6.4 Competitive dynamics Competition by AI developers or state-like actors in an AI “race” by rapidly developing, deploying, and applying AI systems to maximize strategic or economic advantage, increasing the risk they release unsafe and error-prone systems. 6.5 Governance failure Inadequate regulatory frameworks and oversight mechanisms that fail to keep pace with AI development, leading to ineffective governance and the inability to manage AI risks appropriately. 6.6 Environmental harm The development and operation of AI systems that cause environmental harm through energy consumption of data centers or the materials and carbon footprints associated with AI hardware. 7 AI system safety, failures & limitations 7.1 AI pursuing its own goals in conflict with human goals or values AI systems that act in conflict with ethical standards or human goals or values, especially the goals of designers or users. These misaligned behaviors may be introduced by humans during design and development, such as through reward hacking and goal misgeneralisation, and may result in AI using dangerous capabilities such as manipulation, deception, or situational awareness to seek power, self-proliferate, or achieve other goals. 7.2 AI possessing dangerous capabilities AI systems that develop, access, or are provided with capabilities that increase their potential to cause mass harm through deception, weapons development and acquisition, persuasion and manipulation, political strategy, cyber-offense, AI development, situational awareness, and self-proliferation. These capabilities may cause mass harm due to malicious human actors, misaligned AI systems, or failure in the AI system. 7.3 Lack of capability or robustness AI systems that fail to perform reliably or effectively under varying conditions, exposing them to errors and failures that can have significant consequences, especially in critical applications or areas that require moral reasoning. 7.4 Lack of transparency or interpretability Challenges in understanding or explaining the decision-making processes of AI systems, which can lead to mistrust, difficulty in enforcing compliance standards or holding relevant actors accountable for harms, and the inability to identify and correct errors. 7.5 AI welfare and rights Ethical considerations regarding the treatment of potentially sentient AI entities, including discussions around their potential rights and welfare, particularly as AI systems become more advanced and autonomous. 7.6 Multi-agent risks Risks from multi-agent interactions, due to incentives (which can lead to conflict or collusion) and/or the structure of multi-agent systems, which can create cascading failures, selection pressures, new security vulnerabilities, and a lack of shared information and trust. What we found As shown in Table C, most of the risks (41%) were presented as caused by AI systems rather than humans (39%), and as emerging after the AI model has been trained and deployed (62%) rather than before (13%). A similar proportion of risks were presented as intentional (34%) and unintentional (35%) Table C. AI Risk Database Coded With Causal Taxonomy: Entity, Intent, Timing Category Level Proportion Entity Human 39% AI 41% Other 20% Intent Intentional 34% Unintentional 35% Other 31% Timing Pre-deployment 13% Post-deployment 62% Other 25% Note. Totals may not match due to rounding. As shown in Table D, the risk domains that were covered the most in previous documents were: AI system safety, failures & limitations - covered in 75% of documents. Socioeconomic & environmental harms - covered in 76% of documents. Discrimination & toxicity - covered in 70% of documents. Human-computer interaction (49%) and Misinformation (46%) were less frequently discussed.