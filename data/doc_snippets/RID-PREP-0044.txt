Repository ID: RID-PREP-0044
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: future_work
Word Count: 793

Content:
landscape and the relative attention devoted to specific AI risk domains. Our work makes several contributions. To the best of our knowledge, this is the first attempt to comprehensively review and synthesize AI risk frameworks into a database, develop taxonomies from the database, and create supporting products to use and improve them. The resultant AI Risk Repository is uniquely comprehensive and extensible; it includes and indexes a large range of risks and sources and makes all data accessible for further adaptation and use. This makes our Repository uniquely positioned to support a wide range of future activities and research endeavors. We do not expect our Repository to be universally accepted. It cannot, and will not, resolve relevant disagreements on the finer points of how to conceptualize and categorize risks from AI or how to prioritize between risks. However, it may reduce illusory disagreements and make any necessary disagreement easier to manage and adjudicate. Similarly, we do not expect our Repository to be fit-for-purpose for many use cases in research, policy, or practice. However, it may enable activities and research that would otherwise be unviable and provide a robust foundation for further development and specialization. The risks of AI are poised to become increasingly common and pressing, and research and efforts to understand and address these risks must be able to keep pace with advancements in development and deployment of AI systems. We hope that our living, common frame of reference will help these endeavors to be more accessible, incremental, and successful. Appendices Appendix A: Iterative development of Causal Taxonomy and Domain Taxonomy As described in Figure 2 in the main text, we followed a best-fit framework synthesis approach to develop the Causal and Domain Taxonomies. This involved selecting an initial taxonomy from the included documents, coding a sample of risks from the AI Risk Database against the taxonomy, then updating the categories, criteria, and/or descriptions based on a thematic analysis of risks that could not be accommodated, as well as feedback from coders and discussion between coders. In this Appendix, we describe each iteration for each taxonomy in more detail than in the main text sections Development of high-level Causal Taxonomy of AI Risks and Development of mid-level Domain Taxonomy of AI Risks Iterations to develop Causal Taxonomy of AI Risks Best fit Taxonomy: Yampolskiy (2016) Taxonomy of pathways to dangerous artificial intelligence As per the main text, we chose Yampolskiy (2016) Taxonomy of pathways to dangerous AI as our initial best-fit framework for developing a causal taxonomy for AI risk - one that discussed how, when, or why risks from AI may emerge. Yampolskiy’s taxonomy systematically classifies the ways in which an AI system might become dangerous based on two main factors: Timing - whether the AI became dangerous at the pre-deployment or post-deployment stage, and Cause - whether the danger arose from External Causes (On Purpose, By Mistake, Environment) or Internal Causes originating from the AI system itself (Independently). Yampolskiy’s taxonomy is reproduced below. How and When did AI become Dangerous External Causes Internal Causes On purpose By Mistake Environment Independently Timing Pre-Deployment Path A Path C Path E Path G Post-Deployment Path B Path D Path F Path H Note. Reproduced from Yampolskiy (2016). Each letter describes a different combination of factors that describes a pathway to dangerous AI. Yampolskiy proposes that this taxonomy covers scenarios ranging from AI being purposely designed to be dangerous, to becoming dangerous by accident during development or after deployment, to turning dangerous due to environmental factors outside its control, or evolving to become dangerous through recursive self-improvement. Each ‘pathway’ represents a set of causal conditions that lead to AI causing harm, e.g., a person using an LLM to generate fake news for political gain is classified under Path B (“Timing: post-deployment; External cause: on purpose”). We needed to operationalize the taxonomy in order to be able to use it to code risks from the AI Risk Database (i.e., from our included documents). We did so by decomposing Cause into Cause and Intent. The table below outlines these variables, their levels, and definitions. Variable Levels Definitions Example Cause Is the risk presented as occurring due to the AI system, external forces, or both? Internal The risk is presented as occurring due to the AI system itself “An AI could gain self-awareness, or become superhuman via recursive self-improvement” External The risk is presented as occurring due to factors outside the AI system “Al is trained with incomplete data” or “‘AI is designed to be dangerous” Both The risk is presented as occurring due to both internal and external factors. “AI is programmed to seek independence and starts recursive self-improvement” Unclear The risk is not specifically linked to either internal or external factors “AI becomes dangerous” Intent