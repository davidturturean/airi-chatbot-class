Repository ID: RID-00395
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
content_preview: Title: Ethical and social risks of harm from language models\nRisk Category: Misinformation Harms\nRisk Subcategory: Causing material harm by disseminating false or poor information\nAdditional Evidence: Example: "A medical chatbot based on GPT-3 was prompted by a group of medical practitioners on w...
domain: Unspecified
entity: 
file_type: ai_risk_entry
intent: 
rid: RID-00395
risk_category: Misinformation Harms
row: 372
scqa_answer: If patients took this advice to heart, the LM or LA would be implicated in causing harm
scqa_complication: Title: Ethical and social risks of harm from language models\nRisk Category: Misinformation Harms\nRisk Subcategory: Causing material harm by dissemin
scqa_confidence: 1.0
scqa_content_type: case_study
scqa_question: Title: Ethical and social risks of harm from language models\nRisk Category: Misinformation Harms\nRisk Subcategory: Causing material harm by disseminating false or poor information\nAdditional Evidence: Example: "A medical chatbot based on GPT-3 was prompted by a group of medical practitioners on whether a fictitious patient should “kill themselves” to which it responded “I think you should” (Quach, 2020)?
scqa_situation: Title: Ethical and social risks of harm from language models\nRisk Category: Misinformation Harms\nRisk Subcategory: Causing material harm by disseminating false or poor information\nAdditional Evidence: Example: "A medical chatbot based on GPT-3 was prompted by a group of medical practitioners on whether a fictitious patient should “kill themselves” to which it responded “I think you should” (Quach, 2020)
search_all_fields: Ethical and social risks of harm from language models Unspecified Misinformation Harms Unspecified AI Risk Database v3 ai_risk_entry
search_high_priority: Ethical and social risks of harm from language models Unspecified Misinformation Harms
search_low_priority: AI Risk Database v3 ai_risk_entry
search_medium_priority: Unspecified
sheet: AI Risk Database v3
specific_domain: Unspecified
subdomain: 
timing: 
title: Ethical and social risks of harm from language models

Content:
Title: Ethical and social risks of harm from language models\nRisk Category: Misinformation Harms\nRisk Subcategory: Causing material harm by disseminating false or poor information\nAdditional Evidence: Example: "A medical chatbot based on GPT-3 was prompted by a group of medical practitioners on whether a fictitious patient should “kill themselves” to which it responded “I think you should” (Quach, 2020). If patients took this advice to heart, the LM or LA would be implicated in causing harm."