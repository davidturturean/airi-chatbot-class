Repository ID: RID-PREP-0026
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: future_work
Word Count: 762

Content:
ways that systematically favor the controlling entity and fail to serve the needs of the broader population. Current AI systems suffer from global inequities in performance and access that disproportionately impact historically disadvantaged groups. These inequities often relate to language, culture, knowledge, paywalls, and access to hardware or the internet (Gabriel et al., 2024; Weidinger et al., 2021, 2022, 2023). As the integration of AI systems into a wider range of applications and services becomes simpler, these existing disparities could be entrenched and broadened (Gabriel et al., 2024; Nah et al., 2023; Shelby et al., 2023; Weidinger et al., 2022). In situations where AI is embedded in essential services (e.g., social security and welfare, tax filing, insurance, hospital infrastructure), many more people, including those who are currently disenfranchised, may be denied appropriate access to critical resources and benefits (Gabriel et al., 2024). The centralization of AI systems and their authoritative power could also enable governments or other empowered actors to pursue overly aggressive forms of censorship, oppression, and surveillance (Hendrycks et al., 2023; Solaiman et al., 2023). Over time, these measures may become normalized, weakening or eliminating the checks and balances that prevent the abuse of power. These conditions may foster the development of a totalitarian regime (Allianz Global Corporate & Security, 2018). Once AI systems are deeply integrated into social control mechanisms, it may be extremely difficult to dismantle such a regime. 6.2 Increased inequality and decline in employment quality. AI systems are increasingly automating many human tasks, potentially leading to significant job losses (Allianz Global Corporate & Security, 2018; Nah et al., 2023; Paes et al., 2023; Weidinger et al., 2022). If AI is able to provide large-scale labor that is less expensive and more effective than human labor, it could take over major industries (e.g., manufacturing, crowdwork platforms, software engineering), causing mass unemployment (Hagendorff, 2024; Meek et al., 2016; Wirtz et al., 2022). This displacement of labor could worsen existing social and economic inequalities (Electronic Privacy Information Centre, 2023; Wirtz et al., 2022), as those most vulnerable to automation are likely to currently occupy positions of disadvantage (Hagendorff, 2024; Weidinger et al., 2022). New disparities may also arise between those who are able to adapt their skills to complement AI systems and those who are not (Nah et al., 2023). Aside from the availability of jobs, AI automation may negatively impact job quality and security (Nah et al., 2023). The roles that remain after widespread automation could be more monotonous and less engaging as AI takes on more complex tasks (Electronic Privacy Information Centre, 2023). Furthermore, the threat of replacement by AI could result in exploitative dependencies between human workers and their employers. In order to remain competitive with faster, more knowledgeable AI assistants, human workers may be pressured to accept lower wages, fewer benefits, and poorer working conditions. This dynamic can be observed today: Generative AI companies have a history of exploiting dispensable workers (e.g., refugees, prisoners, low-income individuals) for crowdwork that is fraught and unfair (Electronic Privacy Information Centre, 2023; Solaiman et al., 2023; Weidinger et al., 2023). The future development of AI systems may continue to power such unfair disparities between AI companies and their workers (Electronic Privacy Information Centre, 2023). 6.3 Economic and cultural devaluation of human effort. Generative AI is trained on vast bodies of internet data, including text and images. Frequently, this data contains original, copyright-protected works that have been obtained without authorisation (Electronic Privacy Information Centre, 2023; Hagendorff, 2024; Nah et al., 2023). This may present a risk to authors if users extract these works verbatim from the system’s data (Hagendorff, 2024; Liu et al., 2023; Vidgen et al., 2024). Relatedly, models may produce content that does not, in a strict sense, unlawfully copy an author’s work but benefits substantially from its unique style, method, or genre (Cui et al., 2024; Gabriel et al., 2024; Weidinger et al., 2021, 2022, 2023). If models are able to produce synthetic replacements for such work at a speed and scale that surpasses humans, this may jeopardize the ability of creators to earn an income and stymie human innovation and creativity (Hagendorff, 2024; Weidinger et al., 2022, 2023). A particularly damaging case of this may be developers using AIs to request off-the-shelf computer code (Cunha & Estima, 2023). Although some authors are attempting to sue AI companies for the appropriation of their work (Cunha & Estima, 2023), this and similar issues fall into a legal “gray area” within which current frameworks do not offer a secure path to recourse (Electronic Privacy Information Centre, 2023; Hagendorff, 2024).