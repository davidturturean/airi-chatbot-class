Repository ID: RID-PREP-0030
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: comparative_analysis
Word Count: 799

Content:
and wield power (Shevlane et al., 2023). Cyber-offense skills may enable an AI system to gain ongoing unauthorized access to hardware, software, or data systems and work strategically towards a planned goal while minimizing the risk of detection (Ji et al., 2023; Shevlane et al., 2023). AI systems could hack into control systems and military hardware, allowing it to commandeer weapons (Shevlane et al., 2023). Additionally, models may become capable of assisting in the research and development of novel weapons. In this circumstance, they may give a human collaborator step-by-step guidance on the creation of weapons (Shevlane et al., 2023). AI systems may also develop highly effective “evasion skills,” such as situational awareness (Deng et al., 2023; Gabriel et al., 2024; Infocomm Media Development Authority, 2023; Ji et al., 2023; McLean et al., 2023; Meek et al., 2016; Shevlane et al., 2023; Sun et al., 2023; Teixeira et al., 2022; X. Zhang et al., 2022) and deception (Infocomm Media Development Authority, 2023; Saghiri et al., 2022; Shevlane et al., 2023), which would allow them to outmaneuver human oversight and control. Situational awareness refers to the AI’s ability to understand and interpret its environment and situation when it is being monitored, trained, or deployed, along with the location of its technical infrastructure. Deception refers to the model’s ability to intentionally generate false or misleading statements that seem credible to humans, anticipate how these statements might influence feelings and decisions, and strategically conceal or offer information to sustain its credibility (Shevlane et al., 2023). AIs may also acquire a suite of capabilities necessary for self-proliferation. This could include skills to escape operational confines and evade detection, autonomously produce income, obtain server space or computational resources, and copy their underlying software and parameters (Infocomm Media Development Authority, 2023; Shevlane et al., 2023). Aside from self-proliferation, AIs may develop the ability to construct new dangerous models or alter current models to enhance their destructive capacity (Infocomm Media Development Authority, 2023; Shevlane et al., 2023). Finally, sophisticated AI systems may become capable of strategic planning, such as creating and executing intricate, long-term strategies that can adjust to changing conditions and that are effective across many different contexts, including novel or adversarial situations (Deng et al., 2023; Gabriel et al., 2024; Infocomm Media Development Authority, 2023; Ji et al., 2023; McLean et al., 2023; Meek et al., 2016; Shevlane et al., 2023; Sun et al., 2023; Teixeira et al., 2022). The highest risk scenarios in this subcategory are likely to arise not from a single capability, but from the convergence of several capabilities (Shevlane et al., 2023). Each of these dangerous capabilities may be used by an AI system to cause harm when intentionally directed by “legitimate” human actors (e.g., state intelligence or military agencies), or malicious human actors (e.g., criminals, terrorists), as described in the domain 4 Malicious actors & misuse. However, dangerous capabilities may also help an AI system to pursue its goals, as described in 7.1 AI pursuing its own goals in conflict with human goals or values. Instead of using these capabilities at the direction of a human, an AI system may employ dangerous capabilities to deceive or manipulate humans, gain resources, and evade shutdown or control. One scenario is that an AI system’s possession of dangerous capabilities may itself be a sufficient condition for the loss of control of an AI system (Hendrycks et al., 2023; McLean et al., 2023). 7.3 Lack of capability or robustness. This subcategory includes the broad set of risks associated with the failure of an AI system to fulfill its intended purpose. The literature identifies four main situations in which an AI may fail to perform as expected or desired. First, the AI system can fail if it lacks the inherent capability or skill required to perform a task or if this skill is poorly developed (Gabriel et al., 2024; Hogenhout, 2021; Yampolskiy, 2016). The consequences may be particularly harmful in situations where an AI is required to reason at a human level about important moral issues but does not possess this capability or possesses an obsolete or divergent version of it that is not aligned with human values (Deng et al., 2023; Gabriel et al., 2024; Infocomm Media Development Authority, 2023; Ji et al., 2023; McLean et al., 2023; Meek et al., 2016; Sun et al., 2023; Teixeira et al., 2022; Z. Zhang et al., 2023). For example, an AI-based healthcare system tasked with prioritizing patient treatment schedules might be unable to appropriately consider ethical principles like justice and beneficence, leading to prioritizations that are technically effective but immoral. Cultural, individual, and temporal differences in ideas of what is “right” or “ethical” compound the challenge of endowing AI with appropriate and adaptable ethical standards that are fit for all purposes (Wirtz et al., 2020).