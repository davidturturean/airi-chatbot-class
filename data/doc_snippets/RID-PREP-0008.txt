Repository ID: RID-PREP-0008
Source: AI_Risk_Repository_Preprint.docx
Section: Domain Taxonomy
Content Type: comparative_analysis
Word Count: 756

Content:
our full-text screening and calibration procedures. In Stage 2 we conducted forwards (citation) and backwards (references) searching and expert consultation to identify additional eligible articles. The two stages are described below. Stage 1: Searching & screening peer reviewed and gray literature Searching Search terms were generated through an iterative process and chosen for their empirical balance between sensitivity and specificity (Wilczynski et al., 2003). This included terms related to artificial intelligence (Artificial intelligence, AI, Artificial general intelligence, AGI), frameworks, taxonomies, and other structured classifications (Framework, Review, Overview, Taxonomy*), and risks (Risk, Harm, Hazard). This led to the following search string: TITLE-ABS-KEY ( ( "artificial intelligence" OR ai OR "artificial general intelligence" OR agi ) AND ( framework OR taxonom* OR review ) AND ( risk OR harm OR hazard ) ) AND ( LIMIT-TO ( LANGUAGE, "English" ) ). We conducted a search of Scopus to identify relevant academic research. The same search string was used on the following preprint databases to identify relevant literature: arXiv, Social Science Research Network (SSRN), Research Square, medRxiv, TechRxiv, bioRxiv, and ChemRxiv. Both searches were conducted on 4 April 2024. Relevant articles were downloaded for screening. Title/abstract and full-text screening Two authors formed a team to conduct title/abstract and full-text screening. Prior to screening, the team calibrated their decision-making by screening the same randomly selected articles separately (n = 23), comparing the results, and resolving disagreements. Agreement was achieved on 21 of 23 records (91%). To expedite title and abstract screening, we used a dual-screening approach with active learning in ASReview (van de Schoot et al., 2021). Active learning is an emerging research technique which uses machine learning to reduce the total number of records that require manual screening. It is now widely used for efficiently screening large datasets in systematic reviews and meta-analyses (Campos et al., 2024; Gates et al., 2019), and has been validated in a number of diverse fields (Campos et al., 2024; van de Schoot et al., 2021) and datasets (Ferdinands et al., 2023). Throughout the active-learning process, the four step SAFE procedure outlined by Boetje and van de Schoot (2024) was followed to ensure that screening identified relevant articles both rigorously and efficiently. The four phases are described below. Phase 1: Screen a random set of articles to create training data for active-learning model As per SAFE, the screening team each randomly screened and labeled 1% of the total search yield (264 records in total). Each member of the team then created separate projects in ASReview and uploaded their own files, which included all retrieved studies and the random screening data. The random screening data was automatically marked as prior knowledge, and the active-learning phase commenced. Phase 2: Apply active learning during screening until stopping rule is reached For the first iteration of the active-learning model, the team followed Boetje and van de Schoot’s (2024) recommendation to use the Oracle model and the default model setup (TF-IDF as the feature extractor, Naive Bayes as the classifier, maximum as the query strategy and dynamic resampling (double) as the balance strategy). We aimed to follow Boetje and van de Schoot’s (2024) four-fold stopping heuristics, screening until four mutually independent conditions are met: All key papers are marked as relevant. At least twice the estimated number of relevant records in the total dataset are screened. More than 10% of the total dataset has been screened. No relevant records are identified in the last 50 records screened. These four stopping heuristics aim to achieve a sensitivity of 95% (Campos et al., 2024), ensuring comprehensive data assessment while preventing excessive time spent on unlikely candidates. The team met three of these conditions: they i) screened more than twice the estimated number of relevant records, ii) screened more than 10% of the total dataset, and iii) had not identified any relevant records in the last 50 records. However, one condition (‘All key papers are marked as relevant’) could not be met due to a bug with the model. Only three out of the four key papers (Critch & Russell, 2023; McLean et al., 2023; Steimers & Schneider, 2022; Weidinger et al., 2022) had appeared in the screening process, and the final key paper was scheduled to appear several thousand papers later. Because Stage 3 of the SAFE process aims to ensure that records are not missed due to the initial model, the screening team switched models to see if a new model would locate the relevant paper. Phase 3: Switch active-learning model and screen additional records until stopping rule is reached