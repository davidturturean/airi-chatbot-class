Repository ID: RID-PREP-0006
Source: AI_Risk_Repository_Preprint.docx
Section: Domain Taxonomy
Content Type: general
Word Count: 771

Content:
and the public (Center for AI Safety, 2023; UK Department for Science, Innovation and Technology, 2023a, 2023b). The Responsible AI Collaborative’s AI Incident Database now includes over 3,000 real-world instances where AI systems have caused or nearly caused harm (McGregor, 2020). Research and investment in the development and deployment of increasingly capable AI systems has accelerated (Maslej et al., 2024). Concurrent with this attention, researchers and practitioners have sought to understand, evaluate, and address the risks associated with these systems. This work has so far produced a diverse and disparate set of taxonomies, classifications, and other lists of AI risks. Several examples demonstrate how confusion about AI risks may already impede our effectiveness at risk mitigation. Because there is no canonical set of risks, organizations developing AI are more likely to present risk-mitigation plans that fail to address a comprehensive set of risks (cf. Anthropic, 2023; Google DeepMind, 2024) or lack detail (Anderson-Samways et al., 2024). Similarly, AI-risk evaluators and security professionals are less able to comprehensively evaluate and report on AI risks without a clear understanding of the full range of threats (cf. Nevo et al., 2024). Finally, policymakers may require more extensive support from outside sources (e.g., Department for Science & Technology, 2024) to understand what they need to regulate and legislate. Another source of confusion is that risks that share a similar name, e.g., “privacy,” can refer to different harms or categories of harm (e.g., Meek et al., 2016; Wirtz et al., 2020). Taxonomies that focus on similar domains of risk, e.g., “existential,” can vary considerably in their content and how they are constructed (Critch & Russell, 2023; Hendrycks et al., 2023; McLean et al., 2023). Most papers do not base their taxonomies or classifications on a conceptual or theoretical foundation. For those papers that conduct reviews of existing work, most are narrative reviews, rather than the result of a systematic search (for exceptions, Hagendorff, 2024; McLean et al., 2023). In general, the state of taxonomies and classifications for AI risks are not consistent with best practice (Nickerson et al., 2013): they lack mutual exclusivity, collective exhaustivity, or parsimony; are static or based on arbitrary or ad hoc criteria; and tend to be descriptive rather than explanatory. The number of competing taxonomies inadvertently makes it very challenging to integrate relevant research into a cohesive shared understanding. This lack of shared understanding impedes our ability to comprehensively discuss, research, and react to the risks of AI. The absence of shared understanding can cause confusion and impair research usage, cross-study comparison, and the development of cumulative knowledge (Harrison McKnight & Chervany, 2001; e.g., Marcolin et al., 2000). Shared understanding is also important in legal, political, and practical settings; reviews are frequently cited in academic and policy documents (Fang et al., 2020; Haustein et al., 2015), and shared understanding is often cited as a goal for legal and political processes (Röttinger, 2006). For example, the U.S.-EU Trade and Technology Council (TTC) stated in its joint roadmap for Trustworthy AI and Risk Management, “Shared terminologies and taxonomies are essential for operationalizing trustworthy AI and risk management in an interoperable fashion” (European Commission and the United States Trade and Technology Council, 2022). Here, we systematically review existing AI risk classifications, frameworks, and taxonomies. We extract the categories and subcategories of risks from the included papers and reports into a “living” database that can be updated over time. We apply a “best fit” framework synthesis approach (Carroll et al., 2011, 2013) to develop two taxonomies: a high-level Causal Taxonomy of AI Risks to capture three broad causal conditions for any risk (e.g., which entities’ action led to the risk, whether the risk was intentional, when it occurred), and a mid-level Domain Taxonomy which classifies the risks into seven risk domains (e.g., Discrimination and toxicity) and 24 subdomains (e.g., exposure to toxic content). Our key contribution is the creation of a common frame of reference: an AI Risk Repository comprising a comprehensive synthesis of existing AI risk frameworks into a living AI Risk Database of risks, and the Causal Taxonomy and Domain Taxonomy of AI risks. The database and taxonomies can be used individually or in combination to explore the database, as well as for research, policy, and practice to address risks from AI. All of these artifacts are available online. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems. Methods