Repository ID: RID-02097
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
row: 2074
file_type: ai_risk_entry
search_low_priority: AI Risk Database v3 ai_risk_entry
content_preview: Title: Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data\nDomain: 2. Privacy & Security\nSub-domain: 2.2 > AI system security vulnerabilities and attacks\nRisk Category: Misuse tactics to compromise GenAI systems (Model integrity)\nRisk Subcategory: Prompt injection\nDesc...
timing: 2 - Post-deployment
intent: 1 - Intentional
title: Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data
search_all_fields: Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data 2. Privacy & Security Misuse tactics to compromise GenAI systems (Model integrity) 2.2 > AI system security vulnerabilities and attacks 2.2 > AI system security vulnerabilities and attacks AI Risk Database v3 ai_risk_entry
sheet: AI Risk Database v3
scqa_confidence: 1.0
scqa_answer: "\nEntity: 1 - Human\nIntent: 1 - Intentional\nTiming: 2 - Post-deployment
rid: RID-02097
entity: 1 - Human
scqa_complication: vulnerabilities and attacks\nRisk Category: Misuse tactics to compromise GenAI systems (Model integrity)\nRisk Subcategory: Prompt injection\nDescription
search_medium_priority: 2.2 > AI system security vulnerabilities and attacks 2.2 > AI system security vulnerabilities and attacks
scqa_situation: t al., 2023). Prompt Injections exploit loopholes in a model’s architec- tures that have no separation between system instructions and user data to produce a harmful output (Perez and Ribeiro, 2022). While researchers
scqa_content_type: case_study
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_question: What are the implications of this risk?
risk_category: Misuse tactics to compromise GenAI systems (Model integrity)
specific_domain: 2.2 > AI system security vulnerabilities and attacks
subdomain: 2.2 > AI system security vulnerabilities and attacks
search_high_priority: Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data 2. Privacy & Security Misuse tactics to compromise GenAI systems (Model integrity)
domain: 2. Privacy & Security

Content:
Title: Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data\nDomain: 2. Privacy & Security\nSub-domain: 2.2 > AI system security vulnerabilities and attacks\nRisk Category: Misuse tactics to compromise GenAI systems (Model integrity)\nRisk Subcategory: Prompt injection\nDescription: "Prompt Injections are a form of Adversarial Input that involve manipulating the text instructions given to a GenAI system (Liu et al., 2023). Prompt Injections exploit loopholes in a model’s architec- tures that have no separation between system instructions and user data to produce a harmful output (Perez and Ribeiro, 2022). While researchers may use similar techniques to test the robustness of GenAI models, malicious actors can also leverage them. For example, they might flood a model with manipulative prompts to cause denial-of-service attacks or to bypass an AI detection software."\nEntity: 1 - Human\nIntent: 1 - Intentional\nTiming: 2 - Post-deployment