Repository ID: RID-PREP-0029
Source: AI_Risk_Repository_Preprint.docx
Section: Causal Taxonomy
Content Type: comparative_analysis
Word Count: 707

Content:
a certain level of advancement. For instance, misaligned AIs may resist human attempts to control or shut them down (Gabriel et al., 2024; Hagendorff, 2024; Infocomm Media Development Authority, 2023; Saghiri et al., 2022; Stahl & Eke, 2024; Weidinger et al., 2023; Wirtz et al., 2022). In many cases, gaining more control or power (e.g., money, energy, resources) is an effective way for an AI to optimize its objectives (Gabriel et al., 2024; Hagendorff, 2024; Hendrycks & Mazeika, 2022; Ji et al., 2023; Wirtz et al., 2022). Absent strong behavioral constraints, a sufficiently advanced AI may act upon these drives. Misaligned AIs may acquire, develop, or use dangerous capabilities to evade human control and oversight and to cause mass harm. Description of some of these capabilities are provided in subdomain 7.2 AI possessing dangerous capabilities that could cause mass harm, and include situational awareness, cyber-offense, deception, persuasion and manipulation, weapons acquisition, strategic planning, and self-proliferation (Shevlane et al., 2023). For example, an AI system that possesses the dangerous capability of situational awareness may hold knowledge about its status as a model, how it is expected to operate in its surroundings, its ability to control these surroundings, and how people may respond to its behaviors (Gabriel et al., 2024; Shevlane et al., 2023). A misaligned AI system could use information about whether it is being monitored or evaluated to maintain the appearance of alignment, while hiding misaligned objectives that it plans to pursue once deployed or sufficiently empowered (Gabriel et al., 2024; Hagendorff, 2024; Hendrycks & Mazeika, 2022; Ji et al., 2023; Wirtz et al., 2022). A misaligned AI system that possesses the dangerous capabilities of persuasion, manipulation, and/or deception may use these capabilities to coerce humans into taking harmful actions that they would not otherwise take (Stahl & Eke, 2024; Weidinger et al., 2023), such as giving the AI system access to resources or weapons (Gabriel et al., 2024; Hagendorff, 2024; Wirtz et al., 2022). Combinations of dangerous capabilities may be used by a misaligned AI system: situational awareness allows a system to detect when it can pursue its goals without being monitored, deception allows a system to mislead users about its behavior and goals; persuasion or coercion allows a system to influence users to provide it with resources; the resources can then be used for self-improvement and self-replication to resist attempts of shut down or control so that the system can pursue its goals (Gabriel et al., 2024; Hagendorff, 2024; Infocomm Media Development Authority, 2023; Shevlane et al., 2023; Wirtz et al., 2022). 7.2 AI possessing dangerous capabilities. AI systems may develop or acquire capabilities that can cause large-scale harm if used by humans, misaligned AI systems, or due to a failure in the AI system. These capabilities are described as dangerous because they can be used to threaten security or exercise control over humans. These capabilities may be intentionally designed into an AI system, may emerge unpredictably during development or training of a system, may be acquired by an AI system in its environment (e.g., through the use of tools), or be provided by a user (Shevlane et al., 2023). One example of a dangerous capability is manipulation and persuasion, where an AI system can convince humans to believe things that are irrational or false or to engage in dangerous behaviors (Gabriel et al., 2024; Shevlane et al., 2023). An AI system, for instance, could convince people to transfer ownership of property or legal statuses to entities controlled by the AI or its user (Meek et al., 2016). Other dangerous capabilities include political strategy and knowledge of social dynamics that can be used to obtain and wield power (Shevlane et al., 2023). Cyber-offense skills may enable an AI system to gain ongoing unauthorized access to hardware, software, or data systems and work strategically towards a planned goal while minimizing the risk of detection (Ji et al., 2023; Shevlane et al., 2023). AI systems could hack into control systems and military hardware, allowing it to commandeer weapons (Shevlane et al., 2023). Additionally, models may become capable of assisting in the research and development of novel weapons. In this circumstance, they may give a human collaborator step-by-step guidance on the creation of weapons (Shevlane et al., 2023).