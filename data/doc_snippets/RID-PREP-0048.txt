Repository ID: RID-PREP-0048
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: limitations
Word Count: 800

Content:
7 AI system capability & safety 7.1 AI pursuing its own goals in conflict with human goals or values 7.2 AI failure from lack of capability or robustness 7.3 Lack of transparency/interpretability 7.4 AI sentience and rights 7.5 Lethal autonomous weapons Second iteration of coding and changes One author (AS) used the taxonomy to code an additional set of 100 risks from the AI Risk Database and checked the previously coded risks and discussed this with other authors (PS, JS, NT). The following changes were made after this discussion. Change Explanation Amendment of category labels to maintain relevance beyond LLMs The initial taxonomy was specifically designed to identify harms and risks from large language models. Several of the included documents also discussed LLMs or evolutions/products from LLMs (e.g., advanced AI assistants, Gabriel et al., 2024). However, others discussed other types of AI or used different terms (e.g., Artificial General Intelligence, algorithmic systems, Machine Learning). We updated several of the sub-category labels and criteria to include decisions and actions beyond generating textual content in response to prompts. New subcategories added to capture missing risks One of the remaining missing subcategories was the minor theme of infringing upon AI welfare and rights; this was added as a subcategory under AI system capability & safety, with the justification that ‘safety’ could cover both the safety of human rights, values, and interests from AI as well as the safety of AI rights, values and interests from humans. The table below shows the third version of the taxonomy after changes. Risk category Risk sub-category 1 Discrimination & toxicity 1.1 Unfair discrimination and misrepresentation 1.2 Exposure to toxic content 1.3 Unequal performance across groups 2 Privacy & security 2.1 Compromise of privacy by leaking or correctly inferring sensitive information 2.2 AI system security vulnerabilities and attacks 3 Misinformation 3.1 False or misleading information 3.2 Pollution of information ecosystem and loss of consensus reality 4 Malicious actors & misuse 4.1 Disinformation, surveillance, and influence at scale 4.2 Cyberattacks, weapon development or use, and mass harm 4.3 Fraud, scams, and targeted manipulation 5 Human-computer interaction 5.1 Overreliance and unsafe use 5.2 Loss of human agency and autonomy 6 Socioeconomic & environmental harms 6.1 Power centralization and unfair distribution of benefits 6.2 Increased inequality and decline in employment quality 6.3 Economic and cultural devaluation of human effort 6.4 Competitive dynamics 6.5 Governance failure 6.6 Environmental harm 7 AI system safety, failures & limitations 7.1 AI pursuing its own goals in conflict with human goals or values 7.2 Lack of capability or robustness 7.3 Lack of transparency or interpretability 7.4 AI welfare and rights Third iteration of coding and changes One author (AS) used the taxonomy to code all remaining 577 risks from the AI Risk Database (total: 777) and presented the revised taxonomy to all co-authors. Based on this feedback the following changes were made, including the short descriptive definitions for each subcategory of risk. One author (JG) also used the risk categories and the coded risks from the taxonomy to write detailed descriptions for each subcategory (see main text Detailed descriptions of domains of AI risks), which were then reviewed by all authors. The detailed and short descriptions were used to triangulate a shared conceptual definition of each subcategory of AI risk. Change Explanation Development of descriptive definitions for each subcategory To aid in building shared understanding of the content of each subcategory, authors involved in coding or providing feedback collaborated on short descriptions of the AI risk subcategories that would be clear, precise, and accessible to experts and non-experts. Separation of one subcategory into two The subcategory 7.1 AI pursuing its own goals in conflict with human goals or values was separated into two, because this subcategory included both AI system behaviour (i.e., AI systems acting in a way misaligned with the intent of its developers or users), and AI system capabilities (e.g., the capability to persuade humans, develop or obtain weapons, etc). A new subcategory, 7.2 AI possessing dangerous capabilities, was created from this split. After three iterations, the taxonomy was considered complete for the set of risks described in the AI risks database. The main text provides more information on the final taxonomy: the Domain Taxonomy of AI Risks. Appendix B: Characteristics of included documents The table on the following pages describes the characteristics of included documents in the living AI Risk Database. The documents are presented in order of their inclusion in the database (i.e., by paper ID). For up-to-date information on the included documents, visit our website. ID First author Year Title Item type First author affiliation Affiliated organization type First author country DOI Source 1 Critch 2023 TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI Preprint Center for Human-Compatible Artificial Intelligence, UC Berkeley University USA 10.48550/arXiv.2306.06924 Systematic search 2 Cui