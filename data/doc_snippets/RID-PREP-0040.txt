Repository ID: RID-PREP-0040
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: limitations
Word Count: 778

Content:
result of intentional action (34%) as with unintentional action (35%) There is more focus on risks emerging after AI is deployed than during development Approximately 5 times as many risks were presented as occurring after the AI model has been trained and deployed (62%) than before deployment (13%). Human-caused risks were most likely to be seen as intentional; AI-caused risks were most likely to be seen as unintentional or ambiguous Each combination of Entity, Intent, and Timing included 0-9% of risks in the database, but Human-caused intentional risks included 18% of risks; and AI-caused unintentional risks included 15% of risks. Insights from the Domain Taxonomy Our Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental harms, and (7) AI system safety, failures & limitations. A further 24 subdomains create an accessible and understandable classification of hazards and harms associated with AI, with both brief and detailed descriptions (see Table 6 and Detailed descriptions of domains of AI risks). Table 13 summarizes several key insights from the application of the Domain Taxonomy to the documents coded. Table 13. Insights from Domain Taxonomy of AI Risks Insight Supporting evidence Existing taxonomies / classifications varied extensively in the risks they covered Several documents discussed risks from all seven domains; other papers investigated only 1 or 2 domains of AI risk, but in more depth. No document discussed risks from all 24 subdomains; the average was 8 subdomains (range: 2-19). Some risk domains are discussed much more frequently than others Domain 7: AI System Safety, Failures & Limitations and Domain 6: Socioeconomic and Environmental Harms, were most commonly discussed. Domain 3: Misinformation and Domain 5: Human-computer interaction, was least commonly discussed. Unfair discrimination, privacy, and malicious use for mass harm were the most commonly discussed subdomains 1.1 Unfair discrimination and misrepresentation, 2.1 Compromise of privacy, 4.1 Disinformation, surveillance and influence at scale, 4.2 Cyberattacks, weapon development or use, and mass harm and 7.3 Lack of capability or robustness were discussed in >50% of included documents. Some subdomains are relatively underexplored The subdomain of 7.5 AI welfare and rights was mentioned in only 3% of documents and associated with <1% of risks. Similarly, 7.6 Multi-agent risks were mentioned in only 5% of documents and associated with 3% of risks. Implications for key audiences In this section, we discuss how the AI Risk Repository might be useful for different audiences. As AI systems grow in their capabilities and influence, there have been calls for increased regulation, evaluation, and research by, for instance, the multinational Bletchley Park Coalition (UK Department for Science, Innovation and Technology, 2023b), US White House (Executive Office of the President, 2023), European Union (European Commission, COM/2021/206ﬁnal, 2021/0106(COD), 2021; European Parliament, 2024), and various other entities (Chinese National Information Security Standardization Technical Committee, 2023; Committee on Technology, 2021; National Institute of Standards and Technology, 2023; UK Department for Science, Innovation and Technology, 2023a). We therefore present specific examples of how the AI Risk Repository may be useful for policymakers, auditors, industry, and academics working on these areas. Policymakers Regulation of AI systems is increasingly seen as an important mechanism to ensure the safe and ethical deployment of these technologies. The AI Risk Repository may aid policymakers in the development and enactment of regulations in several ways. It can form the basis for operationalizing frequent yet vague mentions of “harm” and “risk” in AI regulatory frameworks and developing compliance metrics that facilitate the monitoring of adherence to standards. For example, regulators need frameworks such as the AI Risk Repository to identify the type and nature of certain types of risks and their sources in order to develop a code of practices for general-purpose AI providers to comply with, such as Article 56 in the EU AI Act and section 4.1 of US Executive Order 14110 (European Parliament, 2024; Executive Office of the President, 2023). It may also help with international collaboration and setting global standards by providing a common language and criteria for discussing AI risks. For example, the EU-US Trade and Technology Council is developing a shared repository of metrics and methodologies for measuring AI trustworthiness, risk management methods, and related tools, and the AI Risk Repository could support this and similar efforts (European Commission and the United States Trade and Technology Council, 2022). Beyond the above examples, the AI Risk Repository may also be valuable to policymakers in need of a comprehensive, up-to-date database of AI risks for their work on risk prioritization, risk trend tracking, the development of AI risk training programs, and more. Auditors