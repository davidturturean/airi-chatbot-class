Repository ID: RID-PREP-0025
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 4 shows how the risks intersect across our three causal factors. The most common triads of causal conditions under which an AI risk was presented as occurring were Entity = Human, Intention = Intentional, Timing = Post-deployment (18% of all risks). This was followed by Entity = AI, Intention = Unintentional, Timing = Post-deployment (15% of all risks).
Content Type: comparative_analysis
Word Count: 725

Content:
undue confidence in their capabilities (Weidinger et al., 2021, 2022). Anthropomorphic perceptions of AIs may encourage users to develop emotional trust in the systems (Hagendorff, 2024), which can make users more likely to follow suggestions, accept advice, and disclose personal information (Weidinger et al., 2021, 2022). This trust could be exploited by manipulative actors who wish to harvest user’s sensitive data or influence their decisions and actions for purposes which are unlikely to be in the user’s best interests (Weidinger et al., 2022). For example, AI systems could be used to power increasingly manipulative recommendation algorithms (Weidinger et al., 2023). Beyond inappropriate trust, humans may develop broader and more vital attachments to AI systems that undermine their ability to function adaptively in the long term. For example, where an attachment becomes an uncontrolled dependence, a person’s ability to make free and independent decisions could be compromised (Gabriel et al., 2024). More broadly, as AIs increasingly take over human tasks (Wirtz et al., 2022) and become better at simulating satisfying and authentic interactions, people may increasingly withdraw from human relationships to immerse themselves in AI-mediated environments (Gabriel et al., 2024; Hagendorff, 2024). Over time, widespread preference for interacting with AIs could weaken social ties between humans. This shift could induce psychological distress because genuinely reciprocal relationships are often important to human satisfaction and well-being (Allianz Global Corporate & Security, 2018; Gabriel et al., 2024; Wirtz et al., 2022). 5.2 Loss of human agency and autonomy. As AI systems become increasingly capable and intelligent, humans may be tempted to delegate many of their decisions and actions to AI (Paes et al., 2023). Although such delegation can be beneficial (e.g., by saving time or money), it may lead to undesirable outcomes where unconstrained or inappropriate. For example, if AIs take over tasks that typically require human creativity and analytical thinking, humans may engage less frequently in these cognitive processes. Over time, this may lead to a decrease in our ability to think critically and solve problems independently (Nah et al., 2023). As individuals become more reliant on AI for everyday decisions – from what to eat and how to spend to more significant choices like career and relationships – there is a risk that they will lose their sense of free will and autonomy (Gabriel et al., 2024; Wirtz et al., 2022). If AIs begin to shape a person’s life path in ways that do not align with their original aspirations and desires, this could limit their personal growth and prevent the pursuit of a fulfilling life (Gabriel et al., 2024; Kumar & Singh, 2023; Shelby et al., 2023). At a societal level, organizations may hand over control to AI systems to stay competitive or reduce costs (Hendrycks & Mazeika, 2022). If a significant number of organizations adopt AI systems and automate decision-making processes, especially in a way that is opaque and difficult to challenge, it could lead to widespread job displacement and a growing sense of helplessness among the general population (Hogenhout, 2021). Domain 6: Socioeconomic & environmental harms 6.1 Power centralization and unfair distribution of benefits. Developing cutting-edge AI technologies requires significant computational power, expertise, financial resources, and datasets (Electronic Privacy Information Centre, 2023; Gabriel et al., 2024; Hogenhout, 2021; Solaiman et al., 2023). As such, there is a risk that the most influential and valuable AI technologies, along with their political and competitive benefits, could be monopolized by a handful of powerful entities, such as major technology corporations or governments (Hendrycks & Mazeika, 2022; Hogenhout, 2021). If AI is primarily controlled by a few entities, its instructions and data could reflect their narrow perspectives, experiences, and priorities (Gabriel et al., 2024; Giarmoleo et al., 2024). Without inputs from diverse parties, AI systems may operate in ways that systematically favor the controlling entity and fail to serve the needs of the broader population. Current AI systems suffer from global inequities in performance and access that disproportionately impact historically disadvantaged groups. These inequities often relate to language, culture, knowledge, paywalls, and access to hardware or the internet (Gabriel et al., 2024; Weidinger et al., 2021, 2022, 2023). As the integration of AI systems into a wider range of applications and services becomes simpler, these existing disparities could be entrenched and broadened (Gabriel et al., 2024; Nah et al., 2023; Shelby et al., 2023; Weidinger et al., 2022).