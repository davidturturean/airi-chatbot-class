Repository ID: RID-00288
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_complication: nability due to their nature. However, not all machine learning algorithms are equal in this regard. Decision trees are often considered highly explainable si
entity: 
scqa_situation: impossible for the experts to interpret how certain outputs are derived from the inputs and design of the algorithm. This suggests the difficulty of assigning liability and accountability for harms resulting from the use of the ML system, as inputs and design rules that could yield unsafe or discriminatory outcomes cannot as easily be
file_type: ai_risk_entry
scqa_confidence: 1.0
rid: RID-00288
subdomain: 
search_medium_priority: Unspecified
search_low_priority: AI Risk Database v3 ai_risk_entry
specific_domain: Unspecified
scqa_question: What are the implications of this risk?
row: 265
risk_category: First-Order Risks
intent: 
search_all_fields: The Risks of Machine Learning Systems Unspecified First-Order Risks Unspecified AI Risk Database v3 ai_risk_entry
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
search_high_priority: The Risks of Machine Learning Systems Unspecified First-Order Risks
scqa_answer: a system that can explain its decision in the event of a mistake is often desirable in high-stakes applications. A mistake can take the form of an ac
timing: 
title: The Risks of Machine Learning Systems
domain: Unspecified
scqa_content_type: case_study
sheet: AI Risk Database v3
content_preview: Title: The Risks of Machine Learning Systems\nRisk Category: First-Order Risks\nRisk Subcategory: Algorithm\nAdditional Evidence: "Explainability/transparency:Algorithmic opacity and unpredictability can pose risks and make it difficult to ensure accountability. While new mandated levels of transpar...

Content:
there have been recent advances in explaining neural network predictions, researchers have also demonstrated the ability to fool attention-based interpretation techniques. This may allow developers to prevent the networkâ€™s predictions from being correctly interpreted during an audit. The choice of an ML algorithm and its training method, therefore, affects this aspect of algorithmic risk."