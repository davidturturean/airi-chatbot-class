Repository ID: RID-01441
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
file_type: ai_risk_entry
search_low_priority: AI Risk Database v3 ai_risk_entry
scqa_complication: t with human goals or values\nRisk Category: Alignment failures in existing ML systems\nRisk Subcategory: Faulty reward functions in the wild\nDescription
subdomain: 7.1 > AI pursuing its own goals in conflict with human goals or values
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_confidence: 1.0
scqa_situation: ions\nSub-domain: 7.1 > AI pursuing its own goals in conflict with human goals or values\nRisk Category: Alignment failures in existing ML systems\nRisk Subcategory: Faulty reward functions in the wild\nDescription: -\nEntity: 1 - Human\nIntent:
intent: 2 - Unintentional
search_high_priority: Advancing AI Governance: A Literature Review of Problems, Options, and Proposals 7. AI System Safety, Failures, & Limitations Alignment failures in existing ML systems
scqa_content_type: risk_description
search_medium_priority: 7.1 > AI pursuing its own goals in conflict with human goals or values 7.1 > AI pursuing its own goals in conflict with human goals or values
risk_category: Alignment failures in existing ML systems
specific_domain: 7.1 > AI pursuing its own goals in conflict with human goals or values
scqa_question: What are the implications of this risk?
row: 1418
scqa_answer: 1 > AI pursuing its own goals in conflict with human goals or values\nRisk Category: Alignment failures in existing ML systems\nRisk Subcategory: Faulty reward functions in the wild\nDescription: -\nEntity: 1 - Human\nIntent: 2 - Unintentional\nTiming: 2 - Post-deployment
sheet: AI Risk Database v3
timing: 2 - Post-deployment
rid: RID-01441
domain: 7. AI System Safety, Failures, & Limitations
entity: 1 - Human
search_all_fields: Advancing AI Governance: A Literature Review of Problems, Options, and Proposals 7. AI System Safety, Failures, & Limitations Alignment failures in existing ML systems 7.1 > AI pursuing its own goals in conflict with human goals or values 7.1 > AI pursuing its own goals in conflict with human goals or values AI Risk Database v3 ai_risk_entry
title: Advancing AI Governance: A Literature Review of Problems, Options, and Proposals
content_preview: Title: Advancing AI Governance: A Literature Review of Problems, Options, and Proposals\nDomain: 7. AI System Safety, Failures, & Limitations\nSub-domain: 7.1 > AI pursuing its own goals in conflict with human goals or values\nRisk Category: Alignment failures in existing ML systems\nRisk Subcategor...

Content:
Title: Advancing AI Governance: A Literature Review of Problems, Options, and Proposals\nDomain: 7. AI System Safety, Failures, & Limitations\nSub-domain: 7.1 > AI pursuing its own goals in conflict with human goals or values\nRisk Category: Alignment failures in existing ML systems\nRisk Subcategory: Faulty reward functions in the wild\nDescription: -\nEntity: 1 - Human\nIntent: 2 - Unintentional\nTiming: 2 - Post-deployment