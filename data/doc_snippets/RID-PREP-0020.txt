Repository ID: RID-PREP-0020
Source: AI_Risk_Repository_Preprint.docx
Section: Table - Table 3. AI Risk Database coded with causal taxonomy: entity, intent, timing
Content Type: comparative_analysis
Word Count: 725

Content:
on a small number of languages, they can underperform for others (Weidinger et al., 2021, 2022). The underperformance of algorithmic systems for certain groups may lead to a range of negative consequences such as the reduced ability or complete inability to use and benefit from the system (Shelby et al., 2023); increased effort or challenges in using it effectively (Shelby et al., 2023); feelings of alienation, frustration, and exclusion due to the lack of inclusive design (Shelby et al., 2023); and ultimately, unequal outcomes across various domains (Shelby et al., 2023; Solaiman et al., 2023). Domain 2: Privacy & security 2.1 Compromise of privacy by obtaining, leaking, or correctly inferring sensitive information. In the context of generative AI, privacy violations arise when systems collect and divulge sensitive information that individuals or corporations do not consent to sharing with others (Cui et al., 2024; Hagendorff, 2024; Sherman & Eisenberg, 2024; Steimers & Schneider, 2022; Vidgen et al., 2024; Weidinger et al., 2023). Privacy violations can occur both accidentally and intentionally. Examples of accidental causes include AI models that memorize and inadvertently reproduce or leak sensitive personal information present in their training data, such as names, addresses, and medical records (Cui et al., 2024; Deng et al., 2023; Hagendorff, 2024; Shelby et al., 2023; Weidinger et al., 2021). Even when personal data is not included in the training dataset or directly offered by the user, models can make inferences about sensitive or protected traits of individuals based on predictive correlations within their history of interactions (Cunha & Estima, 2023; Weidinger et al., 2021), build profiles of users (Hogenhout, 2021; Shelby et al., 2023), or train AI systems (Habbal et al., 2024). As a result, models may save and reproduce sensitive information derived from prior interactions, such as classified intellectual property (Cunha & Estima, 2023; Weidinger et al., 2021). A notable example is the case where Samsung employees accidentally leaked confidential intellectual property to OpenAI after using ChatGPT to help with coding tasks (Cunha & Estima, 2023; Weidinger et al., 2021). Intentional causes include the malicious design and use of AI to exploit users’ trust by influencing them to share personal or private information about themselves or others (Gabriel et al., 2024). Privacy attacks, such as membership inference, could allow adversaries to gain knowledge of the private records used to train an AI model (Gabriel et al., 2024). Malicious actors could also deliberately extract private information from a model by crafting prompts designed to exploit the model’s knowledge of sensitive data (Cui et al., 2024). 2.2 AI system security vulnerabilities and attacks. AI systems, like other software systems, face a range of security threats. These issues may arise from inherent weaknesses in the design of AI algorithms, the data used to train the models, or the operational context. Specific examples include: Toolchain and dependency vulnerabilities that arise unintentionally through the use of automated code-generation tools (e.g., Github Copilot, Python language, OpenCV), deep learning frameworks (e.g., Tensorflow, PyTorch), or as a result of complex interdependencies in the development environment (Cui et al., 2024). External tool and API integration into AI system applications can compromise the trustworthiness and privacy of systems due to their potential unreliability or susceptibility to adversarial control (Cui et al., 2024). Security vulnerabilities in physical and network infrastructure, such as vulnerabilities in graphics processing units, or GPUs, or to sophisticated attacks like side-channel and rowhammer attacks, can lead to unauthorized access or manipulation of model parameters when used during training of AI systems (Cui et al., 2024). The use of distributed network systems for training AI systems such as LLMs exposes them to network-specific threats like pulsating attacks or congestion. Direct manipulation of AI systems such as adversarial attacks and instruction-based attacks. Adversarial attacks focus on altering the model’s learning process or extracting its data. They include perturbations designed to deceive models into incorrect outputs, extraction attacks to steal model insights, and poisoning attacks to alter model behavior (Cui et al., 2024; Gabriel et al., 2024; Liu et al., 2023). Instruction-based attacks manipulate the way the model handles and responds to inputs (Hagendorff, 2024; Liu et al., 2023; Sun et al., 2023). Attackers deliberately craft prompts to induce models to produce biased or unsafe outputs (a.k.a. ‘jailbreaking’). This manipulation directly targets the operational aspects of AI systems with the intent to cause harm. Domain 3: Misinformation