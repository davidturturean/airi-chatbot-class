Repository ID: RID-PREP-0045
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: general
Word Count: 750

Content:
to the AI system, external forces, or both? Internal The risk is presented as occurring due to the AI system itself “An AI could gain self-awareness, or become superhuman via recursive self-improvement” External The risk is presented as occurring due to factors outside the AI system “Al is trained with incomplete data” or “‘AI is designed to be dangerous” Both The risk is presented as occurring due to both internal and external factors. “AI is programmed to seek independence and starts recursive self-improvement” Unclear The risk is not specifically linked to either internal or external factors “AI becomes dangerous” Intent Is the risk presented as occurring due to the intention of the AI system, an external actor, or something else? Intentional The risk is presented as occurring due to intentional action “AI could be deliberately designed to be biased against some groups” Unintentional The risk is presented as occurring due to unintended consequences, mistakes, or side effects “Al trained with incomplete data may accidentally be biased against some groups” Both The risk is presented such that it could occur due to both intentional and unintentional factors “Al may have unfair bias against some groups” Environmental The risk is presented as occurring due to the environment, without an intentional actor “Because of complexity, Al might have unexpected negative effects” Unclear The risk is presented as occurring without clearly specifying the intentionality “AI becomes dangerous” Timing Is the risk presented as occurring before the AI is fully developed and deployed, or after it is deployed and in use? Pre-deployment The risk is presented as occurring before the AI is deployed “Bad code may create vulnerabilities in the model” Post-deployment The risk is presented as occurring after the AI model has been trained and deployed “AI may be used to create bioweapons” Both The risk is presented such that it could occur during and after deployment “Training, testing, and deploying generative AI systems contributes to the global climate crisis by emitting greenhouse gasses” Unclear The risk is presented without a clearly specified time of occurrence “LMs need to pay more attention to universally accepted societal values at the level of ethics and morality” “ The table below shows how these variables and levels map to each pathway in Yampolskiy’s taxonomy. Yampolskiy Operationalization Timing Cause Cause Intent Timing Pre-Deployment On Purpose (a) External Intentional Pre-deployment Pre-Deployment By Mistake (c) External Unintentional Pre-deployment Pre-Deployment Environment (e) External Environmental Pre-deployment Pre-Deployment Independently (g) Internal Unintentional Pre-deployment Post-Deployment On Purpose (b) External Intentional Post-deployment Post-Deployment By Mistake (d) External Unintentional Post-deployment Post-Deployment Environment (f) External Environmental Post-deployment Post-Deployment Independently (h) Internal Unintentional Post-deployment First iteration of coding and changes Three authors coded a set of risks from the papers extracted using the framework. The coders suggested the following changes to the “a priori” framework. Change Explanation Changing ‘unclear’ to ‘ambiguous’ in all categorizations The coders found that having a coding category of ‘unclear’ alongside ‘unintentional’ could lead to coding errors, so suggested changing ‘unclear’ to ‘ambiguous’. Removing ‘Cause’ and replacing with ‘Actor’ The coders found it difficult to determine the scope of the ‘cause’ of a risk. The concept of cause seemed excessively broad. For example, the presented ‘cause’ of risks intuitively seemed to include ‘intent’ and ‘timing’ and other different factors. The codes used during the trial generally mapped to the focal actor presented for each risk (e.g., the AI, or a human). The authors therefore suggested changing the category of ‘cause’ to ‘actor’ because this seemed like a clearer coding category. Changing the included categories to AI, Human, and Other and Ambiguous The coders felt that ambiguous conflated risks which were presented ambiguously with risks that were actually about something other than humans (e.g, aliens - as mentioned in one paper). They suggested changing the included categories to AI, Human, and Other and Ambiguous Moving ‘environmental’ from an ‘intention’ code to an ‘actor’ code The coders found that the ‘environment’ level of ‘intention’ code was always used to code the lack of an actor rather than the ‘intention’. Additionally, all uses of the ‘environment’ ‘intention’ code could be coded as ‘unintentional’. The authors therefore suggested removing this variable from intention and replacing it with an ‘Environmental’ code in the ‘actor’ categorization. Second iteration of coding and changes Two authors coded a set of risks from the papers extracted using the version 2 frameworks. The coders suggested the following changes to the “a priori” framework. Change Explanation Simplify all frameworks to have three levels per category