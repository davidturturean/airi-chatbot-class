Repository ID: RID-PREP-0001
Source: AI_Risk_Repository_Preprint.docx
Section: Limitations and Future Work
Content Type: table
Word Count: 793

Content:
prior classifications to produce an AI Risk Repository, including a paper, causal taxonomy, domain taxonomy, database, and website To our knowledge, this is the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database The risks posed by Artificial Intelligence (AI) are of considerable concern to a wide range of stakeholders including policymakers, experts, AI companies, and the public. These risks span various domains and can manifest in different ways: The AI Incident Database now includes over 3,000 real-world instances where AI systems have caused or nearly caused harm. To create a clearer overview of this complex set of risks, many researchers have tried to identify and group them. In theory, these efforts should help to simplify complexity, identify patterns, highlight gaps, and facilitate effective communication and risk prevention. In practice, these efforts have often been uncoordinated and varied in their scope and focus, leading to numerous conflicting classification systems. Even when different classification systems use similar terms for risks (e.g., “privacy”) or focus on similar domains (e.g., “existential risks”), they can refer to concepts inconsistently. As a result, it is still hard to understand the full scope of AI risk. In this work, we build on previous efforts to classify AI risks by combining their diverse perspectives into a comprehensive, unified classification system. During this synthesis process, we realized that our results contained two types of classification systems: High-level categorizations of causes of AI risks (e.g., when or why risks from AI occur) Mid-level hazards or harms from AI (e.g., AI is trained on limited data or used to make weapons) Because these classification systems were so different, it was hard to unify them; high-level risk categories such as “Diffusion of responsibility” or “Humans create dangerous AI by mistake” do not map to narrower categories like “Misuse” or “Noisy Training Data,” or vice versa. We therefore decided to create two different classification systems that together would form our unified classification system. The paper we produced and its associated products (i.e., causal taxonomy, domain taxonomy, living database and website) provide a clear, accessible resource for understanding and addressing a comprehensive range of risks associated with AI. We refer to these products as the AI Risk Repository. What we did Figure A. Overview of Study Methodology As shown in Figure A, we used a systematic search strategy, forwards and backwards searching, and expert consultation to identify AI risk classifications, frameworks, and taxonomies. Specifically, we searched several academic databases for relevant research and then used pre-specified rules to define which research would be included in our summary. Next, we consulted experts (i.e., the authors of the included documents) to suggest additional research we should include. Finally, we reviewed i) the bibliographies of the research identified in the first and second stages, and ii) papers that referenced that research to find further relevant documents. We initially extracted information from 43 documents, with quotes and page numbers, into a "living" database (see Figure B). Since conducting the original systematic literature search, we have periodically identified additional relevant research (22 new documents as of March 2025), and added this to the living database. You can watch an explainer video for the database here. Figure B. Image of AI Risk Database. We used a “best fit framework synthesis” approach to develop two taxonomies for classifying these risks. This involved choosing the “best fitting” classification system for our purposes from the set of classifications we had identified during our search and using this system to categorize the AI risks in our database. Where risks could not be categorized using this system, we updated the existing categories, created new categories, or changed the structure of this system. We repeated this process until we achieved a final version that could effectively code risks in the database. We also repeated this process when periodically identifying additional classification systems. During coding, we used grounded theory methods to analyze the data. We therefore identified and coded risks as presented in the original sources, without interpretation. Based on this, our Causal Taxonomy groups risks by the entity, intent, and timing presented (see Table A). Table A. Causal Taxonomy of AI Risks Category Level Description Entity Human The risk is caused by a decision or action made by humans AI The risk is caused by a decision or action made by an AI system Other The risk is caused by some other reason or is ambiguous Intent Intentional The risk occurs due to an expected outcome from pursuing a goal Unintentional The risk occurs due to an unexpected outcome from pursuing a goal Other The risk is presented as occurring without clearly specifying the intentionality Timing Pre-deployment The risk occurs before the AI is deployed Post-deployment