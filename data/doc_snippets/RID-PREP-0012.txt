Repository ID: RID-PREP-0012
Source: AI_Risk_Repository_Preprint.docx
Section: Unknown
Content Type: methodology
Word Count: 781

Content:
review (e.g., Weidinger et al., 2021, 2022, 2023), we considered descriptions and definitions from any of these taxonomies in our initial coding. We iterated on the taxonomy to accommodate risks that could not be coded against the existing Weidinger (2022) taxonomy. The most common risks that could not be accommodated were those related to AI system safety, failures and limitations; AI system security vulnerabilities and attacks; and competitive dynamics or other failures of governance to manage the development and deployment of AI systems. We describe this iteration process in detail in Appendix A. Final taxonomy: Domain Taxonomy of AI Risks The final version of the taxonomy, which we named the Domain Taxonomy of AI Risks, included seven domains of AI risk, and 24 subdomains of hazards and harms associated with AI. The domains were (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures & limitations. As with Weidinger’s (2022) taxonomy, these risk domains are not mutually exclusive; many risks span multiple domains or subdomains due to their interconnected nature. For example, a risk related to AI-generated disinformation could be relevant to both the Misinformation domain and the Malicious actors & misuse domain. The Domain Taxonomy is presented and described in more detail in the Results section. Coding Three authors were involved in coding risks against our taxonomies. Risks were coded by a single reviewer and discussed with the team where relevant. The coding process involved systematically categorizing each extracted risk according to the definitions within the relevant taxonomy. Based on grounded theory recommendations (cf. Charmaz, 2006; Corbin & Strauss, 2014), we coded risks as they were presented by the authors, aiming to capture the studied phenomena directly rather than impose our own interpretations or infer intent. When coding risks for our high-level Causal Taxonomy, we categorized risks relevant to multiple levels of each causal factor (e.g., both pre-deployment and post-deployment) as "Other." In our mid-level Domain Taxonomy, we categorized risks relevant to multiple domains and subdomains (e.g., AI-generated disinformation) in the single most relevant category. Ongoing expert consultation We conducted an ongoing expert consultation to identify relevant documents that were missed by our initial systematic search, or published since the date of the initial systematic search. We did so consistent with the aim of creating and maintaining a living review of AI risks (Elliott et al., 2017). We used several methods to identify potentially relevant documents: When we included new documents in the AI Risk Repository, we emailed the authors of those documents to identify additional relevant documents On the AI Risk Repository website, we provided a public form for proposal of relevant documents In personal communications, presentations, and other interactions with experts (e.g., when invited to a roundtable on AI risks), we informally asked attendees if they were aware of additional relevant documents. Approximately every 3 months, we screened the proposed documents. For included documents, we extracted and coded the risks as described above. Results Systematic literature search We retrieved 17,288 unique articles from our searches and expert consultations. Of these records we screened 7,945. We excluded 9,343 records that were not screened by the authors; they were excluded by our stopping criteria while using ASReview which used machine learning to recommend when screening was unlikely to yield further relevant content. We assessed the full text of 91 articles. A total of 43 articles and reports met the eligibility criteria: 21 from our search, 13 from forwards and backwards searching, and 9 from expert suggestions. We present a PRISMA diagram illustrating the search results. Ongoing expert consultation In the period May 2024-March 2025, we received recommendations to consider 44 documents. Of these, 22 met the eligibility criteria. Figure 3. PRISMA Flow Diagram for Systematic Literature Search, Ongoing Expert Consultation, and Screening Note. *The ASReview software was used to assist in screening titles and abstracts. Characteristics of included documents We included 65 documents: 25 peer-reviewed articles, 22 preprints, 6 conference papers, 12 reports. We mainly identified recent literature, with all but five (92%) of the included documents published later than 2020. We coded the corresponding author’s country and found that most of the included documents were from the USA (n = 18), United Kingdom (n = 11), China (n = 9) and Germany (n = 8). Other countries included Australia, Singapore, Portugal, Canada, Iran, Lithuania, Scotland, Netherlands, India, Spain, Türkiye, Brazil and Switzerland. Most of the included documents had a corresponding author affiliated with a University (n = 35), followed by industry organization (n = 15), with the remainder from government, international organizations (i.e., United Nations), or non-government organizations (n = 15).