Repository ID: RID-00297
Source: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_content_type: risk_description
scqa_question: What are the implications of this risk?
entity: 
scqa_complication: g of OOD inputs: Out-of-distribution (OOD) inputs refer to inputs that are from a distribution different from the training distribution. They include inp
domain: Unspecified
risk_category: First-Order Risks
row: 274
content_preview: Title: The Risks of Machine Learning Systems\nRisk Category: First-Order Risks\nRisk Subcategory: Robustness\nAdditional Evidence: "Mechanisms for handling of OOD inputs: Out-of-distribution (OOD) inputs refer to inputs that are from a distribution different from the training distribution. They incl...
timing: 
scqa_confidence: 1.0
search_low_priority: AI Risk Database v3 ai_risk_entry
title: The Risks of Machine Learning Systems
rid: RID-00297
scqa_situation: hat should be invalid, noisy inputs (e.g., due to background noise, scratched/blurred lenses, typographical mistakes, sensor error), natural variation (e.g., di
file_type: ai_risk_entry
search_medium_priority: Unspecified
search_high_priority: The Risks of Machine Learning Systems Unspecified First-Order Risks
sheet: AI Risk Database v3
url: data/info_files/The_AI_Risk_Repository_V3_26_03_2025.xlsx
scqa_answer: , adversarial training) reduces robustness risk, but often comes with extra computational overhead during training or inference
subdomain: 
search_all_fields: The Risks of Machine Learning Systems Unspecified First-Order Risks Unspecified AI Risk Database v3 ai_risk_entry
specific_domain: Unspecified
intent: 

Content:
Title: The Risks of Machine Learning Systems\nRisk Category: First-Order Risks\nRisk Subcategory: Robustness\nAdditional Evidence: "Mechanisms for handling of OOD inputs: Out-of-distribution (OOD) inputs refer to inputs that are from a distribution different from the training distribution. They include inputs that should be invalid, noisy inputs (e.g., due to background noise, scratched/blurred lenses, typographical mistakes, sensor error), natural variation (e.g., different accents, lens types, environments, grammatical variation), and adversarial inputs (i.e., inputs specially crafted to evade perception or induce system failure). Incorporating mechanisms that improve robustness (e.g., adversarial training) reduces robustness risk, but often comes with extra computational overhead during training or inference."