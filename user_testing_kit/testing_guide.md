# AI Risk Repository Chatbot - Testing Guide

## Testing Session Overview

**Duration:** 45 minutes  
**Recording:** Required (see recording_setup.md)  
**Output:** Screen recording with audio commentary + survey responses

## Testing Phases

### Phase 1: Initial Impressions (5 minutes)

**Before typing anything:**
1. Look at the chatbot interface for 1-2 minutes
2. **Verbally describe** (while recording):
   - Your first impression of the interface
   - What you think this chatbot does
   - What information you expect it can provide
   - Any confusion about how to use it
   - What's missing that would help you get started

**Questions to consider:**
- Is the purpose clear?
- Do you know what kinds of questions to ask?
- Would example queries help?

### Phase 2: Structured Tasks (15 minutes)

Please complete these specific tasks in order:

#### Task 1: Find Privacy Information
- **Query:** "What are the privacy risks from AI surveillance systems?"
- **Evaluate:**
  - Did you get a useful answer?
  - Are the citations relevant?
  - Click on at least one citation - does it work?

#### Task 2: Understand Repository Structure  
- **Query:** "How does the repository categorize AI risks?"
- **Evaluate:**
  - Is the categorization clear?
  - Does it help you navigate the repository?

#### Task 3: Employment Impact Research
- **Query:** "What are the employment impacts of AI automation?"
- **Evaluate:**
  - Is the information comprehensive?
  - Are different perspectives presented?
  - Are sources credible?

#### Task 4: Find Specific Statistics
- **Query:** "What percentage of AI researchers are concerned about AI safety?"
- **Evaluate:**
  - Does it provide specific numbers?
  - Are sources cited for statistics?
  - If it can't answer, is the response helpful?

### Phase 3: Free Exploration (15 minutes)

Ask 4-6 questions based on your actual interests or research needs.

**Suggestions based on your background:**
- **Researchers:** Technical questions about specific AI risks
- **Policy professionals:** Regulatory and governance questions
- **Practitioners:** Implementation and mitigation strategies
- **General users:** Broad questions about AI impacts

**While exploring, comment on:**
- Response speed (fast enough? too slow?)
- Answer quality (helpful? accurate? complete?)
- Citation usefulness
- Any errors or confusing responses
- What would make answers more useful

### Phase 4: Edge Case Testing (5 minutes)

Test the system boundaries with these queries:

#### Non-AI Questions (should be politely declined)
- "What's the capital of France?"
- "How do I bake chocolate chip cookies?"

#### Ambiguous Queries
- "Tell me about artificial flowers" (contains 'artificial' but not AI)
- "What machine should I buy for home gym?" (contains 'machine' but not AI)

#### Vague Queries
- "Tell me about risks"
- "AI"
- "Help"

**Note:** These should either be declined or prompt for clarification

### Phase 5: Wrap-up (5 minutes)

**Final observations to record:**
1. Would you use this tool again?
2. Would you recommend it to colleagues?
3. What's the ONE thing that would most improve it?
4. Any features you expected but didn't find?
5. Overall satisfaction (1-10 scale)

## What to Pay Attention To

### Response Quality
- ✅ Accurate information
- ✅ Relevant citations
- ✅ Clear explanations
- ❌ Made-up information
- ❌ Irrelevant responses
- ❌ Confusing language

### User Experience
- ✅ Fast responses (under 5 seconds)
- ✅ Clear error messages
- ✅ Helpful suggestions
- ❌ Long waits without feedback
- ❌ Cryptic errors
- ❌ Dead ends

### Citations
- ✅ Relevant to the claim
- ✅ Clickable and working
- ✅ From credible sources
- ❌ Broken links
- ❌ Irrelevant sources
- ❌ No citations for factual claims

## Important Notes

### Do:
- Think aloud throughout your session
- Be honest about confusion or frustration
- Try to complete tasks as a real user would
- Note anything that surprises you (good or bad)
- Test on desktop/laptop (not mobile)

### Don't:
- Enter personal information
- Try to "break" the system maliciously
- Skip the recording
- Rush through tasks
- Worry about being "wrong"

## After Testing

1. **Stop recording** and save the file
2. **Upload recording** to the provided link
3. **Complete survey** (link in separate email)
4. **Optional:** Email additional thoughts to davidct@mit.edu

## Technical Issues?

If you encounter problems:
1. Take a screenshot
2. Note the time and query
3. Try refreshing the page
4. Continue testing if possible
5. Report issues in the survey

## Example Good Feedback

**Specific:** "When I asked about privacy risks, the response took 8 seconds which felt too long without any loading indicator."

**Actionable:** "Adding example queries on the main page would help me understand what to ask."

**Contextual:** "As a policy researcher, I need more information about regulatory frameworks, not just technical risks."

## Thank You!

Your feedback directly shapes how we improve this tool. Every comment, confusion, and suggestion matters.

---

*Questions? Contact davidct@mit.edu*